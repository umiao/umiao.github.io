{"meta":{"title":"The blog of Blur","subtitle":"Hearing the fall of snow.","description":"Personal blog of blur.","author":"Blur - Shenghui Xu","url":"https://umiao.github.io","root":"/"},"pages":[{"title":"About Me","date":"2022-04-16T17:45:16.000Z","updated":"2022-06-17T03:08:39.592Z","comments":true,"path":"about/index.html","permalink":"https://umiao.github.io/about/index.html","excerpt":"","text":"Who am i This is Shenghui Xu and welcome to my blog. I am currently a year-one MS student with the Electrical Computer Engineering department of University of California, Los Angeles. I am now focusing on the track of Signals &amp; Systems and have a GPA of 4.0. I am also an incoming applied researcher intern at ebay. Skills and Tools Languages Python &#x2F; Matlab &#x2F; R &#x2F; Java &#x2F; JavaScript &#x2F; C++ Bash &#x2F; CMD &#x2F; Code Climate Frameworks TensorFlow &#x2F; Torch &#x2F; Keras &#x2F; Theano &#x2F; CUDA &#x2F; CUDNN Scikit-learn &#x2F; nltk &#x2F; Numpy &#x2F; Spark (PySpark) Qt Web HTML&#x2F;HTML5 &#x2F; CSS &#x2F; Node.js jQuery &#x2F; Vue.js &#x2F; quasar Tornado &#x2F; Chrome Dev Tools Editor &amp; IDE VIM &#x2F; Sublime Text &#x2F; Notepad++ &#x2F; Lime Text Visual Studio &#x2F; Qtcreator &#x2F; Pycharm &#x2F; Eclipse Version Control &amp; Deployment Git &#x2F; SVN &#x2F; Github &#x2F; GitLab &#x2F; Gitee &#x2F; Anaconda &#x2F; npm Testing Jenkins &#x2F; Lint &#x2F; Pytest &#x2F; Docker &#x2F; Unit Testing Data Management MySQL &#x2F; MongoDB &#x2F; Redis &#x2F; Memcached Projects Experiences"}],"posts":[{"title":"Data & Cyber Security Training Notes","slug":"Data-Security-Training","date":"2024-09-16T02:55:57.000Z","updated":"2024-09-16T04:25:39.889Z","comments":true,"path":"2024/09/15/Data-Security-Training/","link":"","permalink":"https://umiao.github.io/2024/09/15/Data-Security-Training/","excerpt":"Notes and revisits of data &amp; cyber security training session.","text":"Notes and revisits of data &amp; cyber security training session. SQL InjectionExample of attackSELECT * FROM Users WHERE Username=&#39;admin&#39; AND Password = abc&#39;OR 1=1;--Here, “abc’OR1&#x3D;1;–” is the constructed malicious input, and -- marks the end of line. Solution: Use parameterized query. Wrong Code: Straight forward concatenantion (trust user’s input as part of the SQL command) like VALUES(request_user_name&quot;,&quot;+request_user_age+&quot;&quot;) Correct Code: Parameterized query. insert_user = db.prepare &quot;INSERT INTO users(name, age) VALUES(?,?)&quot;insert_user.execute(request_user_name, request_user_age) Other ways of injection attackUnion attack: Use UNION to extend the results returned by the original query. It requires both queries to contail the same amount of columns. Conclusion All popular dev frameworks have secure construction of db queries. Use allowist validation on all user input (e.g. ban the &#39;) Apply least privilege principle on all backend db users. Consider GET and POST parameters, Cookies and other HTTP headers. Cross-Site Scripting (XSS)The user recognized a valid and known host name is the URL, clicks on the link and surfs to the URL. The website does not validate or encode the params before rendering &#x2F; reflecting it back to the user. Then, the browser may execute the injected Javascript and redirects the user to a phising site, tricking them into submitting their passwords. Prevention: Never trsut the user input. All user input rendered on the application output should be validated or encoded. If you do not validate the user input, unescaped input would be dispalyed in an immediate response to the user. This would allow attacker to perform actions within the application on behalf of other users. Frameworks offer specific API calls to protect the application from XSS by HTML encoding untrusted user input. Cookies should be securely configured. Set the HttpOnly flag to prevent scripts from accessing them.* Note that you should not mark strings as “safe”, e.g. &#123;&#123;recipe_searchedIssafe&#125;&#125;; Otherwise, the engine would escape the user input so that they would not got executed &#x2F; interpreted as html &#x2F; js. OS Command InjectionSuch vulnerability can happen when user controlled input, through parameters, cookies, http headers, etc are passed to the system shell without any prior validation. Example: If the application simply appends the GET params to the command string, the malicious command would got executed. Note that in this case, the command would run with the privilege of the running application.Note that many of the characters needs to be “url encoded” in order to be transferred and parsed correctly. Solution: User framework specific API calls instead of OS commands. If not possible, validate all user controlled output against a white-list before passing to the shell. Apply least privilege to the application. Never set shell=True. This would bypass the validation and have it executed in the shell directly. Can make use of “check_output” to validate if the user provided part is a shell parameter. If not, then you should fail it. Remote File Inclusion: Open any file &#x2F; Iclude in the page makes the application vulnearable to remote file inclusion injection. An adversary could upload a malicious script to be executed on the client-side server.You can specify it as “text&#x2F;plain” also set security policy to solve the issue. File Traverse:Using the URL GET parameter to open local files makes the application vulnerable to Path Traversal attacks. An adversary could modify the iamge parameter path to include any server file in the application page (e.g., ?image=/etc/passwd) Solution: Remove file response. Also not open server files from URL GET params. Weak Session Token GenerationIf the generation is too weak, an attacker may easily associate to another active user’s session with constructed ID.Use built-in session management functionalities instead of inventing your own. Store the Session ID in a cookie and then protect session cookies. This can be done by setting an expiry timestamp, path, “secure” and “HttpOnly” flag and invalidate on logout. Session ID properties must be secure. Make them unpredictable, time limited and single session. Use a secure communication channel. Missing Function Level Access ControlDefinition: user can perform functions that they are not authorized for, or when resources can be accessed by unauthorized users. When access checks have not been implemented, or when a protection mechanism exists but is not properly configured. Solution: Protect all business functions using a role based authorization mechanism. Implement it on the server side. Authorization should be applied using centrailized routines, provided by the framework or external modules.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Data System","slug":"Data-Science/Data-System","permalink":"https://umiao.github.io/categories/Data-Science/Data-System/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Cyber Security","slug":"Cyber-Security","permalink":"https://umiao.github.io/tags/Cyber-Security/"},{"name":"Data System","slug":"Data-System","permalink":"https://umiao.github.io/tags/Data-System/"},{"name":"Data Security","slug":"Data-Security","permalink":"https://umiao.github.io/tags/Data-Security/"}]},{"title":"Introduction to Git","slug":"Introduction-to-Git","date":"2024-08-07T05:42:55.000Z","updated":"2024-08-14T07:48:43.670Z","comments":true,"path":"2024/08/06/Introduction-to-Git/","link":"","permalink":"https://umiao.github.io/2024/08/06/Introduction-to-Git/","excerpt":"Thorough analysis and learning note based on git documentation (https://git-scm.com/book/en/v2).","text":"Thorough analysis and learning note based on git documentation (https://git-scm.com/book/en/v2). Patches set: the differences between files &#x2F; versions; can be used to recreate files Centralized Version Control Systems: single server contains all versioned files. Clients checkout file from central place. Prone to single point failure. Distributed Version Control Systems: clients fully mirror the repository and history. Git is snapshot based (store and index changed files), rather than difference based (storing the delta &#x2F; change of files).Near all operation is local.Integrity is enforced by checksum (empowered by SHA-1 hash). Git will be able to detect change &#x2F; corruption.Git generally only adds data. committed changes are hard to lost. Three status Modified: file changed, not committed Staged: marked a modified file in its current version to go into your next commit snapshot Committed: data is safely stored in your local database These states lead us to main sections of git project: working tree, staging area, git directory. Installation: refer to this link. Git Config Configuration Overwrite: .git/config (local project) will overwrite (~/.gitconfig &#x2F; ~/.config/git/config, current user) and will overwrite /etc/gitconfig (system level). Show where setting comes from: git config --list --show-origin Setting up user email:git config --global user.name &quot;John Doe&quot;git config --global user.email johndoe@example.com(remove --global to make local setting only) Check all configuration: git config --list Use git help for help page.You can use git COMMAND -h for brief help message, like git add -h. git help add will be more comprehensive. Git Repository Initialize Create repository from scratch: cd /Users/user/my_projectcd /Users/user/my_projectgit add *.cgit add LICENSEgit commit -m &#39;initial project version&#39; Clone existing repo:git clone https://github.com/libgit2/libgit2 (optional_alias) Git StatusUse git status to check the status of your files.You can check if any file is untracked (can be tracked via git add) or change uncommitted. You can use git status -s to get more compacted output. Git AddExample: git add &lt;files&gt;, the input can be either directory or file name.This command can track new files, stage files and mark merge-conflict files as resolved.*Note that git add will only stage the snapshot of a file by the time “add” command is executed. If changes happended later, we need to add again. Ignoring files: for log or temp files which we do not want Git to automatically add or even show as “untracked”, we can configure in .gitignore file. Rules of writing .gitignore file: Blank lines or lines starting with # are ignored. Standard glob (shell simplified regular expression) patterns work, and will be applied recursively throughout the entire working tree. You can start patterns with a forward slash (&#x2F;) to avoid recursivity. You can end patterns with a forward slash (&#x2F;) to specify a directory. You can negate a pattern by starting it with an exclamation point (!). For more examples of .gitignore, please check here. Git Diff git diff:Inspect what have changed but not yet staged. git diff --staged or git diff --cached:Inspect what you’ve staged that will go into your next commit. Git Commit git commit: will run the default text editor for creating a message for the commit, and the changes to be committed will be shown. Use: git commit -m &quot;YOUR MESSAGE&quot; can create the commit with message in one line. git commit -a -m &#39;YOUR MESSAGE&#39; can skip the stage area without manually adding all the changed files. However, we should be careful before doing this (should check git status). Removing Files git rm will remove a file from your tracked files (more accurately, remove it from your staging area) and then commit. The file will also be removed from your working directory. Example: git rm YOUR_FILE_NAME To remove a edited &#x2F; staged file, we need to add -f (force removal) to prevent accidental removal of data that hasn’t been recorded in a snapshot. git rm --cached YOUR_FILE_NAME can remove (accidentally staged) files from staging area but keep the file in the working tree. Example: If you forget to add log files or compiled files to the .gitignore file and accidentally staged. You can pass files, directories, and file-glob patterns to the git rm command. Example: git rm log/\\*.log and git rm \\*~. Note that the \\ is needed because Git does its own filename expansion in addition to shell’s filename expansion. We can rename file with: git mv file_from file_to This is actually equivalent to running 3 commands of: mv README.md README, git rm README.md and git add README Check commit history git log: by default, this command lists each commit with its SHA-1 checksum, the author’s name and email, the date written, and the commit message. Use the -p or --patch mode to show the difference of each commit in the format of patches : git log -p -NUM. Here NUM restricts to show only NUM entries. Use the git log --stat to show some abbreviated stats. Use git log --pretty=oneline to show the commit history in different formats. (also have other options like short, full and fuller). We can use git log --pretty=format:&quot;%h - %an, %ar : %s&quot; to format the commit history and extract the information you need. E.g. %h shows the abbreviated hash, %an shows the author name and %ar is Author date, relative and %s is the subject. Use --graph option to visualize the branch and merge history. Example: git log --pretty=format:&quot;%h %s&quot; --graph Time limiting options such as --since and --until. E.g., git log --since=2.weeks. -S &#x2F; pickaxe option. By using git log -S function_name, we will show only those commits that changed the number of occurrences of that string. Undo things Use git commit --amend takes your staging area and use it for the commit. This will overwrite your previous commit. Use git reset HEAD &lt;file&gt;… to unstage. In this case, it will be removed from the staging area but changes kept in working tree. * It should be noted that reset is a dangerous command. In git, almost all commited data can be recovered. However, uncommited changes may lost forever. Working with Remotes git remove -v: shows you the URLs that Git has stored for the shortname to be used when reading and writing to that remote. git remote add &lt;shortname&gt; &lt;url&gt; will add a new remote explicitly. git fetch &lt;remote&gt;: goes out to that remote project and pulls down all the data from that remote project that you don’t have yet. (If you used clone to copy a remote, it will be automatically added as remote “origin”). Also, git fetch will do the data downloading only without changing your work. git pull: automatically fetch and then merge that remote branch into your current branch. git push &lt;remote&gt; &lt;branch&gt; will push your changes to the upstream. (This command works only if you cloned from a server to which you have write access and if nobody has pushed in the meantime) git remote show &lt;remote&gt; to check more information a remote. git remote rename &lt;old_name&gt; &lt;new_name&gt; to rename a remote. git remote remove &lt;remote&gt; to remove a remote. TaggingGit has the ability to tag specific points in a repository’s history as being important. Lightweight tag: a pointer to a specific commit Annotated tag: full objects, checksummed, with the tagger name, email and date and message. Example: Annotated: git tag -a v1.4 -m &quot;my version 1.4&quot; Light-weighted: git tag v1.4-lw Use git show to check the tagger information, data the commit was tagged and theannotation message. Use git tag -d &lt;tagname&gt; to remove a tag. * By default, git push will not transfer tags to remote. We need to do that explicityly: git push origin &lt;tagname&gt;Or we can use git push origin --tags to push ALL tags.** if you checkout to a tag, your repo will be at detached HEAD status. In such case, if you make changes and then created a commit, the tag will stay the same, but your new commit will not belong to any branch and become unreachable. AliasWe can create alias for git commands for convenience, examples:git config --global alias.co checkoutgit config --global alias.br branchgit config --global alias.unstage &#39;reset HEAD --&#39;git config --global alias.last &#39;log -1 HEAD&#39; To run an external command, rather than a Git subcommand, start the command with a ! character. git config --global alias.visual &#39;!gitk&#39; BranchGit stores snapshots (as BLOB objects) for files, rather than patches or diffs. When making a commit, Git stores a commit object that contains a pointer to the snapshot of the content you staged. Also contains the author’s name and email address, the message that you typed, and pointers to the commit or commits that directly came before this commit (its parent or parents):* 0 parents for the initial commit, 1 parent for a normal commit, and multiple parents for a commit that results from a merge of two or more branches.** Aside from the commit information and blob obj, a tree object is used to record the directory structure and blob object reference. A branch in Git is a movable pointer to one commit. The default branch name in Git is master. Commands: Run git branch without parameters, you will see a list of branches, with the current branch marked with *. git branch --merged &#x2F; git branch --no-merged can show the branches which are already merged (can be safely deleted) &#x2F; not yet merged by the current branch (cannot be deleted by git branch -d, need to use git branch -D). Create a new branch git branch &lt;name&gt;: creates a new pointer to the same commit you are currently on. A speical pointer HEAD marks the current branch.* Note that with git branch, you just created a new branch, do not checkout to that branch by default. You can use git log --decorate to check which commit objects the branch pointers are pointing to. Checkout to branch Use git checkout &lt;name&gt; to have HEAD pointing to another branch (for checking out). You can create a new branch, and checkout to that branch with: git checkout -b &lt;name&gt; A branch in Git is actually a simple file that contains the 40 character SHA-1 checksum of the commit it points to. This makes the branch creating &amp; checkout extremely efficient. * You do not need to provide the entire 40-character SHA-1. You just need to provide at least 4 beginning characters (by default 7) (have to be unambiguous).** Branch reference: If a commit is at the tip of a branch, you can refer to it with git show &lt;commit_id&gt; or git show &lt;branch_name&gt; Merge of branch In case you want to switch to the master branch to add some fix while shelving the changes in your current dev branch, you can first commit and the changes and then checkout to master. When the fix is finished, and the dev is finished after that, you can checkout to master and use git merge &lt;dev_branch&gt; to merge the changes. Rename of branch Locally: git branch --move bad-branch-name corrected-branch-name Globally: git push --set-upstream origin corrected-branch-nameWe can use git branch --all to check if the renaming is successful. Remote branches Remote-tracking branches are references to the state of remote branches. Git moves such local references (pointers) for you whenever you do network communications to make them up-to-date. Remote-tracking branch names take the form &lt;remote&gt;/&lt;branch&gt; Remote-tracking branches (pointers) will not move if you do **NOT** connect to remote server and pull. Use git remote add &lt;remote_alia&gt; &lt;remote_url&gt; to add a new remote repository for your project. git checkout -b &lt;branch&gt; &lt;remote&gt;/&lt;branch&gt; or git checkout --track &lt;remote&gt;/&lt;branch&gt;, to create a local branch which tracks the remote branch. * To set the local branch to use a different name, use git checkout -b local_name remote/branch ** Use -u or --set-upstream-to to set &#x2F; update the upstream branch for an existing local branch. Use git branch -vv for ALL tracked branches. * This may not be updated. Run git fetch --all ahead. Use git push origin --delete remote_branch to remove a branch from server. This can usually be recovered before garbage collection. Rebasing If merge is used in branch merging, a three-way merge will be performed on the 2 latest branch snapshots, and the most recent common ancestor of them 2. * This will result in a new snapshot and commit. Rebasing: take the patch of the change that are introduced in dev branch, and reapply it on top of the master branch. Rebasing works by: a. Going to the common ancestor of the two branches b. Getting the diff introduced by each commit of the branch you’re on c. Saving those diffs to temporary files d. Resetting the current branch to the same commit as the branch you are rebasing onto e. Finally applying each change in turn. Rebasing makes the commits apply cleanly and appears as the work happened in series though they are originally in parallel. Rebasing enables a fast-forward and a clean apply. It is possible to apply the generated replays &#x2F; patches to a different branch than the target branch. git rebase --onto master server client will: Take the client branch, figure out the patches since it diverged from the server branch Replay these patches in the client branch as if it was based directly off the master branch instead. The Perils of RebasingDo NOT rebase commits that exist outside your repository and that people may have based work on. In that case, you’re abandoning existing commits and creating new ones that are similar but different.If you changed the submission history in a public repo, you and cowork may face confusing results (duplicates of commit, and the deprecated commits can be found back). Solution: Ask your coworkers to git pull --rebase. You should not remove commits where others may be doing development on.* ! Rebase local changes before pushing to clean up your work, but never rebase anything that you’ve pushed somewhere. Git protocols on Server Local: shared file system based, remote drive may be slow and inconvenient. Each user will gain complete shell acess of the remote directory. HTTP: “Smart HTTP” works similar to SSH and Git, just on HTTP&#x2F;S port with autentication mechanisms. Fast and easy, can provide encrption and pass firewalls. Setting up the server end can be less convenient. We also have “Dumb HTTP” which expects the bare Git repo to be served like normal files. SSH (Secure Shell): More common, safe and efficient. However, it does not support anonymous access to your Git repository. Git: Based on a special port 9418, NO authentication or cryptography. It is difficult to set up and requires firewall access to port 9418. Reflogreflog: a log of your HEAD and branch references, kept for a few months. It should be noted that it is stored only locally. When a new repo is cloned, it is going to be emppty. Every time your branch tip is updated for any reason, Git stores that information for you in this temporary history. Refer to older commits: git show HEAD@&#123;5&#125; to show the fifth prior value of the HEAD for your repo. Use git show master@&#123;yesterday&#125; to check the commits which are pointed by your master branch yesterday. Use git log -g to check the content of reflog, but using the format of git log output. Ancestry References If you place a ^ (caret) at the end of a reference, Git resolves it to mean the parent of that commit. git show HEAD^ will show the parent of HEAD. You can specify a number to identify which parent you want. d921970^2 means the second parent of d921970. ~ refer to the first parent. Thus, HEAD~ is equivalent to HEAD^. Commit Ranges Double Dot: Show all commits reachable from experiment that aren’t reachable from master: git log master..experiment. Similarly, git log experiment..master shows everything in master not reachable from experiment. * git log origin/master..HEAD Shows what is about to push to remote. Multiple Points: See what commits are in any of several branches that aren’t in the branch you’re currently on: use ^ or --not before any reference from which you don’t want to see reachable commits. Equivalent expressions: 1. git log refA..refB 2. git log ^refA refB 3. git log refB --not refA Use git log refA refB ^refC to see commits reachable from refA and refB, but not from refC. Triple Dot: specifies all the commits that are reachable by either of two references but not by both of them. git log --left-right master...experiment shows which commit is unique and belongs to which branch. Stashing and Cleaning git stash can preserve the half-done work so you can work on other things and come back later. * git stash save is to be migrated to: git stash push Use git status to see if the stash command takes effect (working tree clean) git stash list to check the stored stashes. git stash apply to apply the recently stashed work (by default the most recent one) * To apply an older change, you can specify the name like git stash apply stash@&#123;2&#125; Also the working tree should be clean; Stashed changes can be apply to another branch, if not, merge conflicts. ** The staged files will NOT be restaged. To do that, run git stash apply --index The git stash apply will not remove your stash. To remove, use git stash drop stash@&#123;0&#125; to drop stash by name (0 is the most recent one) git stash pop will apply &amp; delete the most recent stash. By default, only tracked files will be stashed. To stash untracked files: git stash -u / --include-untracked To stash ignored files, git stash -a \\ --all Use git stash --patch to interactively determine what to stash. git stash branch &lt;new branchname&gt; to create a NEW branch based on the stash. Git Clean Use git clean to remove redundant files &#x2F; clean up working directory which are NOT ignored (remove untracked files &#x2F; empty dir by git clean -f -d) (remvoe ignored files using -x) Better git stash --all first, because such files may not be able to be retrieved. Use git clean -d -n / --dry-run to see what it will do. Search Use git grep keyword to search files in your working directory. git grep -n keyword to search the line number. Use -c / --count to summarize the output. Use -p / --show-function to display the enclosing method or function for each matching string. Log Searching To see when a term existed or was introduced, we can use -S:git log -S keyword --oneline. We can use -G to search with regular expression. Use -L to trigger Line Log Search. git log -L :FunctionName:FileName Git will try to figure out what the bounds of that function are and then look through the history and show us every change that was made to the function as a series of patches back to when the function was first created. Rewrite History git commit --amend to rewrite the last commit changes and message. Use git rebase -i HEAD~3 to rewrite the last three commits. You can also specify a branch to rewrite all the diverging commits. When entering the text editor, you can change “pick” into “edit” to edit a commit, or use “squash” to merge a commit. After making the changes, run git commit --amend and git rebase --continue You can use this method to reorder commits, or eliminate some entirely. To rewrite huge swaths of history, you can use filter-branch, or just use the more recommended git-filter-repo. ResetMental frame of Git: Managing 3 trees (collections of files). HEAD: Last commit snapshot, next parent Index: Proposed next commit snapshot Working Directory: Sand box. After git add, content in working tree will be added to index. With git commit, content in index will be further saved as a permanent snapshot, create a commit object pointing to that snapshot and update the current branch. git reset --soft: Move HEAD only to revert the last commit. This can be used to compact the commits. git reset --mixed: Move HEAD and update index to be aligned with the current HEAD. Aside from reverting the commit, we unstaged everything. git reset --hard: the working directory is eventually overwritten with the commit we reset to. It is dangerous because it destroys data. If a path is set, git reset will skip the moving of HEAD but update the index and working directory partially. git checkout &lt;branch&gt; can be very similar with git reset --hard &lt;branch&gt;. However, for checkout, the working directory is safe, and will try to do a trivial merge in the working directory. Also, reset will change a commit a branch pointing to, but checkout will change the branch itself. If a path is attached, checkout works like git reset --hard [branch] file. It will not move HEAD and is not safe for working directory. Tips About Merge If you reverted a merged change, but later want to merge again, it will not work because the original merge is still reachable. In that case, you will also need to revert the revert. git config --global rerere.enabled true can trigger the rerere function. After merging, how conflicts are resolved will be recorded and can be check using git rerere status. git rerere diff will raise the text editor so that conflicts will be resolved. After resolving that, the solution will be kept and remembered. In the future, we can see something like Resolved &#39;hello.rb&#39; using previous resolution. * if you have the solution, just use git rerere to reuse solution for resolving. git blame -L Line_range_beg,Line_range_end &lt;FileName&gt; can check who introduced certain change. git bisect can be used to detect when failure is introduced. a. git bisect start: start searching b. git bisect bad: current commit is bad c. git bisect good &lt;good_commit&gt;: certain commit is good We can keep giving “good” and “bad” feedbacks to figure out the first bad one. After all, use git bisect reset to reset the HEADpointer.","categories":[{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"Software Engineering","slug":"Job-Search/Software-Engineering","permalink":"https://umiao.github.io/categories/Job-Search/Software-Engineering/"}],"tags":[{"name":"Data System","slug":"Data-System","permalink":"https://umiao.github.io/tags/Data-System/"},{"name":"Git","slug":"Git","permalink":"https://umiao.github.io/tags/Git/"},{"name":"Version Control","slug":"Version-Control","permalink":"https://umiao.github.io/tags/Version-Control/"}]},{"title":"14th_May_Researcher_Paper_Work_Group","slug":"14th-May-Researcher-Paper-Work-Group","date":"2024-05-15T06:43:14.000Z","updated":"2024-08-19T18:49:56.398Z","comments":true,"path":"2024/05/14/14th-May-Researcher-Paper-Work-Group/","link":"","permalink":"https://umiao.github.io/2024/05/14/14th-May-Researcher-Paper-Work-Group/","excerpt":"Discussion on challenges with stream processing.","text":"Discussion on challenges with stream processing. Upper-Confidence-Bound Procedure for Robust Selection of the BestAuthor: Yuchen Wan In simulation area, there may lies input uncertainty in the underlying simulation model.Robust selection of the best (RSB): models this input uncertainty by a discrete ambiguity set (containing multiple possible scenarios) and then proposes a two-layer framework under which the best alternative is defined to have the best worst-case mean performance over the ambiguity set.* Categorized as fixed-precision and fixed-budget This paper claims to invent a new robust upper-confidence-bound (UCB) procedure. Ranking and selection (R&amp;S) seeks to select the best alternative (have the smallest meanperformance) from a finite number of alternatives through repeatedly sampling a simulation model. If the input distribution is not known, we may also want to model the input uncertainty (measurement error).2-layers: 1 layer to identify the distribution of worst case, 1 layer to compare these cases mean performance. Difference from MAB (multi-Armed Bandit): Do not count in the regret during “exploration” stage, just aims at selecting the best one. This is under a clear explore-and-exploit 2 stages framework, and should it be easier? For me it is quite intuitive that higher exploration rate will lead to better simple regret.. Take Away: Figure out a well developed field like RL &#x2F; MAB, and transfer the related techniques &#x2F; success into new scenarios or (may be simplified) problems. To be improved: we may expect a stronger theoretical analysis and tighter bound of error &#x2F; probabilistic guarantee. The guarantee shown in the paper may not look very satisfying.","categories":[{"name":"Research","slug":"Research","permalink":"https://umiao.github.io/categories/Research/"},{"name":"Paper Read","slug":"Research/Paper-Read","permalink":"https://umiao.github.io/categories/Research/Paper-Read/"}],"tags":[{"name":"Research","slug":"Research","permalink":"https://umiao.github.io/tags/Research/"}]},{"title":"Designing Data-Intensive-Applications-Note-19 [END]","slug":"Designing-Data-Intensive-Applications-Note-19","date":"2024-05-06T21:51:50.000Z","updated":"2024-05-11T23:30:03.949Z","comments":true,"path":"2024/05/06/Designing-Data-Intensive-Applications-Note-19/","link":"","permalink":"https://umiao.github.io/2024/05/06/Designing-Data-Intensive-Applications-Note-19/","excerpt":"Discussion on challenges with stream processing.","text":"Discussion on challenges with stream processing. Stream ProcessingGeneralization of batch processing, which removes the assumption that the input is bounded of a known and finite size. Daily batch processes may be too slow to reflect chagnes, that is why we can process data continuously, abandoning the fixed time slices entirely and simply processing every event as it happens. That is the idea behind stream processing. Stream: data that is incrementally made available over time. Event: A small, self-contained, immutable object containing the details of something that happened at some point in time.An event usually contains a timestamp, and may be encoded as a text string, JSON or in binary form. In streaming terminology, an event is generated once by a producer (also known as a publisher or sender), and then potentially processed by multiple consumers (subscribers or recipients)Related events are usually grouped together into a topic or stream.In case of continual processing with low delays, polling can be expensive without special optimization.Lower the percentage of requests can return new events, and make the overhead high. Instead, it is better for consumers to be notified when new events appear (cannot be handled well by traditional DB).A common solution is to use Messaging Systems (a producer sends a message containing the event, which is then pushed to consumers).* Unix pipe and TCP connection are easy way to implement, but usually it is extended so that multiple producers &#x2F; consumers are allowed to send &#x2F; receive. In case producers send messages faster than the consumers can process them: drop message, store in queue or apply backpressure (a.k.a flow control, block the producer from sending more messages) [we can have a small fixed-size buffer, block sender if filled up]. In case node crashes or temporarily go offline. Note that durability comes with cost of performance, and should be determined by need of application. Direct messaging methods: UDP multicast: low latency, unreliable, retransmit on demand Brokerless messaging libraries such as ZeroMQ and nanomsg take a similar approach, implementing publish&#x2F;subscribe messaging over TCP or IP multicast. Or we can use unreliable UDP messaging for collecting metrics (like counter) from all machines on the network and monitoring them. Consumer can expose service on network, producers can make direct HTTP or PRC request to push messages to the consumer. A callback URL of one service is registered with another service, and it makes a request to that URL whenever an event occurs. (The idea of webhooks) However, we usually assume producer and consumers are constantly online. Otherwise, consumer can miss messages from producer if offline. Message BrokersAlso known as message queue, a kind of database optimized for handling message streams (consumers and producers connect as clients, read and write from the broker). Clients can come and go, durability is handled by the broker. Unbounded queueing can be allowed. The consuming is also asynchronous: producer will not wait till the message got consumed. Difference with Database: Delivered message will be deleted. Will assume the working set to be small, the queue is short, can overflow to disk. Rather than secondary indexes, brokers usually support subscribing to a subset of topics matching some pattern. Do not support arbitrary queries (based on snapshot isolation), will notify clients when data changes. When serving multiple consumers, we can either do Load balancing (each message is delivered to one of the consumers, for them to share the work of processing) or Fan-out (each message is delivered to all consumers).These patterns can be combined to be used. Acknowledge from client is needed before deleting a message, otherwise it is redelivered to another consumer (atomic commit protocol is needed in case message is processed by lost in network).* Note that combination of load balancing with redelivery inevitably leads to messages being reordered. If there are causal dependencies between messages, do not use load balancing. Messages stored in broker are regarded as temporary, and the acknowledgement is destructive as discussed above. Log based message brokers aims at achieving low-latency notification and durability.Producers append message to end of log, consumers read log sequentially, if it reaches the end, waits for the notification of a new message appended. (just like tail -f in unix)Log can be partitioned on different machines by different topics, and achieve higher throughput.Within each partition, each message has a monotonically increasing sequence number, or offset and messages are totally ordered. No ordering guarantee across different partitionsExamples: Apache Kafka, Amazon Kinesis Streams, and Twitter’s DistributedLog Log based method usually supports fan-out messaging, as consumers can independently read the log.Load balancing can be done on partition level (coarse grained), and consumer read partition using single-threaded manner. Limitations: The number of nodes sharing the work of consuming a topic can be at most the number of log partitions in that topic. Single slow message can hold the processing of subsequent messages in the same partition. Consumer can maintain an offset to record which messages have been processed. In case of consumer failure, another node and restart from this offset.Also, “append-only” log makes replaying and recovering easier. Disk Usage of Log Based Broker:To reclaim disk space, the log is divided into segments and old segments can be deleted or moved to archieve storage.It is possible that a slow consumer fall behind and its consumer offset points to a deleted segments and miss some messages. Effectively, the log implements a bounded-size buffer that discards old messages when it gets full, also known as a circular buffer or ring buffer (the size can be large).After the buffer is filled, old messages start to be overwritten. Each message will be written to disk anyway, making the throughput more constant (and relatively higher) than memory-disk based broker. Stream Data SystemsIn practice, multiple technoligies can be used in applications (e.g., OLTP database, cache, index, data warehouse..).Each has its own copy of data in different representation, so they need to be kept in sync with each other. Dual writes (explicitly writes to each of the systems when data changes) are used in case periodic full database dumps are too slow.However, it is prone to race conditions, the eventual result on each system may not be consistent (due to the order certain message arrives). Also, some write may fail.We can use 2PC to solve, but with high cost. Change Data Capture has become an interesting topic, where we can take changes made in a database and replicate them to different storage technology like indexes or caches.This can be hard, because there is a lot of internal implementation details of databased exposed in log. Implementation:We can make one database the leader and others as followers using log based message broker (without reordering issue).Log consumers are derived data systems, just storing another view on the data and we ensure all changes also reflects in the derived data systems.* Database trigger can be used, but with poor performance. Keep all the changes log for replay can be expensive, so we can have it truncated and create an initial snapshot.Snapshot must correspond to a known position or offset in the change log. Log Compaction: logs can be compacted by discarding duplicates, merge by key and only the latest value is preserved. Deleted keys (indicated by tombstone) would also be removed.* This idea can be extended to log-based message brokers and change data capture, where we only keep the most recent write for a particular key.When rebuilding data system, we can just start scaning from the beginning of a compacted log, without taking another snapshot of the CDC source database. Event Sourcing: store all changes to the application state as a log of change events. For Change Data Capture, the application uses the database in a mutable way (also a low level way, like parsing change logs), and application writing to DB does not need to be aware of it. For Event Sourcing, we expect immutable events are written to an event log (append-only) to reflect things happened at application level, rather than on lower level.This helps us better understand why things happen, like For example, storing the event “student cancelled their course enrollment” clearly expresses the intent of a single action in a neutral fashion, whereas the side effects “one entry was deleted from the enrollments table, and one cancellation reason was added to the student feedback table” embed a lot of assumptions about the way the data is later going to be used. New side effect to easily be chained off the existing event (like a place can be offered to next person). *It shoud be noted that user usually prefer seeing current states of system, rather than history of modifications.Change Data Capture can discard former events and only keep the latest value, but Event Sourcing usually needs the entire history as later event do not override prior events.This is a limition of immutability, and can be optimized using snapshot. The philosophy of event sourcing is to distinguish events and commands.User’s request is initially a command, and may fail due to violation of integrity condition. However, if it is accepted, it becomes a durable and immutable event. It cannot be rejected by consumer of the event stream.Any validation must happen before a command is executed. It should be noted that mutable state (come with accoutability, and is good for analyzing) and append-only log of immutable events do not contradict each other.Database is a cache of a subset of the log, containing the latest values.* append only log is more friendly for achieving atomicity and removing non-determinism. Command Query Responsibility Segregation (CQRS): Derive serval (read-oriented) views from the same event log. We can have separate read-optimized view for the new feature, without changing the existing system.This breaks the design fallacy that data must be written in the same form as it will be queried. This can also help resolving the debate around normalization and denormalization (such view can be translated from event log). Limitation of event sourcing and change data capture is that the event log are usually asynchronous, so the read view may not be synced. We can either synchronously update the read view or use a total order broadcast. For small dataset with high rate of updates and deletes, Immutability may result in large history and performance of compaction and garbage collection becomes crucial for operational robustness For privacy &#x2F; compliance concern, we may want to remove erroneous ifnormation or contain accidental leakage. We may need to rewrite history and pretend that data was never written (called excision or shunning).* Note that truly delete data can be challenging, because copies can live in many places rather than overwritten in place, we just try to make retrival of data harder. Processing StreamsA piece of code that processes streams to produce derived streams is known as an operator or a job.The input is consumed in read-only fashion and writes to a different location in append-only fashion. Since stream is endless, we can NOT use mapping operations such as transforming and filtering (sort-merge) records. Also, it is impossible to restart from beginning after a crash. Stream is widely used for monitoring purpose requesting pattern matching and correlations.Other applications include: Complex event processing (search for certain patterns of events in a stream, such pattern may described using high-level declarative query language or GUI, then maintain a state machine. If detected, a complex event is emitted) In such cases, queries become long-term. Stream analytics: towards aggregations and statistical metrics, usually over fixed time intervals (e.g., averaged request counts and latency in different percentiles). The time interval over which you aggregate is known as a window, and probabilistic algorithms are sometimes used like Bloom filters and HyperLogLog Maintaining materialized views: deriving an alternative view onto some dataset for query purpose, and update it whenever data changes. However, we may need to maintain all evenets forever, which is against the assumption of a time window. Search on streams: we may also need to search individual events based on complex criteria (Elasticsearch‘s percolator can support this). We can store (potentially index) queries and ducuments to dynamically search individual events. Clock can be a big challenge in stream processing, as we do not have a unified reliable clock to rely on, and we need timestamp to detemine window. One mitigation solution is to log three timestamps:the time an event occured (device clock), the device clock when it was sent to the server and the server clock when it is received.By subtracting the second timestamp from the third, you can estimate the offset between the device clock and the server clock Some different types of windows which got commonly used: Tumbling window: fixed-lengthed, every event belongs to exactly one window. Can be implemented by taking each event timestamp and rounding it down to the nearest minute. Hopping window: fixed-lengthed, allow windows to overlap to provide smoothing. Sliding window: Contains all the events that occur within some interval of each other. Can be implemented by a buffer of events, and remove old events when they expire from the window. Session window: No fixed duration, just group all events for the same user occur closely together in time. Ends when user become inactive. Stream Joins Stream-stream join (window join): We may need to link search and click activity (within certain time window) to let them sharing the same session ID (note that you need to learn accurate click-through rate, to learn searches which did not bring clicks events). We can use a stream processor to merge such activities by ID. Stream-table join (stream enrichment): Enrich the activity events with information from the table. If the database querying is slow, consider loading the database copy (may need to be updated by change data capture) to the stream processor. Table-table join (materialized view maintenance): Cache maintenance similar to the example provided at the beginning of this book. When celebrity tweets, sync it too all followers’ tiemlines. We can maintain a materialized view for a query that joins two tables For the aboce cases, time dependency is important, as data in certain DB can change (e.g., tax rate), which prevents us from getting the same result rerunning the same job.In data warehouse, it is called Slowly Changing Dimension (SCD). We can use a unique identifier for a particular version of the joined record (e.g., whenever tax rate changes). This makes the join deterministic, but will also make log compaction impossible. Fault ToleranceExactly-once Semantics &#x2F; Effectively-Once: though some tasks &#x2F; records may failed and processed multiple times, the visible effect is as if they have only been processed once. However, stream is infinite, so we cannot make output until finished. Microbatching: break the stream into small blocks, and treat each block like a miniature batch process (in a implicit tumbling window).Can generate rolling checkpoints of state and write them to durable storage (however, if message is sent to external message broker like email, checkpoint is not sufficient to make it effective once, and will need 2PC) Idempotence: We can require certain operation which can be performed multiple times, but effect only once.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Data System","slug":"Data-Science/Data-System","permalink":"https://umiao.github.io/categories/Data-Science/Data-System/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Data System","slug":"Data-System","permalink":"https://umiao.github.io/tags/Data-System/"},{"name":"Designing Data-Intensive-Applications","slug":"Designing-Data-Intensive-Applications","permalink":"https://umiao.github.io/tags/Designing-Data-Intensive-Applications/"}]},{"title":"Designing Data-Intensive-Applications-Note-18","slug":"Designing-Data-Intensive-Applications-Note-18","date":"2024-05-05T19:00:33.000Z","updated":"2024-05-06T07:08:07.592Z","comments":true,"path":"2024/05/05/Designing-Data-Intensive-Applications-Note-18/","link":"","permalink":"https://umiao.github.io/2024/05/05/Designing-Data-Intensive-Applications-Note-18/","excerpt":"Discussion on challenges with batch processing jobs.","text":"Discussion on challenges with batch processing jobs. Unix Tools (Pipe)We can categorize systems into: Serivices(online systems), Batch Processing Systems (offline systems) and Stream processing system (near real-time). The Unix philosophy in data processing: concatenate data input and output to form data flows, using pipe. Make each program do one thing well. To do a new job, build a fresh rather than complicate old programs by adding new “features”. Expect the output of every program to become the input to another, as yet unknown, program. Don’t clutter output with extraneous information. Avoid stringently columnar or binary input formats. Don’t insist on interactive input. Design and build software, even operating systems, to be tried early, ideally within weeks. Don’t hesitate to throw away the clumsy parts and rebuild them. Use tools in preference to unskilled help to lighten a programming task, even if you have to detour to build the tools and expect to throw some of them out after you’ve finished using them. This approach—automation, rapid prototyping, incremental iteration, being friendly to experimentation, and breaking down large projects into manageable chunks— sounds remarkably like the Agile and DevOps movements of today. In order to loop programs together, we need all programs to use the same input&#x2F;output interface (file, just ordered sequence of bytes).Can be extended to actual files, channels to another process (Unix socket, stdin, stdout), a device driver (say /dev/audio or /dev/lp0), a socket representing a TCP connection, and so on. * Standard protocol may not be trivial, like in Bulletin Board Systems (BBSs), each system had its own phone number and baud rate configuration Take-aways of Unix Philosophy Separation of logic and wiring, as a result of loose coupling design. You can by default use stdin and stdout or change to other sources, by it is not a problem for the program. The input files are normally treated as immutable. You can run the commands as often as you want, trying various command-line options, without damaging the input files. You can end the pipeline at any point, pipe the output into less, and look at it to see if it has the expected form. This ability to inspect is great for debugging. You can write the output of one pipeline stage to a file and use that file as input to the next stage. This allows you to restart the later stage without rerunning the entire pipeline. MapReduceThe idea is similar to the Unix tools, but MapReduce do I&#x2F;O on distributed filesystem (like HDFS, Hadoop Distributed File System and GFS, Google File System).So as object storage services like Amazon S3, Azure Blob Storage, and OpenStack Swift. HDFS follows shared-nothing principle, making it not relying on special hardware (store with redundancy for fault tolerance). Consists daemon process to expose network service allowing file access. A central server called the NameNode keeps track of which file blocks are stored on which machine. MapReduce algorithm requires 2 callback functions: mapper (called once for every input record, generate k-v pairs from input, no state stored) and reducer (takes the k-v pair, iterates to produce output). Mapper and reducer can operate only on one record at a time, do not consider related states. Usually the application codes need to be moved to machines with task assigned first, like with k8s. Hash of key is used, to put k-v paris with the same key to the same reducer. The reducers are also (author configurably) partitioned. The key-value pairs must be sorted (by the mapper, to be performed in stages). After that, the reducers can connect to mappers and download the files of sorted k-v pairs, this is called shuffling. For k-v pairs with the same key from different mappers, they will also be merged before parsed by reducers. It should be noted that chained MapReduce is more like a series of command, where each command’s output is written to a temporary file (Hadoop writes to disk a lot, even unnecessary). Reduce-Side Joins and Grouping: MapReduce do not have indexes in usual sense, so it usually do full table scan (very expensive). Also, random-access request through network is either too slow or too expensive. A better approach may be taking a copy of the database and put it in the same distributed filesystems. Example: You may want to join user activity table with profile table. With such copies in HDFS, you can use map reduce to extrac (user_id, url) and (user_id, date_of_birth) with different mappers, and reduce them into (user_id, [url, date_of_birth, ...])This can be called Sort-merge joins, such tuples containing different fields but with same user_id will be adjacent. Secondary sort: sort such that the reducer always sees the record from the user database, followed by the activity events (in timestamp order) Since the reducer processes all of the records for a particular user ID in one go, it only needs to keep one user record in memory at any one time, and it never needs to make any requests over the network.This algorithm is known as a sort-merge join, since mapper output is sorted by key, from both sides of the join. By bringing related data together, we separated the physical network communication aspects of the computation (getting the data to the right machine) from the application logic (processing the data once you have it) Handling Skew (e.g., hotkeys) can also be important, as MapReduce has to wait till all mappers and reducers to finish in every round. We can detect hot keys with sampling, then let mappers send hotkey related records to serveral reducers (rather than one determined by hashing). Note that other inputs need to be replicated to all reducers which handle the hotkey as well.Or if we know the hot keys already, we can store them separately. Note that such reducers’ outputs need to be aggregated. Map-side JoinsWith certain assumption about the input data, Map-side Joins may optimize the process (while reduce-side joins need expensive sorting, copying and merging)If we can do that, then we can remove the reducer and have mapper writes into certain files. Broadcast hash joins: can be used when joining a large dataset with a small dataset (can fit in memory of each mapper). “Broadcast” reveals that each mapper of a partition of the large input can read the entire small input. An alternative is to store the small join input in a read-only index on the local disk (so we can handle a bit larger “small” set). Partitioned hash joins (bucketed map joins): If the inputs to the map-side join are partitioned in the same way, then the hash join approach can be applied to each partition independently. E.g., if we want to join the user profile and activity tables, then we can partition based on the last digit of user_id, because it works for both tables so that we can process on a smaller subset. Map-side merge joins: If input datasets are not only partitioned in the same way, but also sorted based on the same key, then the merge can be done by the mapper. Application of MapReduce: build search engine index (incremental update will be harder, involves merges segments), K-V pair as output (e.g., building dataset or database) It may not be a good practice Allowing MapReduce to write into database server (one record once) directly. Network request is slower, database’s performance can be impacted, cannot guarantee “atomicity” &#x2F; “clean all-or-nothing”. Better solution may be building new databases inside the batch jobs, write it as files to output DIR of distributed Filesystem. Once finished, make these files immutable and loaded in bulk into servers to serve read-only queries.If replication went wrong, we can easily switch back to old files. Diversity of storage: Hadoop would first dump data into HDFS, then figure out how to process that. This can make data available more quickly.This paradigm of worrying about schema later and allow data collection to be speeded up is known as “data lake“ or “enterprise data hub“. This makes data interpretation a problem just for consumer. Once it is modeled and formated, it can be imported into MPP (Massively Parallel Processing) data warehouse. However, MapReduce still faces severe performance issue in many scenarios and not used as widely, but more methods can be applied on top of Hadoop. Fault Tolerance of MapReduce Batch processes are less sensitive to faults than online systems, as tehy do not immediately affect users and can always be rerun.Failure of single map &#x2F; reduce task can be tolerated and can be retried.Data is frequently loaded to disk, because the data volume is big and for durable storage.These features makes MapReduce suitable for larger jobs. Retry at the granularity of an individual task may help, as rerun the whole task can be expensive. * The faults do not happen very frequently, however, batch processing jobs are usually with lower priority and can be terminated (preempted). This makes ability to recover valuable so that we can make sue of the “scraps” of computation power. Improvements On MapReduceMaterialization: writing intermediate state into files.For complex workflows using a lot of MapReduce jobs, such Materialization can be wasteful because we have to wait all preceding jobs to finish, and Mappers are often redundant (just read back the file written by Reducers). Spark, Tez and Flink aim at resolving the above issues, by treating the entire workflow as one job.They are known as dataflow engines as they explicitly model the flow of data through several processing stages. Work by repeatedly calling a user-defined function (operators) to process one record at a time on a single thread. They parallelize work by partitioning inputs They copy the output of one function over the network to become the input to another function. More flexible, not required to take strict roles of map and reduce. How Operator Connect To Inputs: Repartition and sort records by key Take several inputs and to partition them in the same way, but skip the sorting. For broadcast hash joins, the same output from one operator can be sent to all partitions of the join operator. Advantages: Expensive work like sorting only performed in places where it is actually required. No unnecessary map tasks. All joins and data dependencies in a workflow are explicitly declared, making locality optimization possible (e.g., place data producer and consumer tasks in the same node). Can keep intermediate state between operators in memory or local disk, rather than write to HDFS. Operators can start as soon as their input is ready, rather than waiting for the entire preceding stage. Existing JVM can be reused to run new operators. Fault Tolerance: Spark, Flink, and Tez avoid writing intermediate state to HDFS, so they take a different approach to tolerating faults: If a machine fails and the intermediate state on that machine is lost, it is recomputed from other data that is still available(a prior intermediary stage if possible, or otherwise the original input data, which is normally on HDFS). Framework should track how a given piece of data was computed—which input partitions it used, and which operators were applied to it.Spark: use Resilient Distributed Dataset (RDD) to track the ancestor of data.Flink: checkpoints operator state. * It is important that the computation is deterministic. It matters especially when some lost data is sent to the downstream operators (so that we ensure there is no contradictions between the old and new data).If it is non-deterministic, then downstream operators should be killed as well and rerun. Note that certain programming language may not guarantee any order when iterating over elements, and we should try to use pseudorandom numbers using a fixed seed. ** For expensive computation or small output, we can still materialize it as a file. We can tell that the tradeoff is like whether you want to store results in a temporary file, or pass it like a Unix pipe.It should be noted that Sorting operation must be first completed before passing to next operator (Flink tends to incrementally pass the output of an operator). Graphs and Iterative ProcessingWe may need to look at graphs in batch processing context, e.g. PageRank in recommendation engine. Dataflow engines (Spark, Flink, and Tez) typically arrange the operators in a job as a directed acyclic graph (DAG).Many graph algorithms are expressed by traversing one edge at a time, joining one vertex with an adjacent vertex in order to propagate information, repeat till some condition is met (transitive closure). Graph can be stored in a distributed filesystem, but the “repeative until done” cannot be expressed in plain MapReduce, since it only performs a single pass over the data, but should be implemented in an iterative way (but with very low efficiency, as the entire input dataset will be read though small part of the graph changed). The Pregel processing model has been introduced as an optimization of the above method, using Bulk Synchronous Parallel (BSP). One vertex can “send a message” to another vertex, and typically those messages are sent along the edges in a graph.In each iteration, a function is called for each vertex, passing it all the messages that were sent to it (via all the adjacent edges, much like a call to the reducer).In the Pregel model, a vertex remembers its state in memory from one iteration to the next, so the function only needs to process new incoming messages. * If no messages are being sent in some part of the graph, no work needs to be done. ** It should be noted that it is hard to partition that vertices are colocated on the same machine, a lot of cross-machine communication overhead. Thus we should place such algorithm on single machine if possible.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Data System","slug":"Data-Science/Data-System","permalink":"https://umiao.github.io/categories/Data-Science/Data-System/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Data System","slug":"Data-System","permalink":"https://umiao.github.io/tags/Data-System/"},{"name":"Designing Data-Intensive-Applications","slug":"Designing-Data-Intensive-Applications","permalink":"https://umiao.github.io/tags/Designing-Data-Intensive-Applications/"}]},{"title":"Designing Data-Intensive-Applications-Note-17","slug":"Designing-Data-Intensive-Applications-Note-17","date":"2024-04-27T18:27:31.000Z","updated":"2024-05-05T19:01:24.934Z","comments":true,"path":"2024/04/27/Designing-Data-Intensive-Applications-Note-17/","link":"","permalink":"https://umiao.github.io/2024/04/27/Designing-Data-Intensive-Applications-Note-17/","excerpt":"Discussion on challenges with consensus.","text":"Discussion on challenges with consensus. Consensus: Get all nodes to agree on something, e.g., the election of new leader. If two nodes both believe that they are the leader, that situation is called split brain which can be prevented by consensus. Eventually consistency &#x2F; coverage is a weak guarantee.Stronger consistency guarantee may be possible, but can be less fault-tolerant or with worse performance. LinearizabilityDatabase works as if there is only one replica, no replication lag.Also known as atomic consistency, strong consistency, immediate consistency, or external consistency.It is a recency guarantee, means that we always able to see the latest value written to the database. Considering that we have a write request to the database, while there are other client requesting the database to get the latest value.Any read operations that overlap in time with the write operation might return either old or new value, because we don’t know whether or not the write has taken effect at the time when the read operation is processed.These operations are concurrent with the write. However, that is not yet sufficient to fully describe linearizability: if reads that are concurrent with a write can return either the old or the new value, then readers could see a value flip back and forth between the old and the new value several times while a write is going on. That is not what we expect of a system that emulates a “single” copy of the data. In order to enforce Linearizability, we have to imagine that for some point in time, the above value flips from old value to new value. All subsequent reads must also return the new value, even if the write operations has not yet completed.Also, we can add compare-and-set cas atomic operator, to make the read-write cycle works correctly. (Before write the value $v_{new}$, we can read the value $v_{old}$ which is previously read). Otherwise, return an error. Solution: We assume each operator to happen at certain moment of time. We place markers on these moments, and we require such markers are joined up in a sequential order, and the result must be a valid sequence of reads and writes for a register (every read must return the value set by the most recent write) (new value should take effect, even the write is not completed). The requirement of linearizability is that the lines joining up the operation markers always move forward in time (from left to right), never backward. This requirement ensures the recency guarantee we discussed earlier: once a new value has been written or read, all subsequent reads see the value that was written, until it is overwritten again. It is possible (though computationally expensive) to test whether a system’s behavior is linearizable by recording the timings of all requests and responses, and checking whether they can be arranged into a valid sequential order Linearizability Versus Serializability: Serializability is an isolation property of transactions, each may read &#x2F; write multiple objects. It guarantees that transactions behave the same as if they had executed in some serial order (OK to be different from the order which transactions were actually run) Linearizability is a recency guarantee on reads and writes of a register &#x2F; single object. When the above two are provided, it is called strict serializability or strong one-copy serializability (strong-1SR). * Implementations of serializability based on two-phase locking (2PL) or actual serial execution are typically linearizable.** Serializable snapshot isolation is not linearizable, because read is made from consistent snapshot, does not include writes that are more recent than the snapshot. One way of electing a leader is to use a lock: every node that starts up tries to acquire the lock, and the one that succeeds becomes the leader. The lock must be linearizable to reach consensus. Distributed locking can be used in more granular level, e.g., in Oracle Real Application Clusters (RAC), lock is on disk page level, with multiple nodes sharing access to the same disk storage system. Uniqueness guarantees is similar to lock (or compare-and-set), e.g., to solve naming conflicts. This can also be used to enforce constraints, like make sure bank account balance being non-negative, or inventory not below zero.Such constraints do require linearizability. While foreign key or attribute constraints do not need such property. Cross-channel timing dependencies: Race conditions may be possible between different channels, without linearizability.E.g., a server would first store the full size picture and send message to generate a resized picture (thumbnail). If the latter is faster, it may receive NULL result, or a old-versioned picture.This is due to me have two different communication channels between the web server and the resizer: the file storage and the message queue. Implementations of Linearizability Single Replica: linearizable, but vulnearable to data loss and failure Single-leader replication: potentially linearizable if you read from the leader or synced followers. However, may not be linearizable due to design (snapshot isolation) or concurrency bugs. However, we need the assumption that everyone knows who is the leader. Otherwise we may have delusional leader continues to serve requests. Consensus algorithms: Linearizable. Multi-leader replication: Not Linearizable. Because usually the replication is done asynchronously, may need conflict resolution. Leaderless replication: probably not linearizable. Such Dynamo style thing sometimes claimed that you can obtain strong consistency by requiring quorum reads and writes ($w + r &gt; n$) but this might not be true. Last write wins conflict resolution methods based on time-of-day clocks are almost certainly nonlinearizable, because clock timestamps cannot be guaranteed to be consistent with actual event ordering due to clock skew. Sloppy quorums also ruin any chance of linearizability. We can construct samples that even with quorum reads and writes ($w + r &gt; n$) met, the execution can be non-linearizable: Consider that $n&#x3D;3, w&#x3D;3, r&#x3D;2$, and we have replica A, B and C. First new value is written to replica A, then client X reads A, B to get the new value; Then client Y reads B, C to get the old value. This is non-linearizable as Y reads after X, but get the older value. Safest to assume a leaderless system with Dynamo-style replication does not provide linearizability.With reduced performance, we can make it linearizable: a reader must perform read repair synchronously, before returning results to the application; a writer must read the latest state of a quorum of nodes before sending its writes.Also, a linearizable compare-and-set operation cannot, because it requires a consensus algorithm The CAP theorem： If your application requires linearizability, and some replicas are disconnected from others, then they must wait until the network is fixed, or return an error.Otherwise (no linearizability needed), we can allow each replica to process requests indenpendently even disconnected (e.g., in a multi-leader setting) * CAP is sometimes presented as Consistency, Availability, Partition tolerance: pick 2 out of 3. However, network partition just happens as a kind of fault, and you do not have a choice. When this happens, we can provide either linearizability or total availability.CAP cannot help us best understand the system (does not consider network delays, dead nodes, or other trade-offs) and better avoided. Few systems are actually linearizable in practice, not even RAM on a modern multi-core CPU, because each core has its own memory cache and store buffer. Memory access goes to cache by default, then written out to memory asynchronously. Thus, many systems give up providing linearizability, due to performance concern.It is prove that if you want linearizability, the response time of read and write requests is at least proportional to the uncertainty of delays in the network. However, delays can be long in a network with highly variable delays. OrderingCausality imposes an ordering on events: cause comes before effect.Something has to be created before updated, A and B should not have causal link between them to become concurrent. “Consistent” read also means that snapshot should contains answer as well as the question it trying to answer. If a system obeys the ordering imposed by causality, we say that it is causally consistent. The causal order is not a total order (arbitrary two elements can be compared). $\\{a, b\\}$ cannot be compared with $\\{b, c\\}$, being incomparable (unless one is subset of another) In a linearizable system, we have a total order of operations.Therefore, no concurrent operation exists in such system. If there is any, we can only have a partial order such that they are incomparable (not causally related).Linearizability is stronger than preserving causality. Version control system like Git works as a graph of causal dependencies, where timeline branches and merges. Conclusion: Causal consistency is the strongest possible consistency model that does not slow down due to network delays, and remains available in the face of network failures.Implementation causality consistency requires tracking causal dependencies across the entire database, can be done with version vectors &#x2F; logical clock. Noncausal sequence number generatorIf there is not a single leader (due to multi-leader &#x2F; leaderless database), it is less clear about how to generate sequence numbers for operations. Potential solutions: Each node generate its own independent set of sequence numbers, e.g., only generate multiple of $k$ as version number, or reserve bits in binary representation as unique identifier Attach a timestamp from a time-of-day clock (e.g., last write win). Preallocate blocks of sequence numbers, e.g. (1-1000) just for node A. However, these methods cannot capture the ordering of operations correctly. Lamport timestampsGenerating sequence numbers that is consistent with causalityRepresent timestamp as (counter, node ID) so that each timestamp is unique. counter is used to determine order, and node ID work as tie-breaker.Every node and every client keeps track of the maximum counter value it has seen so far, and includes that maximum on every request.When a node receives a request or response with a maximum counter value greater than its own counter value, it immediately increases its own counter to that maximum (enforcing the timestamp to be in consistent with the causality). Difference between version vectors: Lamport time‐stamps are more compact, but cannot tell whether two operations are concurrent or whether they are causally dependent (total ordering is always enforced). Total order broadcastIt should be noted that Lamport timestamps is not sufficient in some tasks like username creation, because we cannot determine whether concurrent requests are asking for the same name, and which one should win (need knowledge from other nodes). We need not only total ordering of operations, but also when that order is finalized (maybe with total order broadcast). Partitioned databases with a single leader per partition often maintain ordering only per partition, which means they cannot offer consistency guarantees (e.g., consistent snapshots, foreign key references) across partitions. Total order broadcast or Atomic Broadcast: used to scale the system if the throughput is greater than a single leader can handle, and also how to handle failover if the leader fails.Usually described as a protocol for exchanging messages between nodes.Requires: Reliable delivery (No messages are lost: if a message is delivered to one node, it is delivered to all nodes.) and Totally ordered delivery (Messages are delivered to every node in the same order). State machine replication: If every message represents a write to the database, and every replica processes the same writes in the same order, then the replicas will remain consistent with each other (aside from any temporary replication lag). An important aspect of total order broadcast is that the order is fixed at the time the messages are delivered: a node is not allowed to retroactively insert a message into an earlier position in the order if subsequent messages have already been delivered.This fact makes total order broadcast stronger than timestamp ordering We can view it as a way of creating a log, delivering a message is like appending to the log.Since all nodes must deliver the same messages in the same order, all nodes can read the log and see the same sequence of messages. Total order broadcast is also useful for implementing a lock service that provides fencing tokens. Every request to acquire the lock is appended as a message to the log, and all messages are sequentially numbered in the order they appear in the log.The sequence number can then serve as a fencingtoken, because it is monotonically increasing. In ZooKeeper, this sequence number is called zxid Total order broadcast: asynchronous: messages are guaranteed to be delivered reliably in a fixed order, but there is no guarantee about when a message will be deliveredEquivalent to consensus. linearizability: Recency guarantee: a read is guaranteed to see the latest value written. We can try to use Total order broadcast to implement a linearizable storage to resolve the username setting task: Append a message to the log, decalring the name you want to claim Read the log, wait for the message you appended to be delivered back to you. Check for any messages claiming the user name you want; If the first is your own message, then it is succesful, otherwise it is claimed by others. This is like a compare-and-set, each name initially with value NULL and you want to assigny your userID as value of the name. It is only legal to overwrite name with value NULL Because log entries are delivered to all nodes in the same order, if there are several concurrent writes, all nodes will agree on which one came first. Choosing the first of the conflicting writes as the winner and aborting later ones ensures that all nodes agree on whether a write was committed or aborted. A similar approach can be used to implement serializable multi-object transactions on top of a log. Ensures linearizable writes, not linearizable reads. Data read from a store which is asynchronously updated from the log can be stale (this procedure provides sequential consistency &#x2F; timeline consistency, slightly weaker than linearizability). To make reads linearizable: Append a message to a log, read the log, performing the actual read when the message is delivered back to you so reads can be sequenced. If the log allows you to fetch the position of the latest log message in a linearizable way, you can query that position, wait for all entries up to that position to be delivered to you, and then perform the read. (ZooKeepter’s sync() operation) Read from a replica that is synchronously updated on writes,and is thus sure to be up to date. We can also build total order broadcast from linearizable storage.With a linearizable register (or atomic compare-and-set), you can increment and get the linearizable integer, and attach it in the message as sequence number. You can then send the message to all nodes (resend lost messages) and the recipents will deliver the messages consecutively.Unlinke Lamport timestamps, the numbers must form sequence with no gaps, making it total order (you will know that you should wait for No.5 messages even you received No.4 and No.6). In order to restore the register value when network connections fails, you inevitably get a consensus algorithm.Linearizable compare-and-set (or increment-and-get) register and total order broadcast are both equivalent to consensus Consesus is important in leader election (of database with single-leader replication) and Atomic commit (in database cross nodes or partitions). FPL result: No algorithm can reach consensus with risk that a node may crash, in async system model and no clock and timeout is allowed. With timeouts or other methods (even use of random numbers) to detect crashed nodes, then consensus is solvable. Atomic Commit and Two-Phase Commit (2PC)Atomicity prevents failed transactions from littering the database, and makes sure secondary index stays consistent with the primary data. For single node, a database usually makes the transaction’s write durable (writeahead log, e.g.) then append a commit record to the log on disk (2 phase commit). If node is crashed before commit record is written, then the writes are rolled back. For multiple nodes, we need to ensure all nodes either commit or abort. It is not allowed that some commit succeed and some fail.Once committed, a commit is irrevocable.However, we can undo a commit by another independent one: compensating transaction. Implementation:2PC introduces a coordinator (also known as transaction manager) to arrange the commit on all nodes.In 1st phase: Coordinator send request to all nodes and ask if they are able to commit. If anyone responses “no”, then coordinator sends an abort request to all nodes.In 2nd phase: Coordinator send commit request, and commit actually takes place (and broadcasted). Details of 2PC implementation: Before begin a distributed transaction, application needs to request a globally unique transaction ID. The application begins a single-node transaction on each of the participants (to do all reads and writes), and attaches the globally unique transaction ID. If anything go wrong, coordinator or any participant can abort. The coordinator sends a prepare request to all participants, tagged with the global transaction ID, when the application is ready to commit. If any request fails &#x2F; times out, the coordinator sends an abort request for that transaction ID to all nodes. When a participant receives the prepare request, it makes sure that it can definitely commit the transaction under all circumstances. This includes writing all transaction data to disk (a crash, a power failure, or running out of disk space is not an acceptable excuse for refusing to commit later), and checking for any conflicts or constraint violations. By replying “yes” to the coordinator, the node promises to commit the transaction without error if requested. In other words, the participant surrenders the right to abort the transaction, but without actuallycommitting it. When the coordinator has received responses to all prepare requests, it makes a definitive decision on whether to commit or abort the transaction (committing only if all participants voted “yes”). The coordinator must write that decision to its transaction log on disk so that it knows which way it decided in case it subsequently crashes. This is called the commit point. Once the coordinator’s decision has been written to disk, the commit or abort request is sent to all participants. If this request fails or times out, the coordinator must retry forever until it succeeds. There is no more going back: if the decision was to commit, that decision must be enforced, no matter how many retries it takes. If a participant has crashed in the meantime, the transaction will be committed when it recovers—since the participant voted “yes,” it cannot refuse to commit when it recovers. Thus, the protocol contains two crucial “points of no return”: when a participant votes “yes,” it promises that it will definitely be able to commit later (although the coordinator may still choose to abort); and once the coordinator decides, that decision is irrevocable.Those promises ensure the atomicity of 2PC.(Single-node atomic commit lumps these two events into one: writing the commit record to the transaction log.) Coordinator failure:If one of participant’s prepare requests fail or time out, coordinator aborts the transaction.if any of the commit or abort requests fail, the coordinator retries them indefinitely (that is why 2PC is called blocking atomic commit protocol). However, if coordinator failed, participants’s state is called in doubt or uncertain, as they have to wait forever (cannot abort).In 2PC protocol, the only way is to wait coordinator to recover. The commit point of 2PC comes down to a regular single-node atomic commit on the coordinator. 3PC is proposed to make an atomic commit protocol nonblocking, but we need assumption that having a network with bounded delay and nodes with bounded response times.However, in a system without such assumption, 3PC cannot guarantee atomicity (without a perfect failure detector)That is why 2PC is still in use Heterogeneous distributed transactionsDatabase-internal distributed transactions are easier to deal with, like distributed databases. All nodes (participants) run the same database software. They do not have to be compatible with other system, able to use any protocol and do specific optimization. Heterogeneous distributed transactions are harder, e.g., message brokers. Atomic commit must be ensured. Exactly-once message processing: A message from a message queue can be acknowledged as processed if and only if the database transaction for processing the message was successfully committed(atomically commit the message acknowledgment and database writes in a single transaction).E.g., when a message processing is associlated with sending an email. If either one failed, we can roll back both and safely retry. XA transactionsX&#x2F;Open XA (short for eXtended Architecture) is a standard for implementing twophase commit across heterogeneous technologies。The essence is C API for interfacing with a transaction coordinator.Assumes that your application uses a network driver or client library to communicate with other participants &#x2F; services. If driver supports XA, then it calls the XA API to figure out if an operation is part of a distributed transaction, if so, sends necessary information to the database server. Driver also exposes callbacks through which the coordinatro can ask the participant to prepare, commit or abort. The transaction coordinator implements the XA API.However, corrdinator is usually just a library loaded into the same process as the application issuing the transaction rather than a seperate service.This is bit tricky as coordinator can go with died server. The reason that we really care about transaction in doubt is that they might be holding locks (if any exlusive lock or 2PL is used).These locks will be holded until coordinator recovers. In practice, orphaned in-doubt transactions do occur.Coordinator cannot decide their outcome for whatever reason (transaction log may be lost or corrupted). They will sit forever in the database, hold locks and block other transactions. Only solution is for an administrator to manually decide whether to commit or roll back the transactions (a lot of work, and breaks the 2PC constraints) XA implementation may have emergency escape hatch called heuristic decisions: allowing a participant to unilaterally decide to abort or commit an in-doubt transaction without a definitive decision from the coordinator. Heuristic here is a euphemism for probably breaking atomicity, since it violates the system of promises in two-phase commit.Thus, heuristic decisions are intended only for getting out of catastrophic situations, and not for regular use. Limitations of distributed transactions Limitation of XA: transaction coordinator is itself a kind of database (in which transaction outcomes are stored)If it runs on only one single machine, it is a single point of failure and do not even have high availability. Many server-side applications are developed in a stateless model. Coordinator’s logs will break such model. XA is a lowest common denominator, to be compatible with a wide range of data system. Cannot detect deadlocks across different systems (requiring knowledge on locks), does not work with Serializable Snapshot Isolation, SSI (require protocol to identify conflicts across different systems). Even for internal distributed transactions, where we can implement SSI, failures can still be amplified by 2PC as we require all participants to respond. Fault-Tolerant ConsensusSome properties we need consensus to have: Uniform agreement: No two nodes decide differently. Integrity: No node decides twice. Validity: If a node decides value v, then v was proposed by some node. Termination: Every node that does not crash eventually decides some value (usually, a majority is required as the quorum). The idea is that everyone decides on the same outcome, and once you have decided, you cannot change your mind.Having a dictator can resolve most properties (except termination), but lacks fault tolerance.In fact, the termination property requires that a consensus algorithm must make progress.Any algorithm that has to wait for a node (like 2PC) will NOT have the termination property. * It is possible to make consensus robust against Byzantine faults as long as fewer than one-third of the nodes are Byzantine-faulty The best-known fault-tolerant consensus algorithms are Viewstamped Replication (VSR), Paxos, Raft and Zab. They decide on a sequence of values, which makes them total order broadcast algorithms.Remember that total order broadcast requires messages to be delivered exactly once, in the same order, to all nodes. This is equivalent to repeated rounds of consensus (decide what to send next, and decide on the next message to be delivered in total order)Total order broadcast is more efficient than doing multiple value-at-a-time consensus. Epoch numbering and quorumsConsensus protocols internally uses a leader, but leader is a result of consensus. Such protocols do not guarantee that the leader is unique. We can make a weaker guarantee with epoch numbering (ballot number &#x2F; view number &#x2F; term number), and guarantee that within each epoch, the leader is unique. Every time the current leader is thought to be dead, a vote is started among the nodes to elect a new leader. This election is given an incremented epoch number, and thusepoch numbers are totally ordered and monotonically increasing. If there is a conflict between two different leaders in two different epochs (perhaps because the previous leader actually wasn’t dead after all), then the leader with the higher epoch number prevails (so we remove the previous leader even it recovers). For every decision that a leader wants to make, it must send the proposed value to the other nodes and wait for a quorum (usually a majority of nodes) of nodes to respond in favor of the proposal. A node votes in favor of a proposal only if it is not aware of any other leader with a higher epoch. Have two rounds of voting: once to choose a leader, and a second time to vote on a leader’s proposal. The key insight is that the quorums for those two votes must overlap: if a vote on a proposal succeeds, at least one of the nodes that voted for it must have also participated in the most recent leader election. The leader got to know if it is still holds the leadership. Consensus algorithms define a recovery process by which nodes can get into a consistent state after a new leader is elected, ensuring that the safety properties are always met. Limitations of Consensus: Vote on proposal is actually synchronous replication, and can result in data loss. Needs strict majority to operate. Most consensus algorithms assume a fixed set of nodes participate in voting, means you cannot just add &#x2F; remove nodes in the cluster (may be resolved by dynamic membership). Sensitive to network problems, relies on timeout for failure detection. Consesus Algorithm Projects like ZooKeepter are very useful for distributed coordination. It is limited as it can only hold small amount of data able to fit in memory, and replicated across all node using fault tolerant total order broadcast (also Linearizable atomic operations are implemented to implement lock). Failure detection: Clients maintain and long-lived session on ZooKeeper servers to exchange heartbeats, if it cease for longer than the session timeout, session is declared to be dead and locks are released (ZooKeeper calls these ephemeral nodes). Change notifications: One client read locks and values that were created by another client, and can also watch them for changes. Then it can know if a client joins the cluster or fails, without frequently poll to find out. Only the linearizable atomic operations really require consensus. Use Cases: ZooKeeper can be used for leader election, partition assigning, rebalancing and taking over failed node’s work (maybe even service discovery). It runs on fixed number of nodes, performs its majority votes among them and supports potentially large number of clients. Normally, the kind of data managed by ZooKeeper is quite slow-changing (not suitable to store runtime state of application)","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Data System","slug":"Data-Science/Data-System","permalink":"https://umiao.github.io/categories/Data-Science/Data-System/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Data System","slug":"Data-System","permalink":"https://umiao.github.io/tags/Data-System/"},{"name":"Designing Data-Intensive-Applications","slug":"Designing-Data-Intensive-Applications","permalink":"https://umiao.github.io/tags/Designing-Data-Intensive-Applications/"}]},{"title":"Designing Data-Intensive-Applications-Note-16","slug":"Designing-Data-Intensive-Applications-Note-16","date":"2024-04-21T16:54:34.000Z","updated":"2024-05-05T19:02:16.589Z","comments":true,"path":"2024/04/21/Designing-Data-Intensive-Applications-Note-16/","link":"","permalink":"https://umiao.github.io/2024/04/21/Designing-Data-Intensive-Applications-Note-16/","excerpt":"Discussion on challenges with distributed systems.","text":"Discussion on challenges with distributed systems. Faults and Partial FailureWe expect outcomes of program running on a single computer either returns correct result, or crash (without returning wrong results)Idealized system model: operates with perfection, CPU instruction does the same thing, data in memory and disk remains intact and not randomly corrupted. However, in distributed system, we can easily find partial failure that some parts of the system that are broken in unpredicatable ways but remaining parts working fine.You may not even know if something succeeded or not, as the time it takes for message traveling across netowrk is nondeterministic Large-scale computing systems’ design logic is between High-performance Computing (HPC) and Cloud Computing. The prior is more like a single node computer (if failure happens, fix it and restart entire cluster workload from checkpoint [can be expensive]]). Also, it has node communication through shared memory and Remote Direct Momoery Acess (RDMA), use specialized hardware and more reliable. Also, having its nodes close together. The latter focuses on high availability, based on IP and Ethernet, arranged in Clos topologies to provide high bisection bandwidth.System with fault tolerance can do things like rolling upgrade, or kill-n-restart virtual node. Fault Tolerance: It is important to consider wide range of possible faults, even fairly unlikely ones. Fault handling mech should be part of the software design. You can construct a more reoliable system from a less reliable underlying base. E.g., using error-correcting codes to detect random bit error during transmission, and use Transmission Control Protocol (TCP) to form more reliable transport layter on top of IP (Internet Protocol)However, these methods have their own limit of how much error can be detected and handled, and TCP cannot resolve network latency issue. Unreliable Network: Usually, one node can send a message &#x2F; packet to another node, but there is no guarantee when it will arrive, or whether it will arrive at all.Sender cannot tell if the packet was delivered, because recipent’s response message can be lost. Usual way of handling this is setting a timeout.* Redundancy in data center setting cannot prevent human error, which is a main reason for outage. Detecting Faults: Many systems need to detect faulty nodes, e.g., load balancer stop sending request to dead node, and new leader may need to be elected in single-leader setting database.However, it is still hard to get response, or get to know how much data is processed before failure happened. If node’s operating system is still running, another node may be able to take over. Timeout and Unbounded Delays Timeout can be the only sure way of detecting a fault, but the limit is hard to set. Falsely declaring a node dead is problematic, may ended up performing actions twice. Also, extra load would be placed on other nodes, and cause cascading failure (extremely all nodes can be declared dead and stop working). If delivery time is bounded by $d$ and request processing time is bounded by $r$, then we can set timeout limit as $2d+r$. Unfortunately, most systems have unbounded delays. Network Congestion: Multiple different nodes simultaneously try to send packets to the same destination, the network switch must queue them up and feed them into the destination network link one by one. Then packet may need to wait for a slot.* Packet may be dropped if switch queue is full. Also, even a packet reaches the destination machine, we may still need to be queued by the operating system if the system is busy, or need to wait to get a CPU core in a virtualized environment.TCP performs flow control (also known as congestion avoidance or backpressure), in which a node limits its own rate of sending in order to avoid overloading a network link or the receiving node (additional queueing will be needed). * TCP would consider a packet to be lost if it is not acknowledged within the timeout. Application will not see the loss and retreansmission, but will see the latency. TCP v.s. UDP: In latency-sensitive application, like video conferencing, use UDP rather than TCP because delayed data will be worthless. UDP does not perform flow control and does not retransmit loss packets. Human can ask for retransmission instead. Determine timeout: Queueing delays have especially wide range when a system is close to its max capacity and long queue can be built.You can only choose timeouts experimentally to learn the distribution of expected variability of delays. More ideally, systems can continually measure response times and their variability (jitter), and automatically adjust timeouts according to the observed response time distribution.This can be done with a Phi Accrual failure detector Phi Accural Failure DetectorTraditionally, we can use heartbeat for failure detector. Rather than setting the heartbeat timeout to a constant value, we assume the heartbeat message matches certain type of probability model. E.g., the interval obeys the normal distribution.Then, we can use the history data in the sliding window, to estimate the probability parameters with maximum likelihood estimation and calculates the probability of receiving heartbeat message at this moment. If the node is invalid, we want the $\\lim_{t \\rightarrow \\infty}\\phi(t)&#x3D;\\infty$ $\\exists t_0$ such that $\\forall x_1 \\ge x_2 \\ge t_0, \\phi(x_1) \\ge \\phi(x_2)$. If the node is valid, then $\\phi(t)$ is bounded. If the node is valid, then $\\exists t_0$ such that $\\forall x \\ge t_0, \\phi(x) &#x3D;0$. $\\phi(t_{now})$ is defined as $-\\log_{10}(P_{later} (t_{now} - T_{last}) )$$P_{later} (t) &#x3D; \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\int_t^{+\\infty} e^{-\\frac{(x-\\mu)^2}{2\\sigma ^2}}dx $ Here, $\\mu$ and $\\sigma$ can be estimated with the data in the sampling window. Each node &#x2F; subprocess can use their own threshold to compare with (other node’s) $\\phi(t)$ shared by the failure detector. * When you make a call over the telephone network, it would establish a fixed, guaranteed amount of bandwidth through the entire route between callers. This kind of network is synchronous: even as data passes through several routers, it does not suffer from queueing, because the 16 bits of space for the call have already been reserved in the next hop of the network. And because there is no queueing, the maximum end-to-end latency of the network is fixed. We call this a bounded delay. The packets of a TCP connection opportunistically use whatever network bandwidth is available.They are packet switching, instead of circuit-switched networks, as they are optimized for bursty traffic. It can be hard to guess a proper bandwidth allocation. TCP can adapt the rate dynamically. Static resource allocation can provide latency guarantees, but at the cost of reduced utilization. Unreliable ClocksUsually each machine has its own clock, which makes determining the order difficult in case of concurrency.This makes figuring request timeout &#x2F; getting timestamps &#x2F; scheduling services… more challenging. We can synchronize clocks to some degree with Network Time Protocol (NTP).NTP uses hierarchical, semi-layered system of time sources. Each level of hierachy is termerd a stratum, assigned with a number starting from 0. A server synchronized to a stratum $n$ server runs at stratum $n+1$.* Stratum is not always an indication of quality or reliability Stratum 0: High-precision timekeeping devices such as atomic clocks, GNSS (including GPS) or other radio clocks, Stratum 1: Synced within a few microseconds of stratum 0 devices. Stratum 2: Synced with the above level. The NTP algorithms on each computer interact to construct a Bellman–Ford shortest-path spanning tree, to minimize the accumulated round-trip delay to the stratum 1 servers for all the clients. The upper limit for stratum is 15; stratum 16 is used to indicate that a device is unsynchronized Example: We will have 4 timestamps $T_1, … T_4$. Here, client send a request on $T_1$, it reaches the server on $T_2$, response is sent on $T_3$ and arrives on $T_4$.We can estimate $delay &#x3D; \\frac{T_4 - T_1 - (T_3 - T_2)}{2}$, and the offset satisfies: $\\text{offset} + T_4 &#x3D; T_3 + \\text{delay}$, so we can figure out the offset and adjust the clock. Modern computers have at least 2 kinds of clocks: a time-of-day clock (returns date and time according to some calendar, known as wall-clock time, like seconds since the epoch:midnight on 1st Jan 1970, might jump back) and a monotonic clock (for measuring a duration, never jumps back, the unit might be in nano-seconds). NTP may adjust the frequency at which the monotonic clock moves forward, if the local quartz is moving faster or slower (allow error up to 0.05%). Such error is tolerable in distributed systems. There lies some challenges in getting the clock synchronized: Quartz clock in a computer can drift, depending on the temperature. It may be a 17 seconds drift for a clock resynchronized once a day. Local clock may refure to synchronize if it differs too much from an NTP server, or local clock be forcibly reset. NTP synchronization can only be as good as the network delay. Itself may go wrong as well. Leap seconds result in a minute has 59 &#x2F; 61 seconds. NTP server can perform the leap second adjustment gradually over the course of a day (called smearing) Hardware clock can be virtualized in virtual machines, and each VM can pause for tens of miliseconds and clock jumps forward. You cannot trust user device’s clock setting. Inaccurate clock can casue lost of updates, under setting of Last Write Wins.A node with lagging clock cannot overwrite values previously written by a node with a fast clock, casuing data be silently dropped.LWW cannot distinguish truly concurrent writes (writes are not aware of each other) and writes that occurred sequentially in quick succession (writers are aware of others).Clock reading should be viewed as a range within a confidence interval, rather than a point in time. We may need tie breaker, if two nodes generate the same timestamp.Logical clocks based on incrementing counters may be a better solution, only measuring the relative ordering of events. Synchronized clocks for global snapshotsThe most common implementation of snapshot isolation requires a monotonically increasing transaction ID.On single node, this can be generated by a counter.However, it is challenging for database distributed across many machines &#x2F; data centers, because transaction ID must reflect causality, but we have clock accuracy concern. TrueTime API can report clock’s confidence interval, and we can use it to determine if two time reading overlap. We can wait the length of the confidence interval before committing a read-write transaction.Above is used in Google only Process PausesIn single leader setting database, leader has to know if it is not declared by others and safely accept writes.one options is let leader obtain a lease from other nodes, which is similar to a lock with a timeout. If the leader dies, it will not renew the lease and another node can take over. However, if we want to use a timer in the implementation, we will have the clock issue (if different nodes’ clocks go out-of-sync). Even we use a local monotinic clock, it is still possible that an unexpected pause happens after the leader checked that it still hold the lease. Another node may become the leader during this time, but the old leader is not aware of that. Potential Reasons of such pause: (Stop the world) Garbage Collection, may stop all running threads. [One emerging idea is to treat GC pause like brief planned outages of a node, used in some latency-sensitive financial trading systems] Suspended virtual machine (to migrate to another node, for example). In end-user devices like latops, execution can be suspended. Context Switch between threads or virtual machines. CPU time spent in other virtual machines is known as steal time. Slow I&#x2F;O, or delay caused by paging (swaping to disk). Servers can disable paging to avoid thrashing (spend most of the time swapping pages) You can’t assume anything about timing, because arbitrary context switches and parallelism may occur (just like making multi-threaded code).For distributed systems, we do not have tools like mutexes, semaphores, atomic counters, lock-free data structures, blocking queues, and so on. Response time guarantees: In some systems (like rocket, called hard real-time systems), we have a deadline by which the software must respond. In embedded systems, real-time means that a system is carefully designed and tested to meet specified timing guarantees in all circumstances.In network, real-time means servers pushing data to clients and stream processing without hard response time constraints Providing real-time guarantees in a systems can be challenging, as it requires: a real-time operating system (RTOS) that allows processes to be scheduled with a guaranteed allocation of CPU time in specified intervals is needed; library functions must document their worst-case execution times; dynamic memory allocation may be restricted or disallowed entirely (real-time garbage collectors exist, but the application must still ensure that it doesn’t give the GC too much work to do); and an enormous amount of testing and measurement must be done. It should be noted that even if a node believes that it is leader (or hold the lease, etc), that doesn’t necessarily mean a quorum of nodes agrees!* We should not do the validation only on application end. We need the resource &#x2F; data itself in checking tokens. Fencing: Every time the lock server grants a lock or lease, it also returns a fencing token, which is a number that increases every time a lock is granted (e.g., incremented by the lock service). We can then require that every time a client sends a write request to the storage service, it must include its current fencing token.If server ever processed a write with a higher token number, it rejects the write with old token. If ZooKeeper is used as lock service, the transaction ID zxid or the node version cversion can be used as fencing token. Since they are guaranteed to be monotonically increasing Byzantine FaultsFencing tokens can work well (with honest but unreliable assumption). However, if it is not honest, it could fake such token and cause Byzantine fault (reaching consensus in this untrusting environment). The problem needs $n$ generals who need to agree, but we have some traitors among them sending fake messages.A system is Byzantine fault-tolerant if it can tolerate nodes which are malfunctioning and not obeying the protocol, or if malicious attackers are interfering with the network. We can safely assume there is no Byzantine fault in most datacenters &#x2F; databases. Such solution can be complicated ted and need support from hardware level.Usually, the server has central authority to determine what kind of behavior is allowed and do protection with authentication, access control, encryption, firewalls, and so on. Also, if you deploy same software to all nodes, Byzantine fault-tolerant algorithm may not help, because we expect the error to be independent so that we can some nodes functioning correctly. Timing Assumption Model Synchronous model: bounded network delay, bounded process pauses,and bounded clock error. Not very realistic as unbounded delays can occur. Partially synchronous model: Behaves like a synchronous system most of the time, but exceeds the bounds sometimes. Asynchronous model: No any timing assumptions, not even have a clock. Common Models of System (Node) Failure: Crash-stop faults: Only crash may happen, then the node gone forever. Crash-recovery faults: May recover after sometime if crashed. Nodes are assumed to have stable storage (i.e., non-volatile disk storage), while the in-memory state is assumed to be lost. Usually most useful. Byzantine (arbitrary) faults: Node can do anything, including being dishonest We may be interested in the following properties under the above failure models: Correctness: We can write down the properties we want a distributed algorithm to have, to define correctness. Safety and liveness: Usually we can distinguish liveness properties from safety properties, as prior usually include the word “eventually” in their definition. Safety means nothing bad happens, but liveness means something good eventually happens. Formal Definition: If safety property is viloated, we can point out a particular time at which it was broken, and such violation cannot be undone. For liveness property, it may not hold at some point in time, but there is always hope that it may be satisfied in the future. For distributed algorithms, it is common to require safety properties always hold (to ensure it does not return a wrong result in terms of failure). For liveness properties, we can make caveats and state that only if majority of nodes have not crashed, we can say a request needs to receive a response.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Data System","slug":"Data-Science/Data-System","permalink":"https://umiao.github.io/categories/Data-Science/Data-System/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Data System","slug":"Data-System","permalink":"https://umiao.github.io/tags/Data-System/"},{"name":"Designing Data-Intensive-Applications","slug":"Designing-Data-Intensive-Applications","permalink":"https://umiao.github.io/tags/Designing-Data-Intensive-Applications/"}]},{"title":"Designing Data-Intensive-Applications-Note-15","slug":"Designing-Data-Intensive-Applications-Note-15","date":"2024-03-29T20:52:54.000Z","updated":"2024-05-05T19:02:35.138Z","comments":true,"path":"2024/03/29/Designing-Data-Intensive-Applications-Note-15/","link":"","permalink":"https://umiao.github.io/2024/03/29/Designing-Data-Intensive-Applications-Note-15/","excerpt":"Discussion on transaction related topics.","text":"Discussion on transaction related topics. A lot of things may go wrong (database software and hardware &#x2F; application &#x2F; network &#x2F; clients racing &#x2F; dirty data &#x2F; concurrency), and that is why we need transactions: To group reads and writes together into a logical unit(commit), succeeds or fails (abort, rollback) as one. Transactions are created for simplifying programming model for applications accessing a database, to ignore certain error &#x2F; concurrency issues, as database has safety guarantees. Transactions on relational databases are similar.As NoSQL becomes famous, many databases abandoned transactions or describe a much weaker set of guarantees (due to scalability and availability concern). Atomicity, Consistency, Isolation, and Durability (ACID)Systems that do not meet the ACID criteria are sometimes called BASE, which stands for Basically Available, Soft state, and Eventual consistency (also very vague concept). Atomicity: atomic refers to something that cannot be broken down into smaller parts, means no way a thread can see half-finished result of a operation.System can either be in state of before operation or after opertion.If a transaction was aborted, the application can be sure that it didn’t change anything, so it can safely be retried. Consistency: Certain statements about your data (invariants) that must always be true. E.g., credits and debits across all accounts must be balanced.Application is responsible for defining transactions correctly to preserve consistency, not up to the database alone. Isolation: Multiple clients operating on the same record may run into concurrency problems &#x2F; race conditions.Then we need to isolate transactions.However, serializability is too expensive to be used, so we often use looser isolation level, e.g., snapshot isolation. Durability: Data written to non-volatile storage (single-node) or copied to some number of nodes (multi-node). There may NOT be perfect solution when choosing between replication and write to SSD &#x2F; disk. Multi-Object TransactionIf multiple objects are to be operated in a transation, then we need to determine which read &#x2F; write belong to the same transaction. In relational DB: on any particular TCP connection &#x2F; everything between BEGIN TRANSACTION and COMMIT is considered to be of the same transaction. For non-relational DB, we may not have such guarantees and ended up with partially updated state. Examples about why multi-object transaction is needed: Keep foreign key reference up-to-date. When updating denormalized information and keep it in sync (rather than normalized document as a whole). In databases with secondary indexes, the indexes also need to be updated. Single-Object TransactionStorage engines universally provide atomicity (can be implemented by log for crash recovery) and isolation (use lock on each object) on the level of single object (like key-value pair). More complex atomic operations like increment operation can be used to remove the need for a read-modify-write cycleSimilarly popular is a compare-and-set operation, which allows a write to happen only if the value has not been concurrently changed by someone else. Note that single-object transaction is NOT transactions in the usual sense (as it did not group multiple operations on multiple objects into one unit of execution). Retrying failed transaction is not perfect solution: Transaction succeeded but network failed, then retrying transaction may cause it perform twice (if without additional application-level deduplication mechanism). If the error is due to overload, retrying the transaction will make the problem worse, not better. To avoid such feedback cycles, you can limit the number of retries, use exponential backoff, and handle overload-related errors differently (if possible). It is only worth retrying after transient errors (for example due to deadlock, isolation violation, temporary network interruptions, and failover); after a permanent error (e.g., constraint violation) a retry would be pointless. If the transaction also has side effects outside of the database, those side effects may happen even if the transaction is aborted (like sending emails). This can be solved by two-phase commit. If the client process fails while retrying, any data it was trying to write to the database is lost. Weak Isolation LevelsAs mentioned before, Serializable isolation is too expensive so people would use weaker level of isolations to protect against some concurrency issues, but not all. Read CommittedThe most basic level of transaction isolation is read committed. It makes two guarantees: When reading from the database, you will only see data that has been committed (no dirty reads). When writing to the database, you will only overwrite data that has been committed (no dirty writes). If a transaction needs to update several objects, read committed can guarantee that database is not partially updated (keep consistency);Also database will not see later rolled back data (never actually committed to the database.) Usually later writes will be delayed until the first write’s transaction has committed or aborted.However, it does not prevent the race condition between two counter increments. It is most common to use row-level locks to implement read committed. This is fine for requiring read locks to read, but may have performance issues if such lock is required for write purpose, as it can block a lot of read transactions, bad for operability and have knock-on effect. An alternative is, for every object that is written, the database remembers both the old committed value and the new value set by the transaction that currently holds the write lock.While the transaction is ongoing, any other transactions that read the object are simply given the old value. Only when the new value is committed do transactions, switch over to reading the new value. Snapshot Isolation and Repeatable ReadRead Committed may not be sufficient.Non-repeatable read &#x2F; read skew: Before &#x2F; after a commit, a different value would be seen. E.g., if you transfer from an account to another, then at certain point, the sum of two account balances may not be accurate. Some situations can NOT tolerate this: Back Up: During maybe hours of backing up, writes will still be made to the database. Thus, you could end up with some parts of the backup containing an older version of the data, and other parts containing a newer version. If you need to restore from such a backup, the inconsistencies (such as disappearing money) become permanent. Analytic queries and integrity checks: Sometimes, you may want to run a query that scans over large parts of the database or may be part of a periodic integrity check that everything is in order (monitoring for data corruption). These queries are likely to return nonsensical results if they observe parts of the database at different points in time. Snapshot isolation: each transaction reads from a consistent snapshot of the database, and see the data was committed in the database at the start of the transaction (will not see changes happen after that). Implementation: Use write locks to prevent dirty write;a transaction makes a write can block other transactions writing to the same object;read is not blocked.There is NO lock contention between reader and writer. Multi-Version Concurrency Control (MVCC): potentially keepseveral different committed versions of an object, because various in-progress transactions may need to see the state of the database at different points in time. If database only need read committed isolation, then only 2 versions is needed (committed version, and overwritten but not yet committed version). However, MVCC is more frequently used. Implementation When a transaction is started, it is given a unique, always-increasing transaction ID (txid). (You cannot see changes made by a larger transaction ID) Whenever a transaction writes anything to the database, the data it writes is tagged with the transaction ID of the writer. Each row in a table has a created_by field, containing the ID of the transaction that inserted this row into the table. Each row has a deleted_by field, which is initially empty. If a transaction deletes a row, the row isn’t actually deleted from the database, but it is marked for deletion by setting the deleted_by field to the ID of the transaction that requested the deletion. At some later time, when it is certain that no transaction can any longer access the deleted data, a garbage collection process in the database removes any rows marked for deletion and frees their space. Visibility Rules: List all other transactions are in progress (not committed or aborted), ignore their changes. Ignore the aborted transactions. Ignore writes made by transactions with a later transaction ID, regardless of whether they are committed. All other writes are visible. By never updating values in place but instead creating a new version every time a value is changed, the database can provide a consistent snapshot while incurring only a small overhead. Indexes and snapshot isolation Have the index point to all versions of an object, require an index query to filter out any object versions that are not visible to the current transaction. When garbage collection removes old object versions no longer visible to any transaction, corresponding index entries can be removed. Like B-Tree, we can use an append-only &#x2F; copy-on-write variant which does not overwrite pages of the tree when they are updated, but instead creates a new copy of each modified page. The parent pages up to the root of the tree, are copied and updated to point to the new versions of the child pages. With append-only B-trees, every write transaction creates a new B-tree root, and a particular root is a consistent snapshot of the database at the point in time it was created. (no need to filter on transaction IDs because subsequent writes cannot modify an existing tree, but can only create new roots [needs compaction and garbage collection]) * For the 1st method, we can try to fit different versions of the same object on the same page.** Snapshot Isolation has many names like serializable &#x2F; repeatable read. Prevent Lost UpdatesUpdates may lost, if concurrent applications are in a read-modify-write cycle, because the write back value may not contain other application’s updates (later write clobbers the earlier write). This can be resolved by atomic write operations, implemented by an exclusive lock on the object when it is read so no other transaction can read it until the update has been applied.Sometimes it is called cursor stability. Or we can force all atomic operations to be executed on a single thread. Object-relational mapping frameworks make it easy to accidentallywrite code that performs unsafe read-modify-write cycles instead of using atomic operations provided by the database. Explicitly Lock: Also, we can require application to explicitly lock objects to be updated, and implement more complex validation logic (like checking whether move of piece of chess is legal) Automatically detecting lost updates: We canc allow writes to execute in parallel, and if the transaction manager detects a lost update, abort the transaction and force it to retry its read-modify-write cycle. This is good as it does not require application code to use any special database features. Compare-and-set: The atomic compare-and-set opeartion to avoid lost updates by allowing an update to happen only if the value has not changed since you last read it. Otherwise, retry the read-modify-write loop. Conflict resolution and replication: Replicated database has multiple copies on different nodes, make prevention of lost of updates more complex. Lock based &#x2F; compare-and-set will NOT apply as we cannot guarantee that there is a single up-to-date copy of the data, due to asynchrounous write nature. We can allow concurrent writes to create conflicting versions of values (siblings) and resolve &amp; merge versions after the fact.Atomic operations works well with replicated context, especially when they are commutative. Last Write Wins (LWW) can easily lost updates, but it is widely used. Write Skew and PhantomsSome constraints (e.g. checking the inventory before purchase) can be violated, if it depends on certain database data under snapshot isolation, then write happens. After the writes are finished, constraints are violated. This is called Write Skew. Write skew can occur if two transactions read the same objects, and then update some of those objects (different transactions may update different objects).In the special case where different transactions update the same object, you get a dirty write or lost update anomaly (depending on the timing). Atomic operations (because multiple objects involved) and automatic detection will not help. We need true serializable isolation to prevent Write Skew.Otherwise, we can explicitly lock the rows that the transaction dependes on. We can use FOR UPDATE to lock all the read rows. The effect, where a write in one transaction changes the result of a search query in another transaction, is called a phantomSnapshot isolation avoids phantoms in read-only queries, but not in read-write transactions. Using FOR UPDATE to lock rows will not work, if no row is returned. Then, it cannot resolve conference room booking problem, because we want to create a meeting if there does not lie one. Materializing Conflicts:We can try to introduce a lock object into the database to resolve the issue of no object we can attach locks on. However, this can be hard and error-prone, and ugly to let concurrency control mechanism leak into the application data model.A serializable isolation level is much preferable in most cases. SerializabilityWe have different ways of implementing serializability: Actual Serial ExecutionRemove concurrency entirely, but this is only recognizied as feasible only until recently.This is because: RAM became cheap enough to keep entire active dataset in memory; OLTP transactions are usually short and only make a small number of read and writes. Multi-stage process (e.g., search and book for flight) is hard to be encapsulated as a transaction. However, humans are very slow to respond, and this will result in huge number of concurrent transactions and mostly idle. Most OLTP applications avoid waiting for a user within a transaction.This means, a transaction is committed within the same HTTP request—a transaction does not span multiple requests. A new HTTP request starts a new transaction.* Even though the human has been taken out of the critical path, transactions have continued to be executed in an interactive client &#x2F; server style, one statement at a time.In such case, queries, application codes and database run on different machines and resulted in expensive communication cost.It is impossible to disallow concurrency, because database will spend most of the time waiting the application to issue the next query for the current transaction. Systems with single-threaded serial transaction processing don’t allow interactive multi-statement transactions.Application must submit the entire transaction code to the database as a stored procedure (must have all needed data by hand in memory, No network &#x2F; disk IO). Stored Procedure has kind of bad repuatation because they are implemented with different langauges (PL&#x2F;SQL, e.g.), ugly and archaic, lack ecosystem of libraris, harder to debug, version control , deploy and test, as well as monitoring. Badly written stored procedure may impact the performance more. Solution: Use exisiting general-purpose programming langauges.With stored procedures and in-memory data, executing all transactions on a single thread becomes feasible Partition may be needed for single thread procedure.However, procedures may need to be performed in lock-step across all partitions, if we need to access multiple partitions (to ensure serializability, may impact performance). Data with multiple secondary indexes may require a lot of crosspartition coordination. Summary of Stored Procedure Transaction must be small and fast One slow transaction can stall all transaction processing Apply if active dataset can fit in memory. It can be slow if we need to access disk. Write throughput must be low enough, to be handled on single CPU &#x2F; partitioned without requiring cross-partition coordination (possible, but highly limited). Two-Phase Locking (2PL): Several transactions are allowed to concurrently read the same object as long as nobody is writing to it. But as soon as anyone wants to write (modify or delete) an object,exclusive access is required (this lock will block readers as well) 2PL is used for serializable isolation level in MySQL (InnoDB) and SQL Server, and the repeatable read isolation level in DB2. Implementation: Readers must first acquire lock in share mode; Writers must first acquire lock in exclusive mode.Transaction must hold the lock until the end of the transaction (commit or abort), and that is the meaning of two-phase.First Phase: When locks are acquired while the transaction is executing;Second Phase: Locks are release when transaction is finished. Deadlock may happen, and needs to be detected and resolved (by aborting one transaction). So as contention. Thus, 2PL databases may experience severe performance issue, and we may prefer weaker lock in that case. Predicate locks: in order to prevent phantoms.Does NOT belong to a particular object (e.g., one row in a table), it belongs to all objects that match some search condition (e.g. certain inventory &#x2F; conference room availability) If transaction A wants to read objects matching some condition, it must acquire a shared-mode predicate lock on the conditions of the query.If another transaction B currently has an exclusive lock on any object matching those conditions, A must wait until B releases its lock before it is allowed to make its query.If transaction A wants to insert, update, or delete any object, it must first check whether either the old or the new value matches any existing predicate lock. The key idea here is that a predicate lock applies even to objects that do not yet exist in the database, but which might be added in the future (phantoms).If two-phase locking includes predicate locks, the database prevents all forms of write skew and other race conditions, and so its isolation becomes serializable. Index-range locksIndex-range locks &#x2F; nextkey locking, which is a simplified approximation of predicate locking, might be the reason of poor performance (as it place locks on more objects than needed). E.g., it is safe to block all rooms, or block one room’s all slots, if you just want to book a meeting in room A between 1-2PM.Index-range locks might be easier to implement, with existing indexes. Index-range lock is still a good compromise to implement serializability. Serializable Snapshot Isolation (SSI)Now used in both single-node and distributed databases. Two-phase locking is a so-called pessimistic concurrency control mechanism: if things can go wrong (lock is possessed), we should wait till the lock is released, like mutual exclusion. Serial execution is extremely pessimistic, equals to each transaction having an exclusive lock during execution. serializable snapshot isolation is an optimistic concurrency control technique.We hope everything to turn out alright, just check whether isolation was violated before a transaction commits. If so, we have to retry. If there lies high contention, optimistic concurrency control will have poor performance. However, if we have enough spare capacity and reasonable contention, it can perform better. Contention can be reduced with commutative atomic operations. SSI is based on snapshot isolation, and adds an algorithm for detecting serialization conflicts among writes and dtermines which transaction to abort. Under snapshot isolation, the result from original query may not be up-to-date, because data may be modified during transaction execution, and the premise (that this transaction is doable and correct) may no longer be true (means that the transaction needs to be aborted). We need to detect reads of a stale MVCC object version (uncommitted write occurred before the read), and writes that affect prior reads (the write occurs after the read). For detecting stale MVCC object version: Before commit a transaction, database checks if any previously ignored transaction (changing the same data field) is committed. If so, current transaction is aborted as its premise no longer exists. For detecting writes that affect prior reads: Can be implemented using a technique similar to index-range lock, but do not block other transactions.We can track in index or table level, when a transaction commits, it detects whether other transactions read the affected data (let such transactions know their read is not up-to-date).","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Data System","slug":"Data-Science/Data-System","permalink":"https://umiao.github.io/categories/Data-Science/Data-System/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Data System","slug":"Data-System","permalink":"https://umiao.github.io/tags/Data-System/"},{"name":"Designing Data-Intensive-Applications","slug":"Designing-Data-Intensive-Applications","permalink":"https://umiao.github.io/tags/Designing-Data-Intensive-Applications/"}]},{"title":"Designing Data-Intensive-Applications-Note-14","slug":"Designing-Data-Intensive-Applications-Note-14","date":"2024-03-24T21:21:49.000Z","updated":"2024-05-05T19:02:37.700Z","comments":true,"path":"2024/03/24/Designing-Data-Intensive-Applications-Note-14/","link":"","permalink":"https://umiao.github.io/2024/03/24/Designing-Data-Intensive-Applications-Note-14/","excerpt":"Discussion on partition related topics.","text":"Discussion on partition related topics. Partition: Known as Shard in MongoDB, Elasticsearch, and SolrCloud. Known as a region in HBase, a tablet in Bigtable, a vnode in Cassandra and Riak, and a vBucket in Couchbase. Partition aims to act as independent small, non-overlapping databases for better scalability.Different partitions can be placed on different nodes for load balancing (on processor) and large dataset distribution. Each node should be able to independently querying their own partition, able to parallelize complex queries. Old partition databases: Teradata, Tandem NonStop SQL; Recently: Hadoop and NoSQL databases. Partition is usually combined with replication to have each partition stored on multiple nodes. If leader based replication setting is used, then leader and follower of the same partition should belong to different nodes. We expect partition to be spreading data and query evenly, so that our throughput and data volume being able to handle can grow linearly as we have more nodes (if we ignore the replication). Skewed: the partitioning is unfair and some partitions have more data. Reduce the effectiveness of partition. A partition with high load is named a hot spot. We should try to assign records to node randomly, preferrably using primary key. Partition by Key Range: assign a continuous range of keys (from some minimum to some maximum) to each partition (like encyclopedia) The range of keys are NOT guaranteed to be evenly spaced, need to adapt to the data. Some access pattern may lead to hot spots. We can keep the keys sorted in each partition to make scan easier and treat the keys as a concatenated index. Partition by Hash of Key: Due to the risk of skew and hot spots, we can use hash function to determine partition (assign each partition a range of hashes). Good hash function can takes skewed data but still makes it uniformly distributed. Such function not have to be cryptographically strong if it is for partition purpose. E.g., MD5 and Fowler–Noll–Vo. Some hash function (used for hash tables) are not suitable for partitioning, like Object.hashCode() in Java and Object#hash in Ruby, where same key may have different hash values across processes. The partition boundaries can be evenly spaced, or they can be chosen pseudorandomly (consistent hashing &#x2F; hash partitioning, randomly choose partition boundaries to avoid the need for central control &#x2F; distributed consensus. That is to say, also map nodes &#x2F; virtual nodes to a ring, then direct traffic to the next node on the ring). We can NOT do efficient range queries by hashing the keys. No primary key based range queries as queries have to be sent to all partitions.* We can make a tradeoff by using compound primary key but just hash the first part to determine the partition, other columns used as concatenated index.E.g., if you use compound key of (user_id, updated_timestamp), you can effectively query posts of one user, in a certain time range. Each user’s data is guranteed to be stored in a single partition. Relieving Hot Spots: Exactly the same keys would still be directed into the same partition, even with the partition on hash of keys (e.g., visits to certain celebrity). This is hard to be compensated by data systems, so should be done on the application level. The hotspots can be relieved by appending random numbers to the beginning &#x2F; end of the key. This only makes sense when you are adding salt for small number of hotspots. Partition on Secondary Indexes Secondary Indexes usually cannot uniquely identify a record, more like searching occurences of a particular value. It is more complex and does not map neatly into partitions. Document-based partitioning (also named as Local Index): Each partition is sepearate and owns its own secondary indexes covering the documents in that partition. E.g., you can add a qualified red car to each partition’s seondary index’s list of color:red. During query time, you should send query to all partitions and combine the result back (scatter &#x2F; gather, may make the secondary index more expensive and prone to tail latency amplification). It is widely used and ideally you can construct your schema to have secondary index queries served from a single partition. Term-based partitioning: In this case, we are having global secondary indexes which cover data in all partitions. Such index can be stored in partitions in a non-overlap manner. E.g., color:red contains all indexes of qualified cars across partitions, but only got stored in single partition (called term-partitioned as the term we are looking for determines the partition of index). Term &#x2F; Global secondary index is more efficient in read; write is slower and more complex (a write to single document may affect multiple partitions of the index). Global secondary index also require a distributed transaction across all partitionsaffected by a write, which is not supported in all databases. It is often asynchronous and the index may not be updated in real-time. Rebalance PartitionsRebalancing (move data and requests from one node to another) may be needed due to increase of query throughput (to add more CPUs), increase of dataset size or machine failure. After rebalancing, we want: Load (data storage, read and write requests) should be shared fairly between the nodes in the cluster. While rebalancing is happening, the database should continue accepting reads and writes. No more data than necessary should be moved between nodes, to make rebalancing fast and to minimize the network and disk I&#x2F;O load. Strategies of rebalancing: NOT recommended: hash mod N (N is number of new partitions). This is expensive as it over-moves the data. Instead, better divide hash into ranges in partitioning. Fixed number of partitons: creating more partitions than number of nodes, assign several partitions to each node (maybe unevenly). Then partitions can be easily moved between nodes for node extension &#x2F; delete purpose. Number of partitions nor assignment of keys will chagne. Also, old partitions can still handle requests when transferring is in progress. Dynamic PartitioningA fixed number of partitions with fixed boundaries may end up with all data in one partition, if you did not configure the boundaries properly. Dynamic Partitioning (used in HBase &#x2F; RethinkDB): When a partition grows to exceed the configured size (e.g. 10GB), it is split into two partitions approximately half of the original size. If data is delted below same threshold, it can be merged with adjacent partition. It makes the number of partitions adapats to the total data volume to potentially save the overhead. An initial set of partitions can be configured on an empty database (this is called pre-splitting) to prevent the number of partitions to grow from 1, but it requires you to know the key distribution. Partitioning proportionally to nodesDynamic partitioning: number of partitions is proportional to the size of the datasetFixed num‐ber of partitioning: the size of each partition is proportional to the size of the dataset. Proportional Partitioning: Make each node having a fixed number of partitions, and the number of partitions proportional to the number of nodes. When a new node joins the cluster, it randomly (hash-based partitioning is required) chooses a fixed number of existing partitions to split, and then takes ownership of one half of each of those split partitions while leaving the other half of each partition in place. The randomization can produce unfair splits, but when averaged over a larger number of partitions (in Cassandra, 256 partitions per node by default), the new node ends up taking a fair share of the load from the existing nodes. * It may be good to loop human in partitioning to reach a tradeoff between automatic &#x2F; manual partitoning, e.g., data system suggests partition assignment but requires commit from admin.Fully automated partitionings can be unpredictable and expensive, may cause overload of network &#x2F; nodes and cascading failure. Request RoutingRequest Routing is an instance of service discovery, which is about how we route request to certain node &#x2F; port. Allow clients to contact any node (e.g., via a round-robin load balancer). If that node coincidentally owns the partition to which the request applies, it can handle the request directly; otherwise, it forwards the request to the appropriate node, receives the reply, and passes the reply along to the client. Send all requests from clients to a routing tier first, which determines the node that should handle each request and forwards it accordingly. This routing tier does NOT itself handle any requests; it only acts as a partition-aware load balancer. Require that clients be aware of the partitioning and the assignment of partitions to nodes. In this case, a client can connect directly to the appropriate node, without any intermediary. Coordination service like ZooKeeper can help in this process (as routing tier), but you can also have any node take &amp; forward requests. * Massively Parallel Processing (MPP) relational database can be a lot more complex, as query needs to be broken into executive stages and partitions.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Data System","slug":"Data-Science/Data-System","permalink":"https://umiao.github.io/categories/Data-Science/Data-System/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Data System","slug":"Data-System","permalink":"https://umiao.github.io/tags/Data-System/"},{"name":"Designing Data-Intensive-Applications","slug":"Designing-Data-Intensive-Applications","permalink":"https://umiao.github.io/tags/Designing-Data-Intensive-Applications/"}]},{"title":"Designing Data-Intensive-Applications-Note-13","slug":"Designing-Data-Intensive-Applications-Note-13","date":"2024-03-24T16:31:49.000Z","updated":"2024-05-05T19:02:39.514Z","comments":true,"path":"2024/03/24/Designing-Data-Intensive-Applications-Note-13/","link":"","permalink":"https://umiao.github.io/2024/03/24/Designing-Data-Intensive-Applications-Note-13/","excerpt":"Discussion on leaderless replication.","text":"Discussion on leaderless replication. Some data storage systems take a different approach, abandoning the concept of a leader and allowing any replica to directly accept writes from clients.Become fashion after Amazon used it for Dynamo system.vi Riak, Cassandra, and Voldemort are open source datastores with leaderless replication models inspired by Dynamo, so this kind of database is also known as Dynamo-style. In some leaderless implementations, the client directly sends its writes to several replicas, while in others, a coordinator node does this on behalf of the client.Difference with leader database: coordinator does not enforce ordering of writes. There is no failover in leaderless configuration, we mark write as successful if received enough ack &#x2F; OK response.Where failed node return online, it may have stale data due to lost of writes. Thus, a client can send multiple read requests and use latest version. Read repair and anti-entropyWe want to gain eventual consistency, and catch-up mechanism for replicas in leaderless setting. Read Repair: When a client makes a read to serval nodes in parallel, it can write new values back to the stale replica (works well for data being frequently read). Anti-entropy process: have a background process to check difference in data between replicas and copies missing values.This process does NOT copy data in particular order and my have big latency (this helps recovering the less read values). Quorums for reading and writingWe need to determine how many successful read &#x2F; writes are needed to make sure the changes take effect on enough replicas (do not care why some nodes failed).Genearlly, we need changes be confirmed by $w$ nodes out of $n$ replicas, and we read $r$ nodes every time. If $w+r \\gt n$, then at least one read value is latest. If $w \\lt n$, if node is unavailable, we can still process writes.If $r \\lt n$, if node is unavailable, we can still process reads.Usually, reads and writes are sent to all $n$ replicas. * Note that we can have more than $n$ nodes, but ensure for any given value it is only stored on $n$ nodes. This allows us to partition and store dataset too big to fit in single node. Often $r$ and $w$ are chosen to be majority ($\\gt \\frac{n}{2}$) but it is not necessary, it only matters that the sets of nodes used by the read and write operations overlaps in at least one nodes. If quorum condition is not satified (use smaller $r$ and $w$), you may read stale values but allows better performance and availability.Even quorum condition is satisfied, you may still read stale value if sloppy quorum is used ($w$ reads and $r$ writes ended up on different nodes so there is no guarantee for overlap). If two writes occur concurrently, we cannot tell which one happened first. We can merge the concurrent writes.If a write and read occur concurrently, writes may be reflected on only some of the replicas and stale values may be read.If a write succeeded on some replicas ($\\lt w$), it is not rolled back on the replicas where it succeeded. Also, a node carrying a new value may fail and restored with an older value. Better Not view quorum condition as a strong guarantee for data consistency. Staleness is hard to monitor on leaderless setting, as we cannot easily track the replication lag (no fixed order) for metrics analysis. Sloppy Quorums and Hinted HandoffNetwork problem may prevent clients from connecting with active nodes, so the client can no longer reach a quorum. Sloppy quorum: writes and reads still require $w$ and $r$ successful responses, but thoese may include nodes that are not among the designated n “home” nodes for a value.Default on for Riak and default off for Cassandra and Voldermort. Hinted handoff: Once the network interruption is fixed, writes which are temporarily accepted on behalf of another node got sent to the “home” nodes.Tradeoff is that latest values maybe written to other nodes.Only durability is guaranteed that data is stored somewhere in $w$ nodes, but may not be visible by $r$ writes until hinted handoff is completed. Concurrent WritesDynamo-style databases allow several clients to concurrently write to the same key, which means that conflicts will occur even if strict quorums are used. To achieve eventually consistency, we need to detect and resolve the conflicts (e.g., cause by multiple clients’ concurrent writes). Last Write Wins (LWW): Keep only the most recent value, discard concurrent writes to achieve eventual convergence. However, for concurrent writes, the order is undefined. By applying and using the LWW, we achieve the eventual convergence at the cost of durability. Non-recurrent write may be possible to be dropped.In some cases like aching, lost writes are maybe acceptable, otherwise LWW is not suitable. The only safe way of using a database with LWW is to ensure that a key is only written once and thereafter treated as immutable, thus avoiding any concurrent updates to the same key.For example, a recommended way of using Cassandra is to use aUUID as the key, thus giving each write operation a unique key Whether one operation happens before another operation is the key to defining what concurrency means. In fact, we can simply say that two operations are concurrent if neither happens before the other (i.e., neither knows about the other, no causally relationship) Handling of Concurrent Writes The server maintains a version number for every key, increments the version number every time that key is written, and stores the new version number along with the value written. When a client reads a key, the server returns all values that have not been overwritten, as well as the latest version number. A client must read a key before writing. When a client writes a key, it must include the version number from the prior read, and it must merge together all values that it received in the prior read. (The response from a write request can be like a read, returning all current values, which allows us to chain several writes like in the shopping cart example.) When the server receives a write with a particular version number, it can overwrite all values with that version number or below (since it knows that they have been merged into the new value), but it must keep all values with a higher version number (because those values are concurrent with the incoming write). When a write includes the version number from a prior read, that tells us which previous state the write is based on. If you make a write without including a version number, it is concurrent with all other writes, so it will not overwrite anything—it will just be returned as one of the values on subsequent reads (as an new independent copy) If you want to do a bit more, you can try to merge the above multiple versions into one. However, note that delted items may appear again as the result.System must leave a marker with an appropriate version number to indicate that the item has been removed when merging siblings.Such a deletion marker is known as a tombstone. Version VectorsWhen we have multiple replicas accepting writes concurrently, we need to use a version number per replica as well as per key. Each replica increments its own version number when processing a write Also keeps track of the version numbers it has seen from each of the other replicas. This information indicates which values to overwrite and which values to keep as siblings. Version Vector: The collection of version numbers from all the replicas. Sometimes called a Vector Clock.The version vector structure ensures that it is safe to read from one replica and subsequently write back to another replica.Doing so may result in siblings being created, but no data is lost as long as siblings are merged correctly. Dotted Version VectorIn case of conflict, instead of Last Write Wins, we can store all the versions, their values and the Causal Hisotry (e.g., version 3 declares it is from: [v_1, v_2], means that it overwrites value of version 1 and 2) However, the causal history may be too expensive to store. Data structure of Dotted Version Vector (DVV) looks like this:$((i_1, n), [(i_1, m), (i_2, l), (i_3, k) …])$Here, $i_1, ..i_3$ are IDs of nodes, and the number $n, m, l, k$ are version numbers (incremental) of corresponding node. The first part is the version of certain node (called a dot), and the second part (list) is the version vector, stores the state before the event which added $(i_1, n)$ happens. Then we can determine the timing &#x2F; causality. We can use the version vector to determine which event &#x2F; version comes after the other.Otherwise, they are concurrent.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Data System","slug":"Data-Science/Data-System","permalink":"https://umiao.github.io/categories/Data-Science/Data-System/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Data System","slug":"Data-System","permalink":"https://umiao.github.io/tags/Data-System/"},{"name":"Designing Data-Intensive-Applications","slug":"Designing-Data-Intensive-Applications","permalink":"https://umiao.github.io/tags/Designing-Data-Intensive-Applications/"}]},{"title":"Designing Data-Intensive-Applications-Note-12","slug":"Designing-Data-Intensive-Applications-Note-12","date":"2024-03-21T23:27:44.000Z","updated":"2024-05-05T19:02:49.131Z","comments":true,"path":"2024/03/21/Designing-Data-Intensive-Applications-Note-12/","link":"","permalink":"https://umiao.github.io/2024/03/21/Designing-Data-Intensive-Applications-Note-12/","excerpt":"Discussion on replication lag problems and multi-leader replication.","text":"Discussion on replication lag problems and multi-leader replication. Problems with Replication Lag Only Asynchronus Replication is realistic, otherwise single node &#x2F; network failure will turn down the entire system. We may see out-dated data because follower falling behind. Eventual Consistency: If writes stopped and wait for followers, followers can catch up and be consistent eventually. In real world, such latency can be seconds or minutes, bringing real problems. Reading Your Own WritesIn certain scenarios, users want to view the changes they submitted immediately (post &#x2F; comment). If we just write to leader and read from follower, the recent changes may not be replicated yet, looks as if they are lost. Then, we need this read-after-write consistency &#x2F; read-your-writes consistency to guarantee that user will always see their own updates after reloading the page (No promise for other users, their changes may be shown later). Potential Implementations: Read from leader if it is something user may have modified. For something like user profile (only user themself can edit), we can always read from leader. Reading from leader too often can negating the benefit of scaling. Thus we can use some criteria to determine if we read from leader, like: Track time of last update, then in the following 1 minute, read from leader; Monitor replication lag and do not read from followers left too far behind (&gt; 1 min); Record Timestamp of user’s update to ensure the served replica reflects the updates, otherwise wait or switch replica. Timestamp can be logical (e.g., log sequence number indicating order of writes) or system clock (needs clock synchronization). If your replica distribute across multiple datacenters, then request must be first routed to the one containing the leader. Cross-device read-after-write consistency may be needed (across desktop and mobile platforms.) This makes recording timestamps and routing to correct data centers more difficult, may need single user’s devices be routed to the same datacenter. Monotonic ReadsThis consistency prevents user from reading data from the past.E.g., a comment was added to a page. During the replication to followers, a user made 2 exactly the same reads. One read returned the comment (from a follower with small lag) and the later one returned nothing (due to another follower’s greater lag). This can be implemented by always routing to the same replica (map by user ID’s hash). Rerouting is needed if the replica failed. Consistent Prefix ReadsReplication lag anomalies concerns violation of causality (e.g., the answer is shown prior to a question being asked). This is a particular problem in partitioned &#x2F; sharded database.If different partitions operate independently, then there is no global ordering of writes (some part may be new and some may be old). This requires consistent prefix reads, if a sequence of writes happens in a certain order, then anyone reading those writes will see them appear in the same order. We want to have writes which are causally related to each other written to the same partition.This may be hard to achieve in some applications. SummaryRoot cause of replication lag derived issues is, the replication pretends to be synchronous but in fact is asynchronous.Such issue can be mitigated by conduct some reads on the leader.Many distributed (replicated and partitioned) databases have abandoned the transactions as it is expensive in terms of performance and availability.Eventual consistency may be inevitable in scalable system (may be right but over-simplified) Multi-Leader ReplicationHaving only one leader means we may encounter single point failure on leader, then no writes can be done to the database. A natural solution is to allow multiple nodes to accept writes.E.g., Tungsten Replicator for MySQL, BDR for PostgreSQL, and GoldenGate for Oracle. Replication still happens in the same way: each node that processesa write must forward that data change to all the other nodes.We call this a multi-leader configuration (also known as master–master or active&#x2F;active replication).In this setup, each leader simultaneously acts as a follower to the other leaders. If you have a database with replicas in multiple datacenters, then the leader-based setup will require all writes go into the datacenter containing that leader. Among data centers, each leader replicates changes to other leaders (in different datacenter). Within a datacenter, it is regular leader-follower replication. Interdatacenter latency can be hidden from user, means better preceived latency. Allow each datacenter to operate independently. Have better tolerance of network problems (more common as inter-datacenter link is usually on public internet) Challenges: same data modified in different datacenters must have write conflicts resolved. Autoincrementing keys, triggers, and integrity constraints can be problematic. Avoid multi-leader setting if possible. Clients with offline operation: Situation where you need multi-leader replication, as you still want to have certain app like calendar to accept read (see meetings) and write (add meetings) even without network connection.For your devices with the calendar app, they need to be synced when next time online. Every device has a local database that acts as a leader (it accepts write requests) and has a multi-leader replication process.CouchDB is designed for this mode of operation. Collaborative editing: Etherpad and Google Docs allow multiple people to concurrently edit a text document or spreadsheet.The changes need to be applied to local replica instantly, and applied to other users’ replicas asynchronously.Lock mechanism is needed to avoid editing conflicts. Some tradeoff is needed to make collabration faster, like avoid locking and use conflict resolution instead. Conflict DetectionIn multi-leader setting, conflicts cannot be avoided by synchronous conflict detection (wait for writes to be replicated before return success) as it negated the idea of multi-leader. Conflict Avoidance: Try to ensure all writes of certain record go through the same leader, making it single-leader from user’s view. It is suggested as conflicts handling is more difficult.The avoidance can fail if certain datacenter is failed, or the user moved to another physical location and closer to a different datacenter. Converging toward a consistent state:In single-leader database, the last write determines the value of a field. However, this order is not guaranteed in multi-leader setting, ended up with inconsistent result.Data must be eventually the same in all replicas, thus conflicts must be resolved in a convergent way. Convergent conflict resolution Give each write a unique ID (a timestamp, a long random number, a UUID, or a hash of the key and value) and pick the write with highest ID as winner and throw away other writes. If we use timestamp, then it is Last Write Wins (LWW), may cause data loss. Instead, we can give each replica a ID and keep higher numbered replica always take precedence over writes from lower numbered replica. This will also cause data loss. Somehow merge the conflict values together (like concatenating) Record the conflict in an explicit data structure that preserves all information, and write application code that resolves the conflict at some later time (perhaps by prompting the user). The conflicts resolving logic can be customized, as the most appropriate way may depend on the application.On read: If conflict detected in log of replicated changes, then call the conflict handler. This is usually backend faced and NOT interative with user.On write: Store all the conflicting writes and return to application for resolving. (how CouchDB works) Conflict resolution usually applies at the level of individual row &#x2F; document, not transaction.(we will count multiple writes of single transaction) Automatic Conflict Resolution* Conflict resolution rules can quickly become complicated, and custom code can be error-prone. (e.g., if Amazon only records items in the cart but not items got deleted, then deleted items would be recovered.) Conflict-free replicated datatypes (CRDTs):a family of data structures for sets, maps, ordered lists, counters, etc. that can be concurrently edited by multiple users, and which automatically resolve conflicts in sensible ways. Some CRDTs have been implemented in Riak 2.0 Mergeable persistent data structures: track history explicitly, similarly to the Git version control system, and use a three-way merge function (whereas CRDTs use two-way merges). Operational transformation: conflict resolution algorithm behind collaborative editing applications such as Etherpad and Google Docs. It was designed particularly for concurrent editing of an ordered list of items, such as the list of characters that constitute a text document. Note that some conflicts can be harder to detect, e.g., make sure each person does not appear in two meeting rooms. Multi-Replication TopologiesA problem to consider if you have more then two leaders.Some examples: circular (n leaders having n-1 directional edges), star (each edge leader has directional link with the central &#x2F; root leader) or all-to-all (each leader has link to each other, most general one).MySQL by default only supports circular topology. For star and circular (they may prone to single point of failure), a write may need to pass through multiple nodes, so we need each node to place a unique identifer on the replication log to prevent infinite replication loops.For all-to-all topologies, some network links may be faster, the change log may not be delivered in order.E.g., dependent update arrives before the insert.Then we need to resolve this problem of causality, to have a node process the insert first by having a timestamp. It should noted that conflict detection techniques are poorly implemented in many multi-leader replication systems.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Data System","slug":"Data-Science/Data-System","permalink":"https://umiao.github.io/categories/Data-Science/Data-System/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Data System","slug":"Data-System","permalink":"https://umiao.github.io/tags/Data-System/"},{"name":"Designing Data-Intensive-Applications","slug":"Designing-Data-Intensive-Applications","permalink":"https://umiao.github.io/tags/Designing-Data-Intensive-Applications/"}]},{"title":"Designing Data-Intensive-Applications-Note-11","slug":"Designing-Data-Intensive-Applications-Note-11","date":"2024-02-27T04:01:56.000Z","updated":"2024-05-05T19:02:53.472Z","comments":true,"path":"2024/02/26/Designing-Data-Intensive-Applications-Note-11/","link":"","permalink":"https://umiao.github.io/2024/02/26/Designing-Data-Intensive-Applications-Note-11/","excerpt":"Discussion on implementation and optimization of replication log.","text":"Discussion on implementation and optimization of replication log. Statement-based Replication Simplest case, just forward all leader’s write request to its followers (INSERT, UPDATE, DELETE). Statement calls a non-determinsitic function like RAND() can get a different value. For auto-incrementing column &#x2F; operators relying on the existing data in the database, they have to be executed in exactly the same order on each replica. This can be limiting in multiple concurrently executing transactions case. Stataments that have side effects (triggers, stored procedures, user-defined functions) may result in different side effect occuring on each replica, unless they are absolutely determinsitic. These issues can be resolved by replacing non-deterministic function calls with a fixed value. However, there are too many edge cases so other replication methods are preferred. Statement-based replication was used in MySQL before version 5.1. It is still sometimes used today, as it is quite compact, but by default MySQL now switches to rowbased replication if there is any nondeterminism in a statement.VoltDB uses statement-based replication, and makes it safe by requiring transactions to be deterministic Write-ahead log (WAL) shippingIn storage engines, for data on disk, we usually every write is appended to a log: In the case of a log-structured storage engine,this log is the main place for storage. Log segments are compacted and garbage-collected in the background. In the case of a B-tree, which overwrites individual disk blocks, every modification is first written to a write-ahead log so that the index can be restored to a consistent state after a crash. In either case, the log is an append-only sequence of bytes containing all writes to the database. We can use the exact same log to build a replica on another node: besides writing the log to disk, the leader also sends it across the network to its followers to build a copy of the exact same data structures as on the leader (used in PostgreSQL and Oracle). Disadvantage: log describes the data on a very low level, contains details of which bytes were changed in which disk blocks. This makes replication closely coupled to the storage engine.If the database changes its storage format from one version to another, it is typically not possible to run different versions of the database software on the leader and the followers. If the replication protocol allows the follower to use a newer software version than the leader, you can perform a zero-downtime upgrade of the database software by first upgrading the followers and then performing a failover to make one of the upgraded nodes the new leader. If the replication protocol does not allow this version mismatch, as is often the case with WAL shipping, such upgrades require downtime. Logical (row-based) log replicationAn alternative is to use different log formats for replication and for the storage engine, which allows the replication log to be decoupled from the storage engine internals.This kind of replication log is called a logical log, to distinguish it from the storage engine’s (physical) data representation. A logical log for a relational database is usually a sequence of records describing writes to database tables at the granularity of a row: For an inserted row, the log contains the new values of all columns. For a deleted row, the log contains enough information to uniquely identify the row that was deleted. Typically this would be the primary key, but if there is no primary key on the table, the old values of all columns need to be logged. For an updated row, the log contains enough information to uniquely identify the updated row, and the new values of all columns (or at least the new values of all columns that changed). A transaction that modifies several rows generates several such log records, followed by a record indicating that the transaction was committed.MySQL’s binlog (when configured to use row-based replication) uses this approach. Since a logical log is decoupled from the storage engine internals, it can more easily be kept backward compatible, allowing the leader and the follower to run different versions of the database software, or even different storage engines. A logical log format is also easier for external applications to parse. This aspect is useful if you want to send the contents of a database to an external system, such as a data warehouse for offline analysis, or for building custom indexes and caches.This technique is called change data capture. Trigger-based replicationAbove replication approaches are implemented by the database system, without involving any application code. More flexibility may be needed. For example, if you want to only replicate a subset of the data, or want to replicate from one kind of database to another, or if you need conflict resolution logic, You may need to move replication up to the application layer. Oracle GoldenGate, can make data changes available to an application by reading the database log. An alternative is to use features that are available in many relational databases: triggers and stored procedures. A trigger lets you register custom application code that is automatically executed when a data change (write transaction) occurs in a database system. Able to log this change into a separate table, from which it can be read by an external process. That external process can then apply any necessary application logic and replicate the data change to another system. Databus for Oracle and Bucardo for Postgres work like this, for example. Trigger-based replication v.s. built-in replication Greater overheads More prone to bugs and limitations More flexibility.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Data System","slug":"Data-Science/Data-System","permalink":"https://umiao.github.io/categories/Data-Science/Data-System/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Data System","slug":"Data-System","permalink":"https://umiao.github.io/tags/Data-System/"},{"name":"Designing Data-Intensive-Applications","slug":"Designing-Data-Intensive-Applications","permalink":"https://umiao.github.io/tags/Designing-Data-Intensive-Applications/"}]},{"title":"Designing Data-Intensive-Applications-Note-10","slug":"Designing-Data-Intensive-Applications-Note-10","date":"2024-02-26T04:37:34.000Z","updated":"2024-05-05T19:03:00.307Z","comments":true,"path":"2024/02/25/Designing-Data-Intensive-Applications-Note-10/","link":"","permalink":"https://umiao.github.io/2024/02/25/Designing-Data-Intensive-Applications-Note-10/","excerpt":"Discussion on distributed system goal and Replication implementations.","text":"Discussion on distributed system goal and Replication implementations. Distributed System: Motivations: Scalability Fault tolerance &#x2F; High availability Latency. Scale to Higher Load Scale up: under single OS, integrate more CPUs, RAM and disks, under Shared memory architecture. However, single machine solution’s cost grows faster than linearly, restricted to single geographic location, has limited fault tolerance and subject to bottlenecks (like IO and memory) Shared-disk architecture Serval machines with independent CPU and RMA, but stored data on array of disks is shared between the machines Connected via a fast network (used in some data warehouse). However, this method’s scalability is also limited by contention and overhead of locking. Scale Out A Shared-Nothing Architecture, also named horizontal scaling is more popular. Each (virtual) machine is called a node. Each node uses its CPUs, RAM and disks independently. Coordination between nodes is done at software level with conventional network. No special hardware is reuiqred, so you can use whatever machines for best price &#x2F; performance ratio. Data can be potentially distributed across multiple geographic regions to reduce latency and survive the loss of an entire data center, also increase throughput. With cloud deployment of virtual machines, small companies can still have a multi-region distributed architecture.*Such solution requires developer to be most cautious about the constraints and tradeoffs. It also introduces additional complexity for applications and limits the expressiveness of data model you are using. Replication v.s. Partitioning: Common ways of distributing data across multiple nodes are replication and partition. Replication:Keeping a copy of the same data on several different nodes, potentially in different locations, provides redundancy. Partition:Splitting a big database into smaller subsets called partitions. These two ways can go with each other. Models of Replication:Leaders and Follower: Each node stores a copy of data is called a replica. We want to ensure that (if our data can fit into arbitrary node, a simplifying assumption) every write ends up on all the replicas. That is why we need leader-based replication (active-passive or master-slave replication). Implementation of Leader-based Replication: One replica is designated the leader (master &#x2F; primary). When clients want to write to the database, they must send their requests to the leader, and leader first write the new data to its local storage. Other replica known as followers (slaves &#x2F; secondaries &#x2F; hot standbys), will receive leader’s change as part of replication log &#x2F; change stream, updating their local copies by applying all writes in the same order. When a clinet wants to read from the database, it can query either the leader or any of the followers. However, writes are only accepted on the leader(followers are read-only from leader’s view). Usage as built-in feature of relational DB: PostgreSQL (since 9.0) MySQL Oracle Data Guard SQL Sever’s AlwaysOn Availability Groups. Usage as non-relational DB: MongoDB RethinkDB Espresso. Also used in distributed message brokers like Kafka, RabbitMQ’s highly available queues.Also used by some network file systems and replicated block devices. Synchronous v.s. Asynchrounous Replication:In relational DB, this is usually configurable. However, in other systems, it is usually hard-coded. Example of Synchronous: Leader get the request, send write request to one of the follower, waiting for its “OK” before return to the client. The leader will continue to wait for the next follower to finish updates. Advantages: follower is guaranteed to have an up-to-date &amp; consistent copy, which is usable even leader fails.Disadvantages: the followers might fall behind the leader by a long time due to failure, capacity issue or network issues. All follower writes will be blocked in this case.Due to the above reason, usually only 1 follower is synchronous and the rest are asynchronous.If the synchronous follower becomes unavailable, another asynchronous follower will become synchronous.[This guarantees 2 up-to-date copy on at least 2 nodes]This is sometimes called semi-synchronous. Usually, leader-based replication is configured to be completely asynchronous.If leader fails and not recoverable, any write not yet replicated will lost.[Not durable write, even confirmed with client]The good side is that leader can continue processing writes even all followers are falling behind (especially when we have a lot of them). *Chain Replication Chain Replication is a variant of synchronous replication that has been successfully implemented in a few systems such as Microsoft Azure Storage (which avoid loss of data) Basic components of CR are:A sequence (chain) of nodes with 2 special nodes — HEAD (receives requests from clients) and TAIL (end of the chain, provides the guarantee for consistency). Such chain has at least following properties: Tolerates failure of up to $n-1$ nodes. Write performance is around write performance of P&#x2F;B (Primary &#x2F; Backup) approach. Cluster reconfiguration happens much faster, in case HEAD failure happens. For other nodes — around the same time as for P&#x2F;B.It is very important to note that chain replication requires a strong and reliable FIFO link between nodes. Implementation: Clients send write requests to the head and read requests to the tail. When the head receives a request, it calculates the delta of the states, applies the change and propagates the delta down the chain (to successor). As soon as tail receives the delta, it sends ACK back through each node to the head. As you can see, if a read request returns some value X, that means it has been saved on all nodes. In each node, we will save:Pending(i) — list of received requests by not yet processed by the tail.Sent(i) — list of not yet processed by the tail requests, sent to the node’s i successor.History(i, key) — list of changes for the key. It could be either full history or just last value. Note that:$History(j, key) \\subseteq History(i, key), \\forall j \\gt i$ and $Sent(i) \\subseteq Pending(i), History(i, key) &#x3D; History(i + 1, key) \\cup Sent(i)$ Coping with failures:We need a special master process will: Detect a failed node; Notify predecessor and successor of the failed node; Notify clients if the failed node is either HEAD or TAIL.We also assume that our nodes are fail-stop, means server stops working in case of its failure, never send an incorrect response. Failure is always detectable by the master process. Basc Approach: Only write on HEAD and read from TAIL (the only tail, making it a hotspot which is bad; If tail is in another data center, it can slow down the entire chain). When we need a new node, insert it after tail will be easiest (just copy the tail state and ask the previous tail to continue transfer requests).We can tell that, if head fails, we have $Pending(head) – Sent(head)$ lost; If TAIL fails, we can recover by Node TAIL-1. If other node fails, we can recover based on it predecessor and succesor. Chain Replication with Apportioned Queries （CRAQ）:Read only on TAIL can be a bottleneck. We want to improve this by allowing read requests to be processed by all nodes but the TAIL.To preserve consistency, we maintain a version vector for write requests and will do requests to the TAIL to get latest committed version in case of ambiguity. For read request, HEAD will return the response to clients.For write request, each node except TAIL can return the value back to the client. Each non-tail node can maintain multiple versions of the same key, and those versions are monotonically increasing.Each version can be either “clean” or “dirty”, in the beginning, all versions are clean. When a node receives a write request, it adds the received version to its local list of versions of that key. If the node is the TAIL then it marks the version as clean, the version now is committed and the tail sends ACK back to the head. Otherwise — the node marks the version as dirty and passes to the next node (successor). When a node receives ACK from its successor it marks the version as clean and removes all older versions. When a node receives a read request: If the latest known to the node version is clean — return it. Otherwise — ask TAIL to get last committed version of the given key, which it sends back to the client. (Such version will always exist on the tail by design). Summary:CRAQ’s performance grows linearly with the amount of nodes with mostly read requests;In case of mostly write requests, the performance will be close to the basic approach.It can be deployed in multiple data center to be physically closer to client. CRAQ provides strong consistency.But the TAIL might commit the latest version, before a node sends the response to client.We can use monotonic read (subsequent read requests don’t go in the past; that is to say, subsequent read should see all changes happened ahead) on the whole chain. Other consistency provided:Eventual consistency: the node doesn’t request the latest committed version from the TAIL.This will still provide monotonic reads but only on one node. (subsequent read requests must hit the same node).Besides, this allows CRAQ to tolerate network partitioning (into independent subnets)Bounded Eventual Consistency: Allow to return dirty version only under some conditions, like not older than N revisions or T minutes. Fast Array of Wimpy Nodes (FAWN):Similar to the idea of Amazon Dynamo and consistent hash, we map a physical server into serveral virtual nodes, each has its own unique VID, forming a ring.We have multiple ranges formed by virtual nodes, and each VI takes care of the keys “behind” it and within the certain range. Then, to improve fault tolerance, data is replicated over $R$ next virtual nodes in the clockwise direction (for example, if $R&#x3D;2$ then keys from A1 are replicated to B1 and C1, if the chain only has A1-C1). Thus, we get a chain replication (basic approach).Read requests are routed to the TAIL, i.e. read from A1 will be routed to C1.Write requests are routed to the head and are propagated to the TAIL. Setting Up Follower (without downtime): We may need to set up new followers due to failed nodes &#x2F; need to increase replica. We want the added followers to be accurate. Since the database is changing all the time, copy data from one node to naother is not sufficient.Use lock to make file consistent will against our goal of availability. Method: Take a consistent snapshot of the leader’s database at some point (ideally without a lock in the entire databases).This may be supported by database infra, or third party tool like innobackupex for MySQL. Copy the snapshot to the new follower node. New follower connects to the leader and requests all the data changes after the snapshot(snapshot should be associated to an exact position in the leader’s replication log)[such position named as log sequence number in PostgreSQL, binlog coordinates in MySQL]. After follower processed the backlog of data changes, it is caught up. Node Outage:Each node can possibly go done, should be able to reboot without downtime of sysTEM. It can be done by Catch-up recovery.Each follower keeps a log of data changes received from the leader, and be able to identify the last successful transaction before the fault occurred. The follower can request the data changes after the outage and catch up. Leader Failure: Failover In case of the leader failed, one of the followers needs to be promoted to the new leader. Clients need to be reconfigured to send writes to the new leader Other followers start to consume data changes from the new leader. This is called “failover”, can be either manually done or automatically done. Automatic Failover Process Determining the leader failed. Failure reason could be various (crashes, power outages, network issues, …) but we can simply use timeout to detect. If a node doesn’t respond for some period of time—say, 30 seconds—it is assumed to be dead. Choosing a new leader. This could be done through an election process (where the leader is chosen by a majority of the remaining replicas), or a new leader could be appointed by a previously elected controller node.The best candidate for leadership is usually the replica with the most up-to-date data changes from the old leader (to minimize any data loss).Getting all the nodes to agree on a new leader is a consensus problem. Reconfiguring the system to use the new leader. Clients now need to send their write requests to the new leader. If the old leader comes back, it might still believe that it is the leader,not realizing that the other replicas have forced it to step down. The system needs to ensure that the old leader becomes a follower and recognizes the new leader. Things Could Go Wrong If asynchronous replication is used, the new leader may not have received all the writes from the old leader before it failed. If the former leader rejoins the cluster after a new leader has been chosen, what should happen to those writes? The new leader may have received conflicting writes in the meantime. The most common solution is for the old leader’s unreplicated writes to simply be discarded, which may violate clients’ durability expectations. * Discarding writes is especially dangerous if other storage systems outside of the database need to be coordinated with the database contents. For example, in oneincident at GitHub, an out-of-date MySQL follower was promoted to leader. The database used an autoincrementing counter to assign primary keys to new rows, but because the new leader’s counter lagged behind the old leader’s, it reused some primary keys that were previously assigned by the old leader. These primary keys were also used in a Redis store, so the reuse of primary keys resulted in inconsistency between MySQL and Redis, which caused some private data to be disclosed to the wrong users. Two nodes coul both believe that they are the leader. This situation is called split brainif both leaders accept writes, and there is no process for resolving conflicts, data is likely to be lost or corrupted.As a safety catch, some systems have a mechanism to shut down onenode if two leaders are detected. However, if this mechanism is not carefully designed, you can end up with both nodes being shut down. A longer timeout means a longer time to recovery in the case where the leader fails. However, if the timeout is too short, there could be unnecessary failovers. (load spike, network glitch can trigger failovers, and further impact the performance of system) Thus, many teams prefer to perform failovers manually.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Data System","slug":"Data-Science/Data-System","permalink":"https://umiao.github.io/categories/Data-Science/Data-System/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Data System","slug":"Data-System","permalink":"https://umiao.github.io/tags/Data-System/"},{"name":"Designing Data-Intensive-Applications","slug":"Designing-Data-Intensive-Applications","permalink":"https://umiao.github.io/tags/Designing-Data-Intensive-Applications/"}]},{"title":"Designing Data-Intensive-Applications-Note-9","slug":"Designing-Data-Intensive-Applications-Note-9","date":"2024-02-25T20:49:40.000Z","updated":"2024-05-05T19:03:05.418Z","comments":true,"path":"2024/02/25/Designing-Data-Intensive-Applications-Note-9/","link":"","permalink":"https://umiao.github.io/2024/02/25/Designing-Data-Intensive-Applications-Note-9/","excerpt":"Discussion on Data Flow and Message Passing.","text":"Discussion on Data Flow and Message Passing. Data Flow in Databases: Data usually has longer life cycle, comparing with codes. You need to be aware that if a new version of application wrote the data, it then got read &amp; rewrite by an old ver application, we may face loss of fields &#x2F; inconsistency. The solution can be easy (like keeping the unknown fields, even when you want to transform it into an object then convert back), you just need to be aware of this.* For archival storage (like creating database’s snapshot), it is better to keep them all stored in latest version, rather than following the original mixture of formats. Data Flow Through Services: REST and RPC: When arranging communication, you usually need to assign roles like clients (access API though network) and servers (expose API over network). Clients use get to dowload HTML, CSS, JavaScript, images, etc. Use POST requests to submit data to the server. API consissts of a standardized set of protocols and data formats (HTTP, URLs, SSL&#x2F;TLS, HTML, etc.) A client-side JavaScript application running inside a web browser can use XMLHttpRequest to become an HTTP client (this technique is known as Ajax), usually transferring JSON. Service Oriented Architecture (SOA) &#x2F; microservices architecture Decompose large application into smaller services by area of functionality One server itself can be the client of other servers by requesting certain sub-services. This helps application easier to change and maintain by making services independently deployable and evolvable. This also means that older version and later version servers and clients must be compatible across versions of the service API. Service can impose fine-grained restrictions on what user can do, but limiting the input and output. Web Services: Definition: when HTTP is used as the underlying protocol for talking to the service.The context of “web” can be: Personal device usage through public internet. Same organization’s services talk to each other within the data center as part of a service-oriented structure (software supporting this kind of use case is named as middleware). Requests between organizations, usually via internet. REST Design philosophy that builds upon the principles of HTTP Emphasizes simple data formats Using URLs for identifying resources and using HTTP features for cache control, authentication, and content type negotiation.[More famous than SOAP] *API designed according to REST principles is called RESTful. It prefers less code generation and automated tooling. A definition format such as OpenAPI, a.k.a Swagger, can be used to describe RESTful APIs and produce documentation. SOAP: XML based protocol for making netwrok API requests. Aims to be independent from HTTP and avoids using most HTTP features Instead with sprawling and complex multitude of related standards (the web service framework, known as WS-*) The XML based languaged SOAP API use is called Web Services Description Language (WSDL). It is NOT human readable and hard to construct mannually, heavily rely on tool support, code generation and IDEs.Client access a remote service using local classes and method calls (encoded to XML messages and decode by the framework).[Good for statically typed language]Also, the implementation can be different across vendors. Remote Procedure Calls: The Remote Procedure Calls (RPC) model tries to make a request to a remote network service look the same as calling a function or method in your programming language, within the same process(this abstraction is called location transparency).Problems Local func call is predictable and either succeeds or fails depend on the paran; A network request is unpredictable due to network problem (for both request and response). Network can have completely no result due to a time-out (you have no clue what happened). If you retry, then the requests may be executed for multiple times, just the response got lost. This will interfere with the behavior, unless you build mechanism for deduplication (idempotence). The time for func call is also unstable. Larger objects are hard to be encoded and send via network.We may also encounter type issues when the client and server are not implemented using the same language. Like Java has problem dealing number $&gt; 2^{53}$. Enterprise JavaBeans (EJB) and Java’s Remote Method Invocation (RMI) are limited to Java.The Distributed Component Object Model (DCOM) is limited to Microsoft platforms.The Common Object Request Broker Architecture (CORBA) is excessively complex, and does not provide backward or forward compatibility Due to the shorcomings mentioned above, RPC is mainly used in request between services owned by the same organizaiton, typically in the same data center. Rest: will not try to hide the fact this is a network protocol, but you can still build RPC libraries on top of that. RESTful APIs are： good for experimentation and debugging (easily make request using browser &#x2F; tool curl) Supported by all mainstream programming languages and platforms Has a vast ecosystem of tools (servers, caches, load balancers, proxies, firewalls, monitoring, debugging tools, testing tools, etc.). Evolvability: For RPC, we can do the simplifying assumption that all servers will be first updated, then come the clients.Then, we only have backward compatibility on requests, and forward compatibility on responses.If RPC is used for service providing, upgrading can be hard on clients.Then compatibility may need to be maintained to long time, maybe forever. Service providers often end up maintaining multiple versions of the service API side by side.This can be done via specifying the version in HTTP header &#x2F; URL &#x2F; or use API Keys. * New generation of RPC frameworks is more explicit about the fact that remote request is different from a local function call. E.g., use futures (promises) to encapsulate async actions that may fail.**Some frameworks also provide service discovery, allowing a client to find out at which IP address and port number it can find a particular service. Asynchronous Message-passing Systems: It is between RPC and databases.A request (message) is delivered to another process with low latency, but got converyed by an intermediate called message broker (message queue &#x2F; message-oriented middleware). Usually no data model is enforced, the message is just a sequence of bytes with some metadata. For consumer, if it wants to republish the message, it should keep all the unknown fields. Difference with RPC is that the message-passing is usually one-way.Sender does not expect replies, just send and move forward. Open source implementations: RabbitMQ, ActiveMQ, HornetQ, NATS, Apache KAFKA Advantages: Act as a buffer to improve system reliability; Redeliver messages to a crashed process, avoid it from being lost; Avoids the sender needing to know the IP address and port number of the recipent (useful in a cloud deployment where virtualmachines often come and go); One message to be send to serveral recipents; Logically decoupes the sender from the recipent (sender does not care who is the message consumer). Distributed actor frameworks: The actor model is a programming model for concurrency in a single process. Rather than dealing directly with threads (and the associated problems of race conditions, locking, and deadlock), logic is encapsulated in actors. Each can represent one client &#x2F; entity, may have some not shared local states.Process one message at a time, can be scheduled independently by framework. Communicating with each other by sending – receiving asynchronous messages. Delivery is not guranteed. In distributed actor framework, this programming model is used to scale an application across multiple nodes. Same message-passing mechanism is used regardless of sender &amp; recipents in the same node or not.If not, message will be encoded into byte sequence, send over network and decode on the other side. It essentially integrate message broker and the actor programming model into single framework.But still need to take care of forward &amp; backward compatibility. Location Transparency: ability to access objects without the knowledge of their location. In actor model, location transparency works better than model in RPC.The message is already assumed to be possibly lost, and we have less of a fundamental mismatch between local and remote communication. Examples: Akka uses Java’s built-in serialization by default, which does not provide forwardor backward compatibility. However, you can replace it with something like Protocol Buffers, and thus gain the ability to do rolling upgrades. Orleans by default uses a custom data encoding format that does not support rolling upgrade deployments;To deploy a new version of your application, you need to set up a new cluster, move traffic from the old cluster to the new one, and shut down the old one.Like with Akka, custom serialization plug-ins can be used. In Erlang OTP it is surprisingly hard to make changes to record schemas (despite the system having many features designed for high availability);Rolling upgrades are possible but need to be planned carefully.An experimental new maps datatype (a JSON-like structure, introduced in Erlang R17 in 2014) may make this easier in the future.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Data System","slug":"Data-Science/Data-System","permalink":"https://umiao.github.io/categories/Data-Science/Data-System/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Data System","slug":"Data-System","permalink":"https://umiao.github.io/tags/Data-System/"},{"name":"Designing Data-Intensive-Applications","slug":"Designing-Data-Intensive-Applications","permalink":"https://umiao.github.io/tags/Designing-Data-Intensive-Applications/"}]},{"title":"Designing Data-Intensive-Applications-Note-8","slug":"Designing-Data-Intensive-Applications-Note-8","date":"2024-02-25T18:58:40.000Z","updated":"2024-05-05T19:03:10.093Z","comments":true,"path":"2024/02/25/Designing-Data-Intensive-Applications-Note-8/","link":"","permalink":"https://umiao.github.io/2024/02/25/Designing-Data-Intensive-Applications-Note-8/","excerpt":"Discussion on Encoding and Schema.","text":"Discussion on Encoding and Schema. Encoding and Evaluation: System require evolvability, e.g., capture new field or record types. In relational database, we usually assume that all data in database follow only one valid pattern. However, in schema-on-read &#x2F; schemaless databases, it is possible that you can read a mixture of old &#x2F; newer data formats written at different times. In larger application, you may need rolling upgrade &#x2F; staged rollout. It deploys the new version into a few nodes at a time, checking whether the new version is running smoothly, gradually working your way through all nodes. This can enable update without service downtime, encourage more frequent releases. User may not update client-side applications in time.Thus we need： backward compatibility (newer code can read data that was written by older code) and forward compatibility (old code can read data written by new code).The prior is easier but the latter is harder and needs old ver of code to ignore additions made by the new codes. Programs usually work with data in at least two ways: In-memory (objects, structs, lists, arrays, hash tables, trees, can be effciently accessed and manipulated by CPU via pointers) Over network (byte sequence like JSON). By converting between them, we are doing encoding (serialization, marshalling) and decoding (deserialization, unmarshalling). Examples: For encoding in-memory objects, Java: java.io.Serializable (bad performance, bloated encoding); Ruby: Marshal; Python: pickle. Limitations Reading is restricted to the same language Hard to switch programming language in the receive end Prevents integration of system. Decoding process needs to be able to instantiate arbitrary classes, usually a source of security problems. Thus, use the language built in encoding is a bad idea. JSON, XML and binary variants:JSON and XML are well-known standardized encodings. However, XML is often criticized as too verbose and complicated, JSON is benefited from its built-in support in browsers and simplicity. In XML &#x2F; CSV, you cannot differ number from strings; In JSON, you can not differ int from float and no precision specified (for number larger than $2^{53}$, it may be an issue). Also, they do not support binary strings. You can use base64 to get around but the data size increased by 33%. JSON is still less effective than binary encoding. MessagePack, is a binary encoding for JSON which sacrifice human readability to make the encoding result size smaller (for 18.5% less size in the given example, as a tradeoff).Meanwhile, csv does not have schema at all, and you may need to deal with complex escaping rules. Thrift and Protocal Buffers: Apache Thrift and Protocol Buffers (protobuf) are binary encoding libraries that are based on the same principle, and a schema is required to define the type, field name and whether is optional. Thrift has 2 binary encoding formats: BinaryProtocol and CompactProtocol. Note that if a field is specified as required, this is only checked in run-time. Variable-length integers: rather than using a full eight bytes to store integer, we can instead encode in 2 bytes (units) and use the top bit of each byte to indicate if there are more bytes to come.Thrift has a dedicated list data type. Protocol Buffer: Is very similar to the Thrift CompactProtocol. However, it does not have a list &#x2F; array type, just a repeated marker for fields. Nice effect: Can change option into repeated; New code reading old data sees a list with zero or one elements (depending on whether the field was present) Old code reading new data sees only the last element of the list. Compatibility: You can add new fields to the structure, however, just cannot specify it as a required field or the runtime check will fail. Old codes can simply ignore unrecognized fields and new codes can adapt old schema. Thus, added fields should be optional with default values. In terms of delete, you can only delete optional fields and can never use the same tag number again. Changing data type is possible, but may come with risk of losing precision &#x2F; get truncated. Avro: Based on Protocol Buffer and thrift. 2 schema languages: (Avro IDL) intended for human editing, the other based on JSON and easier to be read by machine. No tag number &#x2F; field to identify type. Integers based on variable length encoding. This means that the binary data can only be decoded correctly if the code reading the data is using exactly the same schema as the code write the data. Any mismatch in the schema between the reader and the writer would result in incorrectly decoded data. Writer’s schema: the version of schema an application knows about when it writes (encodes) the dataReader’s schema: when read &#x2F; decode, the schema it expect to see. It can cause problem when there is mismatch between reader and writer schema. Schema evolution: Forward compatibility: have a new version of schema as writer; Backward compatibility: have a old version of schema as reader.You can only add or delete a field that has a default value.*In avro, if you want to allow a field to be null, you have to use a union type like: {null, long, string} Enforcing the writer schema: the schema can be reused (like declaring in the start of file), e.g., usage in hadoop, storing files with millions of records.You can specify a file format (object container files) to do this.We may have multiple authors across different times, so specify the version at the beginning of each encoded record and use author’s schema. For network connection case, processes can negotiate the schema version for the lifetime of the connection. Version number can be an incrementing integer or a hash of the schema. This is in fact, a pattern of dynamically generated schema.It can be flexibly configured during every run.Label &#x2F; type can be changed though name stay unchanged as the identifier.For statically typed languages like Java, C++ or C#, Thrift and Protocol Buffers are more friendly, allowing type checking and auto-completion.For dynamically typed programming languages like Javascript, Ruby or Python, there is not much point in generating code, and can become an obstacle to getting data. *Avro can support both.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Data System","slug":"Data-Science/Data-System","permalink":"https://umiao.github.io/categories/Data-Science/Data-System/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Data System","slug":"Data-System","permalink":"https://umiao.github.io/tags/Data-System/"},{"name":"Designing Data-Intensive-Applications","slug":"Designing-Data-Intensive-Applications","permalink":"https://umiao.github.io/tags/Designing-Data-Intensive-Applications/"}]},{"title":"Designing Data-Intensive-Applications-Note-7","slug":"Designing-Data-Intensive-Applications-Note-7","date":"2024-02-24T18:07:04.000Z","updated":"2024-05-05T19:03:15.599Z","comments":true,"path":"2024/02/24/Designing-Data-Intensive-Applications-Note-7/","link":"","permalink":"https://umiao.github.io/2024/02/24/Designing-Data-Intensive-Applications-Note-7/","excerpt":"Discussion on techniques related to data warehouse &#x2F; OLAP.","text":"Discussion on techniques related to data warehouse &#x2F; OLAP. Transaction: A bunch of operations, not necessarily having ACID (atomicity, consistency, isolation, and durability).In fact, it just allows low-latency read &#x2F; write comparing with batch processing jobs (this is more widely used by analytic purpose, rather than serving customers).Online Transaction Processing Systems (OLTP System) Small number of records read per query Fetch by key Random access and low-latency write based on user input Serve end-users via web application Based on latest state of data, usually have dataset from GB-TB Online Transaction Analytic Systems (OLAP System) Aggregate read on records Bulk or event stream write Used by internal analyst for decision support Data based on history of events; dataset from PB – TB as it is history data Data Warehouse Company has the trend to stop analyzing on OLTP database, instead on separate database (running analytic queries on OLTP can be expensive, and harm the performance, as they should be low-latency and has high-availability). Thus we expect data warehouse to be separate from OLTP, read-only, get stream of updates and transform them into an anlysis-friendly schema, clean and load to data warehouse. This is called Extract-Transform–Load (ETL).The information source can be from multiple databases.* Indexing optimization discussed above may work well for OLTP, but NOT for analytic queries. Data warehouses are common to be relational, as SQL is generally a good fit for analytic queries. There are many graphical data analysis tools that generate SQL queries, visualize the results and explore data like: drill-down (navigate from high-level summary to more detailed level, break down aggregated data into more granular levels) slicing (select a subset of data based on specific criteria or dimensions) dicing (like slicing, bat invloves selecting and analyzing data based on multiple criteria &#x2F; dim). Stars and Snowflakes: schema for analytics: Raw data tables are linked to Fact Table. Certain row in that table indicates certain events, e.g., when and where an item is sold at which price. Some columns in that table are attributes, and some are foreign key references (called dimension table, indicating event’s who, what, where, when, how, and why). Star Schema (dimensional modeling): the fact table is the center and connection to other tables are like the rays of the star.[Easier to use] Snowflake Schema: dimensions are further broken into sub-dimensions. In some table like product table, each row can reference foreign keys to represent brand &amp; types, rather than stroing them as string.[More normalized] Column oriented storage: Fact tables can be very large (trillions of rows) but dimension a tables are usually much smaller (millions of rows).Fact table can have a lot of columns, but we usually only access a few of them at a time.Based on the above facts, unlike most OLTP databases (including document databases) whose stroage is laid out on row-oriented fashion, we can instead all values from each column together. This helps reducing the overhead by NOT loading the not requested columns.Note that the column-oriented storage layout relies on each column file containing the rows in the same order. Parquest is a columnar storage format that applies to non-relational data as well, supporting a document data model. Column compression: The value sequences in many columns are quite repeatitive, indicating that compression can be used. An effective technique is bitmap encoding. Follows a graph about how bit map works: In the following example, we have 18 rows and 6 possible column values, so we can use 1 bit to store the occurrence of a certain value. It is possible to be further reduced by run-length encoding. It is especially efficent when we want to do boolean computation on a few values. Sort Order: In column storage, the order may not matter, but we can force certain (meaningful) column to be ordered and use it as indexing mechanism.This can also help compressing the column, as we can use run_length encoding for sorted column.It needs to be noted that such compression has the best performance on the first sorted key, e.g., secondary key will be group by the first key so we will not have so many duplicate values. It may be a good idea to store multiple copies of data, sorted in different way. In that way we are adding redundancy and being able to further optimize queries. The idea is like secondary indexing, but the different is that index is usually stored in one place, and only pointer &#x2F; offset is stored, but in this way, the values got duplicated as well. [Note that such optimization makes write harder, compression is even impossible for in-place methods like B-Tree]This can be optimized by LSM tree, which accumulate enough writes (first add to memory and added to sort structure), then to be updated to disk in batch. Memory bandwidth and vectorized processingPotential concerns include: Memory bandwidth The bandwidth from main memory into the CPU cache Branch mispredictions and bubbles in the CPU instruction processing pipeline Make use of single-instruction-multi-data (SIMD) instructions in modern CPUs. Column-oriented storage layouts are good for making use of CPU cycles, while reducing the volume of data needed to be loaded from disk. For example, the query engine can take a chunk of compressed column data that fitscomfortably in the CPU’s L1 cache and iterate through it in a tight loop (that is, withno function calls). A CPU can execute such a loop much faster than code thatrequires a lot of function calls and conditions for each record that is processed. Column compression allows more rows from a column to fit in the same amount of L1 cache.Operators, such as the bitwise AND and OR described previously, can bedesigned to operate on such chunks of compressed column data directly.This technique is known as vectorized processing Materialized aggregate: Rather than handling raw data every time, we can use materialized view to cache some counts &#x2F; sums which are frequently used. Difference with standard (virtual) view in relational database: materialized view are values of query result written to disk, while virtual view is just some stored queries. When underlying data changes, materialized view needs to be updated as it is a denormalized copy of the data.This comes with higher write cost, making it not often used in OLTP databases.They are more valuable in heavy-read data warehouses. Data Cube &#x2F; OLAP Cube: a grid of aggregates grouped by different dimensions.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Data System","slug":"Data-Science/Data-System","permalink":"https://umiao.github.io/categories/Data-Science/Data-System/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Data System","slug":"Data-System","permalink":"https://umiao.github.io/tags/Data-System/"},{"name":"Designing Data-Intensive-Applications","slug":"Designing-Data-Intensive-Applications","permalink":"https://umiao.github.io/tags/Designing-Data-Intensive-Applications/"}]},{"title":"Designing Data-Intensive-Applications-Note-6","slug":"Designing-Data-Intensive-Applications-Note-6","date":"2024-02-24T17:34:35.000Z","updated":"2024-05-05T19:03:21.086Z","comments":true,"path":"2024/02/24/Designing-Data-Intensive-Applications-Note-6/","link":"","permalink":"https://umiao.github.io/2024/02/24/Designing-Data-Intensive-Applications-Note-6/","excerpt":"Discussion on Indexes and Multi-Column Index solution (R-Tree).","text":"Discussion on Indexes and Multi-Column Index solution (R-Tree). Secondary Index The mapping may not be unique, i.e., we may have multiple rows mapped to the same key in secondary index. This can be solved by either[1] making each value in that key to be list of matching row identifiers, or by[2] making each key unique (adding a row identifer to the key). May help with Join. Storing values within the indexIn index, the key can just be the search keyword, but the value can be:[1] Actual row (or document &#x2F; vertex) in question[2] Reference to the row stored elsewhere(That file is called heap file, in no particular order). Heap file helps: Eliminating duplication, because if there lies multiple secondary index, only the reference of data is stored; Efficient at overwriting values, if new value is no larger than the old value. Otherwise, the new value needs to be moved to somewhere with enough space. In that case, we can either update all secondary indexes to be updated to point to the new location, or leave a forwarding pointer in place. Clustered index: If extra hop from index to heap file is too much of performance penalty for reads, we can directly store indexed row within index.E.g., In MySQL’s InnoDB engine, the primary key of a table is always a clustered index and secondary indexes refer to the primary key. In SQL server, you can specify one clustered index per table. Compromise between clustered &#x2F; non-clustered index: covering index or index with included columns, only stores some of the columns within the index. Both cluster and covering index can speed up the reads but require more storage &#x2F; write overhead. Multi-column indexes: Can help querying multiple columns of a table Trivial implementation can be concatenated index, which concat multiple columns. This is not very flexible as it is hard coded, you can hardly search by some column’s sub-pattern. Standard B-Tree &#x2F; LSM-tree index can hardly answer quries which restrict range on multiple columns. Implementation 1: To translate a multi-dimensional location into a single number using a space-filling curve, and then to use a regular B-tree index Implementation 2: A better choice is using spatial indexes like R-trees. R-Tree (R for Rectangle): An generalization of B-tree to high-dim space. Used Minimal Bounding Rectangle (MBR) for space dividing. That is to say, just like segment tree, we want a spatial “rectangle tree” so that given a node, we can get the sub-nodes &#x2F; leaves belonging to that region. This tree can be multi-way. Structure of leaf nodes: leaf nodes are saved as tuples of (l, tuple-identifier). Where tuple-identifer is a n-dim vector (can be realized as a single record &#x2F; data point) and l is a n-dim rectangle which can exactly include all the datapoints belonging to this leaf node. Stucture of non-leaf nodes: (l, child-pointer), where l is the n-dim rectangle, and child-point points to the child node(s). Variants of R-tree: R* tree (use re-insertion to reduce overlap and improve query performance, using a combination of a revised node split algorithm and the concept of forced reinsertion at node overflow).R-tree is sensitive to the order of insertion, thus we can improve it by: when a node overflows, a portion of its entries are removed and reinserted, limited to one time to avoid indefinite cascade re-insertion.) R+ Tree: a tradeoff betweem R-tree and kd-tree. If needed, insert an object into multiple leaves to avoid overlapping of internal nodes. Minimal coverage reduces the amount of “dead space” (empty area), and reduces the set of search path to the leaves.The difference is that R+ Tree is not guaranteed to be at least half-filled, the entries of any internal node do not overlap, and an object ID maybe stored in more than one leaf node. Full-text search and fuzzy indexes: Motivation: Instead of search the exact value, we want to allow search for similar keys like mis-spelled words. Full text search: Include synonyms of the word, to ignore grammatical variations ofwords, and to search for occurrences of words near each other in the same document, and support various other features that depend on linguistic analysis of the text. Lucene allows searching of words within a certain edit distance, using quasi-SSTable structure to get the offset in the sorted file where they need to look for the candidate keys. In LevelDB, this in-memory index is a sparse collection of some of the keys, but in Lucene, the in-memory index is a finite state automaton over the characters in the keys, similar to a trie. Keep everything in memory (persistent memory storage?): Disks are cheaper and durable (will NOT lose data when power off), but needs to be laid out carefully for good performance on reads and writes. Now memory is cheaper, and for use cases like Memcached, data loss is acceptable. For durability required by database, we can log changes to disk, replicating in-memory state to other machines, this also helps with back-up &#x2F; inspection and analysis. Memory databases are faster because they can avoid the overheads of encoding in-mem data to disk form. They can also provide priority queques and sets (by Redis, e.g.) which are hard to implement on disk. We can actually support memory database which requires more space than the memory (overlay?) When there is no space, evict content to disk, when needed, load it back. It is like OS memory management (swap) but in record granularity.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Data System","slug":"Data-Science/Data-System","permalink":"https://umiao.github.io/categories/Data-Science/Data-System/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Data System","slug":"Data-System","permalink":"https://umiao.github.io/tags/Data-System/"},{"name":"Designing Data-Intensive-Applications","slug":"Designing-Data-Intensive-Applications","permalink":"https://umiao.github.io/tags/Designing-Data-Intensive-Applications/"}]},{"title":"Designing Data-Intensive-Applications-Note-5","slug":"Designing-Data-Intensive-Applications-Note-5","date":"2024-02-24T08:38:40.000Z","updated":"2024-05-05T19:03:26.470Z","comments":true,"path":"2024/02/24/Designing-Data-Intensive-Applications-Note-5/","link":"","permalink":"https://umiao.github.io/2024/02/24/Designing-Data-Intensive-Applications-Note-5/","excerpt":"Discussion on Storage structures.","text":"Discussion on Storage structures. Storage and Retrieval:Real life DB may need concurrency control, reclaiming disk space so that log does not grow forever, handling errors and partially written records. Log: an append-only sequence of records. Index: additional structure that is derived from the primary data (faster read, slower write) Hash Indexes: basic implementation is to use in memory hash map to store byte offset (then you can generalize memory hash map into disk hash). Log run out of space:To cope with such risk, we can: Break log into segments of a certain size, Closing a segment file when it reaches certain size Make subsequent writes to new segment file.We can do compaction on segments to keep only the most recent data (this may even allow us to compress multiple segments into one). Good Practice in storage: Store file as binary format, first encode length of a string in bytes, followed by the raw string Consider tombstone (special delete record) to avoid really deleting records (“delete” execute on read time). Crash recovery: in memory cache will lost, we can consider storing a snapshot of each segment’s hash map on disk, which can be loaded into memory more quickly. Partially written result: we can keep checksums to detect corrupted parts. Concurrency control: we need the log to be strictly ordered, so better have one writer thread only. Reason of Append Only Design: Appending and segment merging are sequential write operations, which are generally much faster than random writes, especially on magnetic spinning-disk hard drivesor even flash-based solid state drives (SSDs).Crash recovery will also be easier as values will not be partially overwritten.Also we can avoid fragmented data file. Shortcomings of hash table index: It relies on memory, and performs poor on range queries. You have to go through all keys. SSTables and LSM-Trees: If we further require writes of segment files to be sequential (sorted by key), then it is a Sorted String Table (SSTable). Advantages over hash indexes: Easier to merge segments as they are ordered (applicable to merge sort). Tiebreak: if multiple keys exist, we only keep the latest record. (similar as the idea of using timestamp &#x2F; ItemID as additional rank keyword) Sparse Index: Now that we do not need to keep all keys’ indexes, as we can use similar idea of interpolation. As the storage is now ordered, we only need to find out the interval where a key might exist. Maybe we can keep an index every few KBs (like paging). Also it is possible to compress multiple keys into a compressed block. To save disk space &#x2F; IO cost. Maintenance: it is possible to maintain SSTables in disk (using B-Trees) but it is easier using memory (called memtable) with red-black trees &#x2F; AVL trees. Memtable When memtable gets bigger than threshold (a few MBs), write it to disk as an SSTable file, and the new SSTable file becomes the most recent segment of database. While writing to disk, writes can continue to a new memtable instance. When serving reads, try to find key in the memtable, then in reversed order to check on-disk segments (check the latest ones first) Run merging and compaction process from time to time to merge segment files, and remove out-dated values. Recovery of memtable: memory is not persistent, so keep a separate log on disk to record every write to restore crashed memtable (discard records after such restoration). LSM-Tree: details and optimization: MemTable resides in memory, is NOT persistent, and ordered by key. Can enhance reliability by Write-Ahead Logging (WAL). Memtable turn into Immutable Memtable after reaching certain size, and no longer writable. New memtable will handle write request, and Immutable Memtable is pending transit to SSTable. SSTable is LSM-Tree’s data structure in disk. Can be optimized using bloom-filter and indexes on key. Note that the updating of disk is journal &#x2F; log like, and the write is also in order. The write performance is improved, however there lies redundancy due to out-dated data (that is why we need compact). Also query needs to be in reverse order as that is the latest. Compact Strategy:Because we have amplified read &#x2F; write &#x2F; space, that is to say, we are reading (two level read of MemTable &#x2F; SSTable) &#x2F; writing (Compact may be triggered so we are writing more) &#x2F; storing (due to redundancy) more data than actually needed. Size-tiered: Each tier of SSTable has similar size, and restrict number (N) of SSTable in each tier. If the number of SSTables reach N, then compact them and pass to next tier.(This will result in huge SSTable in deeper level)(Also for same tier SSTables, each key may has multiple records) Leveled: Each level restricts the gross size of SSTables, and in the same level, slice data into SSTables with similar sizes. However, the SSTables are guranteed to be globally ordered so only stored once.Strategy: if a level exceeds the limit on size, then take out 1 SSTable, merge it with next level SSTables which have intersection. (This process can be recursive if still exceeds the size limit) Make LSM-tree out of SSTables: This is used by key-value storage engine like LevelDB and RocksDB. Known as Log-Structured Merge-Tree (LSM-tree). Instance: Lucene (mapping from term to postings kept in SStable-like sorted files), is an indexing engine for full-text search used by Elasticsearch and Solr. It uses a similar method for storing its term dictionary.The mentioned full-textr search engine actually stores all links related to a keyword as key, with list of resources as the value. B-Tree Standard index implementation in almost all relational databases Instead of break database into variable-size segments and always write sequentially, B-Tree break into fixed-size blocks &#x2F; pages (traditionally 4KB) and read or write one page at a time. It is more close to hardware, and each page can be identified using an address, stored on disk.Implementation Starting from the root, keep going down to narrow the range of indexes and eventually check existence. The number of references to child pages in one page of the B-tree is called the branching factor, usually a several hundred. To update a key, search for leaf page containing that key and write back. To add new key, find the page whose range encompasses the new key and add it to the page. If there is not enough space, split the page into 2 half-full pages. This ensures that the tree is balanced, has a depth of $O(\\log n)$. Most database can fit in a depth of 3 – 4. (A four-level tree of 4 KB pages with a branching factor of 500 can store up to 256 TB) Comparing with LSM-Tree: B-Tree’s rewrite is in-place and can be viewed as happening at hardware level, rather than on log level.Robustness If you split a page because an insertion caused it to be overfull, you need to write the two pages that were split, and also overwrite their parent page to update the references to the two child pages. This is a dangerous operation, because if the database crashes after only some of the pages have been written, you end up with a corrupted index (e.g., there may be an orphan page that is not a child of any parent). Solution: Include an additional data structure of Write-Ahead Log (WAL) (redo log), which is append-only file which records every B-tree modification before it applied to the pages. This can restore B-Tree to a consistent state. Also, if multiple threads want to access B-Tree, we will need lightweight locks for consistency, which is more complex than log-based structure. Optimizations Can replace WAL with a copy-on-write scheme. That is a modified page is written to a different location, and a new version of the parent pages in the tree is created to point at the new location. This is also useful for concurency control. We can save space in pages by abbreviating the keys, as we only need enough information to act as boundaries between key ranges (e.g., omit common prefixs). Thus we can have higher branching factor and fewer levels. We can try to layout the tree that leaf pages appear in sequential order in disk to save disk seeking time. However, it is difficult. For LSM-trees, such locality is easier as it rewrites large segments during merging. Comparison with LSM-Tree: LSM-tree is faster in write, and B-Tree is faster in read. B-tree must do write twice: once to WAL (Write-Ahead Log) and once to the tree page itself (need more when split is needed). Need to change a page at minimum. Some storage engines even overwrite the same page twice in order to avoid ending up with a partially updated page in the event of a power failure. Log-structured indexes also write data multi-times due to repeated compaction &amp; merging (write amplification), impacting SSD; disk bandwidth is also limited and may cause fragmentation. LSM-tree usually has smaller write amplification as it do not have to overwrite several pages in the tree. Also, magnetic hard drive’s sequential writes is much faster than random writes, giving LSM-tree more advantage.However, the compaction can impact read-write performance (due to limited disk bandwidth), even storage engines try to perform compaction incrementally and without affecting concurrent access.Initial write and compaction will share the same bandwidth. As database become larger, the compaction is less effective and may be the bottleneck of performance &#x2F; use up the disk space (write is usually not restricted in SSTable-based storage).For high-percentile, LSM-tree can have high response time and less predictable. Many SSD firmware will use log-structured algo to turn random writes into sequential write.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Data System","slug":"Data-Science/Data-System","permalink":"https://umiao.github.io/categories/Data-Science/Data-System/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Data System","slug":"Data-System","permalink":"https://umiao.github.io/tags/Data-System/"},{"name":"Designing Data-Intensive-Applications","slug":"Designing-Data-Intensive-Applications","permalink":"https://umiao.github.io/tags/Designing-Data-Intensive-Applications/"}]},{"title":"Designing Data-Intensive-Applications-Note-4","slug":"Designing-Data-Intensive-Applications-Note-4","date":"2024-02-24T00:38:51.000Z","updated":"2024-05-05T19:03:31.821Z","comments":true,"path":"2024/02/23/Designing-Data-Intensive-Applications-Note-4/","link":"","permalink":"https://umiao.github.io/2024/02/23/Designing-Data-Intensive-Applications-Note-4/","excerpt":"Discussion on data model and query language.","text":"Discussion on data model and query language. Concept of Data Model &#x2F; Query Language:We have the questions to answer:- We want to build abstraction, one data model layer on top of another, but we need to know how one layer is represented by below layer.- Data modeling &#x2F; API assignment. How to model the data, JSON &#x2F; XML? Table in DB? Graph Model? Relational Model v.s. Document Model: Relational: Data organized into relations (called tables in SQL) and each relation is an unordered collection of tuples (rows in SQL). Hide details behind cleaner interface. Document: NoSQL, retrieve document by name, each stores information of one object and any of its related metadata.Examples: JSON, BSON, XML.A collection is a group of documents. Driving Forces behind NoSQL (Maybe not only SQL is more accurate):- Greater scalability, like very large dataset or very high write throughput- Preference for free and open source software over commercial database products- Specialized query operations not well supported by relational DB- Frustration with the restrictiveness of relational schemas, and a desire for a more dynamic and expressive data model Shortcomings of document model: In document model, you cannot directly refer to some nested items (because it is not stored independently). Thus we should not use it when document structure is too complex &#x2F; nesting is too deep. Also it has poor support for Join. Object-Relational Mismatch:Usually, an awkward translation layer is required between the objects in the Relational Model (tables) Versus Document Model (sometimes named as impedance mismatch) Object-Relational Mapping (ORM) like ActiveRecord and Hibernate can reduce the amount of boilerplate code, but the difference still exists (such mismatch) ID can out-perform pure string in Many-to-One &#x2F; Many-to-Many mappings.Store fields as IDs and retrieve their values can help:1. Consistent styling2. Avoid ambiguity3. Ease of updating4. Localization support5. Better search matching Both stroing by ID &#x2F; storing by string has some sort of redundancy. The previous is for human, meaningful info is stored in just one place and everything refers to it use an meaningless ID. For string storage, you are copying the same values (might consume more storage).However, the benefit using ID is that you do not have to update ID, and can better enforce consistency when update is needed. In document model DB, such unifying can be hard as you may need to query DB for multiple times (Good at one-many, not good at many-one, many-many). Conference on Data Systems Languages (CODASYL) and the network model: Disclaimer: This may be something old and just material for fun Allowing an item to have multiple parents to model many-one &#x2F; one-many The link is more like pointers (static pointers stored on the disk), follow a path to traverse (start from root record) [This may be problematic in many-many cases, you need to maintain complex paths of data to update] Relational Model: Relation (table) is simply a collection of tuples (rows), you can read any of the rows and select those matching your condition. You can insert new row into table without thinking of foreign key issues (this is done in query time not in insert time) Key insight: For relational DB, you only need to build a query optimizer once, and then all applications that use the database can benefit from it.If you don’t have a query optimizer, it’s easier to hand-code the access paths (a.k.a the network model) for a particular query than to write a general-purpose optimizer—but the general-purpose solution wins in the long run. Document model: Different from network model, as it uses unique identifier (foreign key in relational, document reference in documental) to link. It is possible to use denormalization (add redundant copies, grouping data, etc) to reduce need of join (in relational DB), but it takes extra work to enforce consistency. Schemaless &#x2F; Schema flexibility in Document Model In JSON, usually we do not support enforcing of any schema (there may be some schema, but not enforced by the DB). We may see any value &#x2F; any key. schema-on-read: the structure of the data is implicit, and only interpreted when the data is read. Just like dynamic type checking rather than static type chekcing. This can be tricky for MySQL, as it copies the entire table when doing alter table. Data Locality of queries May grant performance advantage if you want to access entire document frequently.(If in multiple tables, then may need more disk seek time) Unless the modification does not change the encoded size, we usually need to rewrite entire document when making modifications. (thus should try to keep documents small) This idea is beyond document database, also applicable to relational DB. Google’s Spanner database: offers the same locality properties in a relational data model, by allowing the schema to declare that a table’s rows should be interleaved (nested) within a parent table. Oracle: Offers the same using a feature called multi-table index cluster tables. The column-family concept in the Bigtable data model (used in Cassandra and HBase) has a similar purpose of managing locality. More and more relational DB is adding support to JSON (allowing values to be nested obj, rather than only trivial values). Query Languages for Data:Declarative SQL is regarded as declarative. Just specify the pattern of result data you want, what conditions to meet, but not about how to receive this result. Usually order is not guaranteed as well, but it is good for parallelization Details are hidden, and optimization can be done without modifying the query Imperative IMS &#x2F; CODASYL query are using imperative code. Guide device to do certain operations in certain order. This can be harder to optimize &#x2F; rewrite on multiple machines HTML is declarative as well. Document Object Model (DOM) API is imperative instead (a bad rewrite using DOM API can be less interactive as it detect conditions &amp; runs only one time). Map-Reduce Map-Reduce can be in between of declarative and imperative, consisting of important operations of map (collect) and reduce (fold &#x2F; inject). Usually map func provides the partial result, and reduce func works on aggregation. Restrictions: they have to be “pure” functions, no additional queries can be done, no side effect should be made (so that they can be executed in any time &#x2F; any order). Graph-Like Data Model: Describe data with Vertices and edges. It is good for, e.g., describing social network and hierarchical relationships. Property Graphs: Each vertex consists a unique identifier, a set of outgoing edges, a set of incoming edges, a collection of properties (K-V pairs). Each edge consists of a unique identifier, the vertex which the edge starts, a label to describe the kind of relationship between the two vertices, a collection of properties (K-V pairs). Any two vertices can be linked; You can find the incoming &#x2F; outgoing edges for a vertex; You can use labels for different kinds of relationships, store different kind of info in a single graph. This actually allow us to store data in different granular. Cypher Query Language:Demos an example of creation of a property graph: Example of query: Such query effectively specify an endpoint &#x2F; destination vertex, and we want to find a path to get there. We first declare a person should have a “born_in” edge, then we want to keep tracking “within” edge to reach certain country &#x2F; address vertex. Application in SQL: This can be difficult as the number of “join”s needed is not clear. You can use RECURSIVE syntax (used to enlarge the same table &#x2F; set) for such purpose (but very complex and ineffective). Triple-Stores: Basically the same as property graph model, just using different words of: (subject, predicate, object)This just like (in_vertex, directional_edge, out_vertex) SPARQLA query language for triple-stores using the RDF (Resource Description Framework) data model.It looks like: DatalogSimilar triple-stores languages, write as predicate(subject, object).See example of decalration as well as query below: It can also recursively find a person who born in USA and lives in Europe.Note that in above example, rule “within_recursive” can derive itself. (You can view the first line of row as “tail recursive”)","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Data System","slug":"Data-Science/Data-System","permalink":"https://umiao.github.io/categories/Data-Science/Data-System/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Data System","slug":"Data-System","permalink":"https://umiao.github.io/tags/Data-System/"},{"name":"Designing Data-Intensive-Applications","slug":"Designing-Data-Intensive-Applications","permalink":"https://umiao.github.io/tags/Designing-Data-Intensive-Applications/"}]},{"title":"Designing Data-Intensive-Applications-Note-3","slug":"Designing-Data-Intensive-Applications-Note-3","date":"2024-02-24T00:28:51.000Z","updated":"2024-05-05T19:03:37.158Z","comments":true,"path":"2024/02/23/Designing-Data-Intensive-Applications-Note-3/","link":"","permalink":"https://umiao.github.io/2024/02/23/Designing-Data-Intensive-Applications-Note-3/","excerpt":"Discussion on maintainability, evolvability and operability.","text":"Discussion on maintainability, evolvability and operability. Maintainability Fixing bugs Keeping its systems operational Investigating failures Adapting it to new platforms Modifying it for new use cases Repaying technical debt Adding new features Try to focus on the following 3 design principles: OperabilityMake it easy for operations teams to keep the system running smoothly. Duties of operations teams include: Monitor health Track issue &amp; cause Update software and platform Track how systems interfere each other Manage deployment and configuration Anticipate and resolve future problems Perform complex maintenance System security Define process Preserve org-level knowledge about sys Examples of improving maintainability: Provide visibility into runtime behavior and internal of system Automation and integration Avoid dependency on individual machines Docs and operational models Good default behavior Self-healing of sys Exhibit predictable behavior. SimplicityMake it easy for new engineers to understand the system, by removing as muchcomplexity as possible from the system. Examples of complexity: Explosion of the state space, Tight coupling of modules, Tangled dependencies, Inconsistent naming and terminology, Hacks aimed at solving performance problems, Special-casing to work around issues Best Resolving practice: Introduce abstraction to hide details. Extract data system into well-defined, reusable components. Note this is not the same as simplicity of the user interface. EvolvabilityMake it easy for engineers to make changes to the system in the future, adapting it for unanticipated use cases as requirements change.Also known as extensibility, modifiability, or plasticity. Examples of Evolvability needs:- You learn new facts,- Previously unanticipated use cases emerge,- Business priorities change,- Users request new features,- New platforms replace old platforms,- Legal or regulatory requirements- Change, growth of the system forces architectural changes","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Data System","slug":"Data-Science/Data-System","permalink":"https://umiao.github.io/categories/Data-Science/Data-System/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Data System","slug":"Data-System","permalink":"https://umiao.github.io/tags/Data-System/"},{"name":"Designing Data-Intensive-Applications","slug":"Designing-Data-Intensive-Applications","permalink":"https://umiao.github.io/tags/Designing-Data-Intensive-Applications/"}]},{"title":"Designing Data-Intensive-Applications-Note-2","slug":"Designing-Data-Intensive-Applications-Note-2","date":"2024-02-24T00:01:35.000Z","updated":"2024-05-05T19:03:41.859Z","comments":true,"path":"2024/02/23/Designing-Data-Intensive-Applications-Note-2/","link":"","permalink":"https://umiao.github.io/2024/02/23/Designing-Data-Intensive-Applications-Note-2/","excerpt":"Topics about scalability, load &amp; press. Definition, metrics and mitigation.","text":"Topics about scalability, load &amp; press. Definition, metrics and mitigation. Scalability Systems can go wrong, with larger load and press. Examples of loads: requests per second to a web server ratio of reads to writes in a database number of simultaneously active users in a chat room Hit rate on a cache. Example: Twitter is more frequently read than write, so rather than the first version of: (when a user checking timeline, look up all the people they follow, merge their tweets), we can update everyone’s timeline when a post is created (version 2). Concern: number of followers is highly biased, and some head users’s posting can cause a lot of writes. Mixed solution: for celebrities, read their posts when creating timeline, and then merge with the preprocessed family tweets. Describe PerformanceWays to describe a system’s performance:1. Increase load param, so that system resources unchanged.2. Increase load param, measure extra sys resources needed to keep performance unchanged.Scenarios: In Hadoop, we care more about throughput. In online sys, we care more about response time. Latency: duration of a request waiting to be handled. Response time: client’s view, end to end time including network delays and queueing delays.Simply averaging these metrics is NOT good due to existence of outliers. Better checking percentiles.High percentiles (aka tail latencies) are important to user’s experience. Service Level Objectives v.s. Service Level AgreementsService Level Objectives (SLO) and Service Level Agreements (SLA) are contracts defining the expected performance and availability of a service. E.g., require median response time &lt; 200ms, P99 latency &lt; 1s (SLO), available for 99.9% of time (SLA). Random FactorsRandom factors which may cause same response taking different time:Random additional latency could be introduced by: Context switch to a background process. The loss of a network packet and TCP retransmission. Garbage collection pause. Page fault forcing a read from disk. Mechanical vibrations in the server rack. Queueing delayA few slow requests can hold up the processing of subsequent requests, known as head-of-line blocking. In real life, user would send requests no matter of whether the previous one is finished (this habit will result in a longer line in queue) We can have a solution of keeping all request within the time window, sort their response times every minute. We can have approximation algos like: Forward Decay (sounds like remove an item from a list with a probability related to its size, retire smaller ones). T-digest (some kind of sketching, use clustering to simplify. Use centroids (mean. weight) to replace a group of numbers). HdrHistogram (check if it is just histogram with low accuracy fixed-point num) if we cannot count percentiles accurately. *User can hardly tell a 200ms latency, and can usually bear a &lt;1s latency. Long latency will impair user experience. Solutions to cope with load: Scaling Up &#x3D; Vertical Scaling: Move to more powerful machine. Scaling Out &#x3D; Horizontal Scaling: Use more machines and distribute load. (Usually inevitable) Some sys can be elastic (adding resources automatically or manually). Tips &#x2F; Methodology Distributing stateful data sys to multiple machines can be complex, thus try to keep database on a single node (scale up) if possible Scalable architecture depends on the volume of reads, writes, data to store, data complexity, requirement of response time, access pattern or mixture of above. Usually, iterating quickly on product features is more important than scaling to some hypothetical future load.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Data System","slug":"Data-Science/Data-System","permalink":"https://umiao.github.io/categories/Data-Science/Data-System/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Data System","slug":"Data-System","permalink":"https://umiao.github.io/tags/Data-System/"},{"name":"Designing Data-Intensive-Applications","slug":"Designing-Data-Intensive-Applications","permalink":"https://umiao.github.io/tags/Designing-Data-Intensive-Applications/"}]},{"title":"Designing Data-Intensive-Applications-Note-1","slug":"Designing-Data-Intensive-Applications-Note-1","date":"2024-02-23T23:44:41.000Z","updated":"2024-05-05T19:03:47.774Z","comments":true,"path":"2024/02/23/Designing-Data-Intensive-Applications-Note-1/","link":"","permalink":"https://umiao.github.io/2024/02/23/Designing-Data-Intensive-Applications-Note-1/","excerpt":"Introduction to designing data intensive applications, a.k.a data systems, providing high-level ideas about what it is and why it is needed.","text":"Introduction to designing data intensive applications, a.k.a data systems, providing high-level ideas about what it is and why it is needed. Key Concepts Reliability: tolerating hardware &amp; software faults; as well as human error Scalability: Measuring load &amp; performance; latency, percentiles and throughput Maintainability: operability, simplicity &amp; evolvability Motivation: More applications are data intensive than compute-intensive. The amount of data, complexity and how fast it changes is the issue. Commonly used components: Store data so that they, or another application, can find it again later (databases) Remember the result of an expensive operation, to speed up reads (caches) Allow users to search data by keyword or filter it in various ways (search indexes) Send a message to another process, to be handled asynchronously (stream processing) Periodically crunch a large amount of accumulated data (batch processing) Meaningfulness of having the data system concept: The boundaries between categories got blurred: datastores that are also used as message queues (Redis), and there are message queues with database-like durability guarantees (Apache Kafka) Single tool is harder to handle multiple use cases and needs. So we can break down into tasks which can efficiently run on single tool. Selection of components and getting them to work make you also a data system designer Fault and FailureFault tolerant &#x2F; resilient: System anticipate faults (certain types of faults) and can cope with them.Fault $\\ne$ failure, as fault is one component of the system deviating from its spec, whereas a failure is when the system as a whole stops providing the required service to the user. We should try stop fault turn into failure. Intentionally triggering faults may help prevent failure (like release resources).This book focuses more on curable faults. Hardwares We assume hardware related errors to be random and independent. Hard disks are reported as having a mean time to failure (MTTF) of about 10 to 50years. Thus, on a storage cluster with 10,000 disks, we should expect on average one disk to die per day. Solution:Add Disk redundancy, like RAID configuration.Servers: dual power supplies and hot-swappable CPUsPower: batteries and diesel generators SoftwareFor AWS, it is fairly common for virtual machine instances to become unavailable without warning, as the platforms are designed to prioritize flexibility and elasticity over single-machine reliability. Software &#x2F; systematic error can be harder to anticipate, and can occur a lot more than hardware failures as they are correlated. Like software bug, or deplete of resources (CPU time, memory, disk, etc). The failures can also be cascading, one trigger another. Mitigation MethodsSome methods to help mitigate system failures: Carefully thinking about assumptions and interactions in the system. Thorough testing. Process isolation. Allowing processes to crash and restart. Measuring (do some validation &#x2F; verification), monitoring, and analyzing system behavior in production Human can be very unreliable, causing ~75% failures by false operation rather than 10-25% of hardware outages. Methods:1. Well-designed abstractions, APIs, and admin interfaces make it easy to do “theright thing” and discourage “the wrong thing.” (should reach a balance, not become too complex)2. Decoupling testing sandbox from prod.3. Thorough, full-grained test.4. Fast recovery &#x2F; rollback, gradual roll out, logs and data recomputation.5. Monitoring on metrics (telemetry).6. Good managing and training.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Data System","slug":"Data-Science/Data-System","permalink":"https://umiao.github.io/categories/Data-Science/Data-System/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Data System","slug":"Data-System","permalink":"https://umiao.github.io/tags/Data-System/"},{"name":"Designing Data-Intensive-Applications","slug":"Designing-Data-Intensive-Applications","permalink":"https://umiao.github.io/tags/Designing-Data-Intensive-Applications/"}]},{"title":"Name of Every Keyboard's Key","slug":"Name-of-Every-Keyboard-s-Key","date":"2023-09-18T05:52:17.000Z","updated":"2023-09-18T06:13:45.446Z","comments":true,"path":"2023/09/17/Name-of-Every-Keyboard-s-Key/","link":"","permalink":"https://umiao.github.io/2023/09/17/Name-of-Every-Keyboard-s-Key/","excerpt":"For better communication during teamwork &#x2F; paircoding &#x2F; code review… It is vital to keep every keyboard’s key’s name in mind.","text":"For better communication during teamwork &#x2F; paircoding &#x2F; code review… It is vital to keep every keyboard’s key’s name in mind. kbd { background-color: #eee; border-radius: 3px; border: 1px solid #b4b4b4; box-shadow: 0 1px 1px rgba(0, 0, 0, 0.2), 0 2px 0 0 rgba(255, 255, 255, 0.7) inset; color: #333; display: inline-block; font-size: 0.85em; font-weight: 700; line-height: 1; padding: 2px 4px; white-space: nowrap; } .tg {border-collapse:collapse;border-spacing:0;} .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; overflow:hidden;padding:10px 5px;word-break:normal;} .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;} .tg .tg-depl{background-color:#F7F8FA;color:#1D2129;text-align:left;vertical-align:top} .tg .tg-lq71{background-color:#F7F8FA;border-color:inherit;color:#1D2129;font-weight:bold;text-align:left;vertical-align:top} .tg .tg-d5mg{background-color:#F7F8FA;color:#1D2129;font-weight:bold;text-align:left;vertical-align:middle} .tg .tg-6zo8{background-color:#F7F8FA;color:#1D2129;font-weight:bold;text-align:left;vertical-align:top} .tg .tg-cw55{background-color:#F7F8FA;color:#1D2129;text-align:left;vertical-align:middle} Char English Name Chinese Name ~ tilde 波浪符 ` back quote 反引号 ! exclamation mark or bang 叹号 @ at 艾特 # hash or&nbsp;&nbsp;&nbsp;number or sharp 井号 $ dollar 美元符 % percent 百分号 ^ caret 脱字符、插入符 &amp; and or&nbsp;&nbsp;&nbsp;ampersand 与和符 * asterisk 星号 ( ) parentheses 圆括号、小括号 [ ] brackets 方括号、中括号 { } curly brackets 花括号、大括号 - hyphen or dash or minus 连字符、减号 _ underscore 下划线 + plus 加号 = equal 等号 / slash 斜线 \\ back slask 反斜线 | pipe or&nbsp;&nbsp;&nbsp;bar 竖线 : colon 冒号 ; semicolon 分号 ' single quote 单引号 \"&nbsp;&nbsp;&nbsp;\" quote 双引号 &lt; less than or angle brackets 小于 &gt; greater than or angle brackets 大于 , comma 逗号 . period or dot 句号 ? question mark 问号 ESC escape key 跳离键 Backspace backspace key 退格键 Insert insert key 插入建 Home home key 原位键 Delete delete key 删除键 End end key 结尾键 Page Up page up key 向上翻页键 Page&nbsp;&nbsp;&nbsp;Down page down key 向下翻页键 Enter enter key 回车键 Tab tab key 制表键 Caps&nbsp;&nbsp;&nbsp;Lock caps lock key 大写锁定键 ALT alternate key 可选键 CTRL control key 控制键 SHIFT shift key 上档键 Space space bar 空格键 Num num lock key 数字键盘锁定键","categories":[{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"Software Engineering","slug":"Job-Search/Software-Engineering","permalink":"https://umiao.github.io/categories/Job-Search/Software-Engineering/"}],"tags":[{"name":"Keyboard","slug":"Keyboard","permalink":"https://umiao.github.io/tags/Keyboard/"},{"name":"Term","slug":"Term","permalink":"https://umiao.github.io/tags/Term/"}]},{"title":"Basics About Linux & Vim Usage","slug":"Basics-About-Linux-Vim-Usage","date":"2023-09-18T05:16:13.000Z","updated":"2023-09-18T05:51:30.177Z","comments":true,"path":"2023/09/17/Basics-About-Linux-Vim-Usage/","link":"","permalink":"https://umiao.github.io/2023/09/17/Basics-About-Linux-Vim-Usage/","excerpt":"For a Software Engineer &#x2F; Researcher, Linux and Vim should be the basic of basics (as important parts of CLI, Command Line Interface). In this post, we have discussed some basic knowledge as well as useful tips.","text":"For a Software Engineer &#x2F; Researcher, Linux and Vim should be the basic of basics (as important parts of CLI, Command Line Interface). In this post, we have discussed some basic knowledge as well as useful tips. Notes about Linux Usage:File System Structure of LinuxA tree like directory, the root would be “&#x2F;”.Some of the root folders: “bin” storing binary executives “etc” for system configurations “home” for user files “lib” for libraries and modules “opt” for optionalpackages “root” for super user folder “tmp” for temporary files, etc. Format of commands:cmd [options] [arguments]The options and arguments all serve as part of input, use whitespace to separate.For single character option, use -option. For a word, use –-option (two -s). Wildcard Matching Operator “”* for any character of any length “?” for single character [ab..] for any character among a, b, …, [!ab] for any single character except a, b. File Type Normal file “-” Directory “d” Symbolic link “l” (hard link: point to a drive’s block of the file, just like a file; soft link: save an absolute path of a file) Character device file “c” Block device file “b” Socket “s” Named pipe file “p” Frequently Used Commands: “pwd”: show the current directory of current user “cd”: change dir “.”: current directory “..”: parent directory “-”: the directory before “cd” command got executed “~”: Absolute path name of the user’s main directory Absolute directory: start with “&#x2F;”, describe the complete path to the location of file. Relative directory: not start with “&#x2F;”, appoint a location in relative to your currentworking dir. Auto-Complete: can press “Tab” to auto fill the command name &#x2F; file name &#x2F; etc. Ifthere are too many possibilities, need multiple press. “ls”: show the information of file &#x2F; dir “mkdir”: create an empty dir in current dir “rmdir”: remove empty dir only “touch”: generate an empty file, or update the time of a file (-a, -m, -d) “cp”: copy file or directory “mv”: move &#x2F; rename file or dir “rm”: remove file or dir “ln”: create a symbolic link “find”: look up for a file “stat”: look for file’s type &#x2F; property “cat”: create file &#x2F; check the content of text file “more”: used to view the text files, displaying one screen at a time. “less”: similar to more, support search &#x2F; roll back “tail”: check the last n lines of file, with tail -n filename “head”: check the first n lines instead “echo”: redirect content into given files “|”: pipe command. Pass the result of prior command to latter command, like: ls -la | wc. This is for listing info and word count. “&gt;, &gt;&gt;”: &gt; would override content, &gt;&gt; would append content. “zip &#x2F; unzip &#x2F; gzip &#x2F; gunzip &#x2F;rar &#x2F; tar”: used to zip &#x2F; unzip &#x2F; pack. Regular Expression related: “grep”: stands for “global search regular expression”. Format: grep [options] PATTERN [FILE…]Examples: grep &#39;^[a-zA-Z]&#39; myfile grep -v &#39;^#&#39; myfile (reverse, only keep not matched result) grep -lr root &#x2F;etc&#x2F;* (recursively list all filenames under &#x2F;etc&#x2F; with“root”) Shell Variables:It can be classified as: Internal Variable: provided by the system, use only to the user (cannot be modified) Environment Variable: define the work env, can be used in shell; user can modify some of them User Variable: defined by user (often used in script). (Definition: varName&#x3D;Value, Reference: $varName). “export” can switch variables between “Global” and “Local” Some common shell env variables: HOME, LOGNAME, USER, PWD, MAIL, HOSTNAME,INPUTRC, SHELL, LANG, HISTSIZE, PATH, PS1, PS2. VI &#x2F; VIM (Visual Interface iMproved)Entering: vim +n filename (open file, place cursor to the beginning of nth line) vim + filename (open file, place cursor to the beginning of last line) vim +/pattern filename (open file, place cursor to first matched position) Modes: Normal: default mode when run vim Insert: press i, o, a to enter insert mode Cmdline: type commands starting with :, &#x2F;, ?, ! Under Normal Mode: G : jump to the end of file ZZ : save and exit ZQ : exit without save &#x2F; or ? : for string look up. (can be used with “n” for next match) n : Next. Search for next match. yy : copy one line p : paste to next line; P: paste to previous line dd : delete one line dw : delete a word; can use “dnw” to delete n words. d$ : delete to the end of current line. “dG” will delete to end of document. y : similarly, we can use “y” just as d to duplicate words. p : use p for pasting. Can use “10p” to paste for 10 times. x : delete the character at the cursor. u : Undo. Undo the last operation. “Ctrl + r” : to revoke the undo. “h,j,k,l” : move the cursor to left&#x2F;down&#x2F;up&#x2F;right “0” : move to head of line “^” : move to the first non-blank character “$” : move to end of line “g_” : move to the last non-blank character of current line “w” : move to the beginning of next word “e” : move to the end of next word “fa” : move to the position of next “a”. If use “Fa”, would move to previous a. “nfa” : would move to the nth “a”’s position in current line. Can use “;” to jump to next appointed character, “,” can jump to previous appointed character. “nG” : move cursor to the beginning of line n. “gg” : move cursor to beginning of line 1. “G” : move to the beginning of last line. “H &#x2F; M &#x2F; L”: move cursor to the beginning &#x2F; middle &#x2F; end of current screen “% &#x2F; * &#x2F; #” : match parenthesis &#x2F; match next word where cursor locates &#x2F; match previous word where cursor locates Under Insertion Mode:After entering this mode, any typed character would be recognized as content of file and then added. “Esc”: press to exit insertion mode (back to normal mode) Under Command Mode:Under Normal Mode, press “:” to enter Command mode.Vim would get into “Pending command” state. “:w” : save file “:w newfile” : save as a new file named “newfile” “:wq” : save and exit “:q!” : exit without save “:q” : save without changes made “:!command” : execute linux command, e.g., “date”, “ls”, … “:r !command” : e.g., “:r !date”: add the result to the position of cursor Environment Setup: “:set autoindent &#x2F; noautoindent” “:set number &#x2F; nonumber” : row number setting “:set ic &#x2F; noic” : require being insensitive &#x2F; sensitive to capitalized characters “:set tabstop&#x3D;value” : map tab into “value” spaces “:set all” : show all the configurable options “:n” : locate at line n “2, 5d” : delete lines with row numbers of 2 – 5. “:s&#x2F;aa&#x2F;bb&#x2F;g” : replace all “aa” with “bb” at current line. Replace “s” with “%s” would do the replacement across the entire document. Replace “s” with “n1, n2” would do the replacements for lines (with line number n1~n2). If omit “&#x2F;g”, would only replace the first appearance. If replace “&#x2F;g” with “&#x2F;gc”, confirmation would be required.","categories":[{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"Software Engineering","slug":"Job-Search/Software-Engineering","permalink":"https://umiao.github.io/categories/Job-Search/Software-Engineering/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://umiao.github.io/tags/Linux/"},{"name":"Vim","slug":"Vim","permalink":"https://umiao.github.io/tags/Vim/"},{"name":"CLI","slug":"CLI","permalink":"https://umiao.github.io/tags/CLI/"}]},{"title":"Learn English Vocabulary by Frequency","slug":"Learn-English-Vocabulary-by-Frequency","date":"2023-09-16T06:01:32.000Z","updated":"2023-09-16T17:31:55.354Z","comments":true,"path":"2023/09/15/Learn-English-Vocabulary-by-Frequency/","link":"","permalink":"https://umiao.github.io/2023/09/15/Learn-English-Vocabulary-by-Frequency/","excerpt":"As an engineer working in the industry, English is a very important and basic tool…Also, if we think like a machine, it would be then very natural for us to study English Vocab in its frequency order.. A weird idea, right?","text":"As an engineer working in the industry, English is a very important and basic tool…Also, if we think like a machine, it would be then very natural for us to study English Vocab in its frequency order.. A weird idea, right? English Words (not familiar to me) (in descending order of frequency) This post is based on this dataset [link](https://www.kaggle.com/datasets/rtatman/english-word-frequency) shared by Kaggle. > In fact, I already found this source is highly skewed, since it is likely crawled from Internet. ‘eBay’ has been a very frequent word, which might because of hundreds millions of listings going into this dataset. kbd { background-color: #eee; border-radius: 3px; border: 1px solid #b4b4b4; box-shadow: 0 1px 1px rgba(0, 0, 0, 0.2), 0 2px 0 0 rgba(255, 255, 255, 0.7) inset; color: #333; display: inline-block; font-size: 0.85em; font-weight: 700; line-height: 1; padding: 2px 4px; white-space: nowrap; } A very useful hotkey: you can use Ctrl + Shift + Home / End to quickly select all the content above / beneath your cursor. This would be super useful if you want to skip a bunch of high freq words! #customers { font-family: Arial, Helvetica, sans-serif; border-collapse: collapse; width: 100%; } #customers td, #customers th { border: 1px solid #ddd; padding: 5px; } #customers tr:nth-child(even){background-color: #f2f2f2;} #customers tr:hover {background-color: #ddd;} #customers th { padding-top: 12px; padding-bottom: 12px; text-align: left; background-color: #04AA6D; color: white; font-size: 10px;} Word Meaning Frequency lyrics words of song; poem expresses writer's emotions 92579731 catalog (make) a list of items 57567413 Dave Name; Hebrew Dawid meaning \"beloved\" or \"favourite\" 32281596 vintage year or place wine is produced; Adj: denoting high quality wine 31830831 provisions Verb: set aside, supply; Noun: action / thing of supplying 31126350 voyeur people enjoy seeing pain/sex/distress 27928748 census official count of a population 25385152 glossary alphabetical list of terms 23506998 fiscal relating to government revenue (taxes) 23506040 &lt;/tbody&gt; &lt;/table&gt;","categories":[{"name":"Productivity","slug":"Productivity","permalink":"https://umiao.github.io/categories/Productivity/"}],"tags":[{"name":"English","slug":"English","permalink":"https://umiao.github.io/tags/English/"},{"name":"Vocabulary","slug":"Vocabulary","permalink":"https://umiao.github.io/tags/Vocabulary/"},{"name":"Word Frequency","slug":"Word-Frequency","permalink":"https://umiao.github.io/tags/Word-Frequency/"}]},{"title":"Options Futures and Other Derivatives - Note - 2","slug":"Options-Futures-and-Other-Derivatives-Note-2","date":"2023-09-11T06:59:30.000Z","updated":"2024-05-05T19:03:49.592Z","comments":true,"path":"2023/09/10/Options-Futures-and-Other-Derivatives-Note-2/","link":"","permalink":"https://umiao.github.io/2023/09/10/Options-Futures-and-Other-Derivatives-Note-2/","excerpt":"Discussing different types of traders.","text":"Discussing different types of traders. Types of tradersThree broad categories of traders can be identified: Hedgers: Use derivatives to reduce the risk that they face from potential future movements in a market variable. Speculators: Bet on the future direction of a market variable. Arbitrageurs: Take offsetting positions in two or more instruments to lock in a profit. Hedgers","categories":[{"name":"Investment","slug":"Investment","permalink":"https://umiao.github.io/categories/Investment/"}],"tags":[{"name":"options","slug":"options","permalink":"https://umiao.github.io/tags/options/"},{"name":"futures","slug":"futures","permalink":"https://umiao.github.io/tags/futures/"},{"name":"trading","slug":"trading","permalink":"https://umiao.github.io/tags/trading/"},{"name":"investment","slug":"investment","permalink":"https://umiao.github.io/tags/investment/"}]},{"title":"Options Futures and Other Derivatives - Note - 1","slug":"Options-Futures-and-Other-Derivatives-Note-1","date":"2023-09-09T19:42:55.000Z","updated":"2023-09-11T06:58:48.832Z","comments":true,"path":"2023/09/09/Options-Futures-and-Other-Derivatives-Note-1/","link":"","permalink":"https://umiao.github.io/2023/09/09/Options-Futures-and-Other-Derivatives-Note-1/","excerpt":"Learning note of “Options Futures and Other Derivatives”, 11th Edition.","text":"Learning note of “Options Futures and Other Derivatives”, 11th Edition. Introduction Derivatives exchange is a market where individuals trade standardized contracts that have been defined by the exchange. Chicago Mercantile Exchange (CME) Open outcry system is being replaced by electronic trading -&gt; growth in algorithmic trading. Over-the-counter Market Alternative to exchanges; larger than the exchange-traded market (in total volume of trading) Usually between financial institutions; prepared to quote both a bid price (a price at which they are prepared to buy) and an offer price (a price at which they are prepared to sell). The terms of a contract do not have to be those specified by an exchange. Market size rises to $600+ trillion in 2022. Forward Contracts Agreement to buy or sell an asset at a certain future time for a certain price. A spot contract is an agreement to buy or sell an asset today. One of the parties to a forward contract assumes a long position and agrees to buy the underlying asset on a certain specified future date for a certain specified price. The other party assumes a short position and agrees to sell the asset on the same date for the same price. Payoffs from Foward ContractsAssuming we have forward quotes for the USD&#x2F;GBP exchange rate, for 6-month forward, the bid price: 1.4416 and offer price: 1.4422, then we can buy 1M GBP for 1.4422M USD.If the spot exchange rate rose to 1.5000, at the end of the 6 months, the forward contract would be worth $57,800 (&#x3D; 1,500,000 - 1,442,200). Similarly, if the spot exchange rate fell to 1.3500 at the end of the 6 months, the forward contract would have a negative value to the corporation of $ 92,200. We assume the Delivery price&#x3D;$k$, and price of asset at contract maturity &#x3D; $S_T$.The payoff from a long position in a forward contract on one unit of an asset is $S_T - K$, and for a short position, the payoff is $K - S_T$. Forward Prices and Spot PricesForward Prices and Spot Prices are tightly related. If a a stock that pays no dividend and is worth 60, and you can borrow or lend money for 1 year at 5%, then the 1-year forward price should be 63. If not, then you can hold long &#x2F; short forward contracts for profit. Future ContractsLike a forward contract, a futures contract is an agreement between two parties to buy or sell an asset at a certain time in the future for a certain price.Unlike forward contracts, futures contracts are normally traded on an exchange. Options Options are traded both on exchanges and in the over-the-counter market. A call option gives the holder the right to buy the underlying asset by a certain date for a certain price. A put option gives the holder the right to sell the underlying asset by a certain date for a certain price. The price in the contract is known as the exercise price or strike price; The date in the contract is known as the expiration date or maturity. American options (most of the options) can be exercised at any time up to the expiration date. European options can be exercised only on the expiration date itself. One contract is usually an agreement to buy or sell 100 shares. It should be emphasized that an option gives the holder the right to do something.The holder DOES NOT have to exercise this right. This is what distinguishes options from forwards and futures, where the holder is obligated to buy or sell the underlying asset. The price of a call option decreases as the strike price increases (negatively correlated), while the price of a put option increases as the strike price increases (positively correlated).Both types of option tend to become more valuable as their time to maturity increases (increased uncertainty, converged to the delivery price). ExampleSuppose an investor instructs a broker to buy one December call option contract on Google with a strike price of $520. The broker will relay these instructions to a trader at the CBOE and the deal will be done. The (offer) price is $32.00, this is the price for an option to buy one share.In the United States, an option contract is a contract to buy or sell 100 shares. Therefore, the investor must arrange for $3,200 to be remitted to the exchange through the broker. The exchange will then arrange for this amount to be passed on to the party on the other side of the transaction. The investor got the right to by 100 Google shares for $ 520 each at the cost of 3200 USD (cost on options). If Google reached 600 USD and the option is exercised, the profit would be $(600-520)*100-3200&#x3D;4800$. If Google stayed off 520 USD, the maximal loss is made (the premium) since the holder of the option won’t exercise it. If a call &#x2F; put option is sold, the sellar would immidiately receive the premium. For now, we discussed 4 types of participants in options markets: Buyer of calls Seller of calls Buyer of puts Seller of puts Buyers hold long position, and sellers hold short position. Selling an option also known as writing the option.","categories":[{"name":"Investment","slug":"Investment","permalink":"https://umiao.github.io/categories/Investment/"}],"tags":[{"name":"options","slug":"options","permalink":"https://umiao.github.io/tags/options/"},{"name":"futures","slug":"futures","permalink":"https://umiao.github.io/tags/futures/"},{"name":"trading","slug":"trading","permalink":"https://umiao.github.io/tags/trading/"},{"name":"investment","slug":"investment","permalink":"https://umiao.github.io/tags/investment/"}]},{"title":"Make Full Use of Apple Watch","slug":"Make-Full-Use-of-Apple-Watch","date":"2023-09-04T19:48:04.000Z","updated":"2023-09-05T02:22:29.955Z","comments":true,"path":"2023/09/04/Make-Full-Use-of-Apple-Watch/","link":"","permalink":"https://umiao.github.io/2023/09/04/Make-Full-Use-of-Apple-Watch/","excerpt":"My pup sent me an Apple Watch as a gift, and for long time I have now idea how to make full use of it.","text":"My pup sent me an Apple Watch as a gift, and for long time I have now idea how to make full use of it. Control Center &#x2F; 控制中心 打开控制中心：从表盘上向上轻扫。从其他屏幕，按住屏幕底部，然后向上轻扫。 【注】你不能从 Apple&nbsp;Watch 的主屏幕打开“控制中心”。而需要按一下数码表冠前往表盘或打开 App，然后打开“控制中心”。 关闭控制中心：从屏幕顶部向下轻扫，或者按一下数码表冠。 图标 描述 有关更多信息 断开与无线局域网的连接。 轻触以连接/断开网络。 呼叫你的 iPhone。 轻触就会叮叮叮！ 检查电池百分比。 静音 Apple&nbsp;Watch。 如果 Apple Watch 正在充电，闹钟和计时器即使在静音模式中仍会响起。 打开剧院模式。 剧院模式可防止 Apple Watch 屏幕在抬腕时亮起，以始终保持屏幕熄灭状态。还会打开静音模式，但你仍会收到触感通知。 选取专注模式/勿扰模式。 可于设置中打开并设置专注时间。 关闭“睡眠”专注模式。 可使用睡眠 / health app 管理睡眠计划、质量等。长按表冠以从睡眠模式解锁。 打开手电筒。 向左轻扫以选取一种模式：稳定的白光、闪烁的白光或者稳定的红光。 打开飞行模式。 打开入水锁定。 开启防水功能以防误触。 选取音频输出。 切换音频输出，如连接至蓝牙设备。 检查耳机音量。 更改文字大小。 打开或关闭“对讲机”。 要求足够高的watchOS与iOS版本，可以使用FaceTime，并使用Walkie-Talkie程序（从联系人中选取添加好友、接受邀请、进行对话）。 APP &#x2F; 应用小技巧 “健康”等相关应用用于检测运动情况、心率、睡眠等。 “计算器”速算小费（TIP）等。 “相机”可远程唤起手机相机，用于自拍等。 “指南针”中包含了Pin（记录位置、停放车辆等）、足迹追踪（Back Tracking， 防止迷路）等功能。 “微信”可强制发起弹窗（长按联系人的一条信息，即可发出信息 &#x2F; Signal)。","categories":[{"name":"Productivity","slug":"Productivity","permalink":"https://umiao.github.io/categories/Productivity/"}],"tags":[{"name":"Productivity","slug":"Productivity","permalink":"https://umiao.github.io/tags/Productivity/"},{"name":"Apple Watch","slug":"Apple-Watch","permalink":"https://umiao.github.io/tags/Apple-Watch/"},{"name":"Tips","slug":"Tips","permalink":"https://umiao.github.io/tags/Tips/"}]},{"title":"Need-To-Knows For Software Security Engineer","slug":"Need-To-Knows-For-Software-Security-Engineer","date":"2023-09-01T02:35:51.000Z","updated":"2023-09-01T03:45:50.741Z","comments":true,"path":"2023/08/31/Need-To-Knows-For-Software-Security-Engineer/","link":"","permalink":"https://umiao.github.io/2023/08/31/Need-To-Knows-For-Software-Security-Engineer/","excerpt":"This post covers common software &#x2F; cyber-security threats and preventions. Including SQL Injection, Cross-Site Scripting (XSS), OS Command Injection, Weak Session Token Generation and Missing Function Level Access Control.","text":"This post covers common software &#x2F; cyber-security threats and preventions. Including SQL Injection, Cross-Site Scripting (XSS), OS Command Injection, Weak Session Token Generation and Missing Function Level Access Control. SQL Injection Example of attack:SELECT * FROM Users WHERE Username = ‘admin’ AND Password = abc’ OR 1=1;— Here, “abc’ OR 1&#x3D;1;—” is the constructed malicious input. ‘—’ (double dashes) states end of line. Solution:Use parameterized query: Wrong Code:Straight forward concatenation (trust user’s input as part of the SQL command).VALUES (request_user_name “,” + request_user_age + “”) Correct Code:Parametrized query:insert_user = db.prepare “INSERT INTO users (name, age) VALUES (?, ?)” insert_user.execute (request_user_name, request_user_age). Other ways of injection attack: Union attack: use UNION to extend the results returned by the original query. It requires both queries to contain the same amount of columns. Conclusion: All popular dev frameworks have secure construction of db queries. Use allowlist validation on all user input. (e.g., ban the ‘) Apply least privilege principle on all backend db users. Consider GET and POST parameters, Cookies and other HTTP headers. Cross-Site Scripting (XSS)Example: (https://bank.com/search.html?keyword=&#60;script&gt;document.Location=“https://phising.com”&#60;/script&gt;)The user recognized a valid and known host name is the URL, clicks on the link. Surfs to the URL. The website (http://bank.com) does not validate or encode the params before rendering &#x2F; reflecting it back to the user.Then, the browser may execute the injected JavaScript and redirects the user to a phishing site, tricking them into submitting their passed. Prevention: you should never trust the user input. All user input that is rendered on the application output should be validated or encoded.The essential is that, if you do not validate the user input, the unescaped input would be displayed in an immediate response to the user. This would allow attacker to perform actions within the application on behalf of other users. Frameworks offer specific API calls to protect the application from XSS by HTML encoding untrusted user input. Cookies should be securely configured. Set the HttpOnly flag to prevent scripts from accessing them. Note that you should not mark strings as “safe”, e.g., &#123;&#123;recipe_searched|safe&#125;&#125;. Otherwise, the engine would escape the user input so that they would not got executed &#x2F; interpreted as html &#x2F; js. Eliminating such dangerous use cases can help prevent Cross-Site Scripting, which should be already well prevented by the dev framework. OS Command InjectionSuch vulnerability can happen when user controlled input, through parameters, cookies, http headers, etc are passed to the system shell without any prior validation. Example: url&#x2F;delete?fileToDelete&#x3D;aFile.txt;&#183;rm -rf &#x2F;var&#x2F;www&#183;If the application simply appends the GET params to the command string, the malicious command would got executed.Note that in this case, the command would run with the privilege of the running application.Note that many of the characters needs to be “url encoded” in order to be transferred and parsed correctly. Solution: Use framework specific API calls instead of OS commands, If not possible, validate all user controlled output against a white-list before passing to the shell. Note that you should apply least privilege to the application.Also,!!! Never set shell=True. This would bypass the validation and have it executed in the shell directly.You can make use of “check_output” to validate if the user provided part is a shell parameter. If not, then you should fail it. Remote File InclusionOpen any file &#x2F; Include in the page makes the application vulnerable to remote file inclusion injection. An adversary could upload a malicious script to be executed on the client-side server.You can specify it as “text&#x2F;plain” also set security policy to solve the issue. File TraverseUsing the URL GET parameter to open local files makes the application vulnerable to Path Traversal attacks.An adversary could modify the image parameter path to include any server file in the application page (e.g. ?image=/etc/paswd). Solution: Remove file response. Also not open server files from URL Get Params. Weak Session Token GenerationIf the generation is too weak, an attacker may easily associate to another active user’s session with constructed ID. Solution: Use built-in session management functionalities, instead of inventing your own. Store the session ID in a cookie and then protect session cookies. This can be done by setting an expiry timestamp, path,“secure” and “HttpOnly” flag and invalidate on logout. Session ID properties must be secure. Make them unpredictable, time limited and single session.Use a secure communication channel. Missing Function Level Access ControlDefinition: user can perform functions that they are not authorized for, or when resources can be accessed by unauthorized users.When access checks have not been implemented, or when a protection mechanism exists but is not properly configured. Solution: Protect all business functions using a role based authorization mechanism, implemented on the server side. Authorization should be applied using centralized routines, provided by the framework or external modules. Deny access by default. Use least privilege principle. Implement function access control on the server, never the client.","categories":[{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"Software Engineering","slug":"Job-Search/Software-Engineering","permalink":"https://umiao.github.io/categories/Job-Search/Software-Engineering/"}],"tags":[{"name":"Software Engineering","slug":"Software-Engineering","permalink":"https://umiao.github.io/tags/Software-Engineering/"},{"name":"Cyber Security","slug":"Cyber-Security","permalink":"https://umiao.github.io/tags/Cyber-Security/"},{"name":"Certificate","slug":"Certificate","permalink":"https://umiao.github.io/tags/Certificate/"}]},{"title":"Object Oriented Design -- Principles and Practices","slug":"Object-Oriented-Design","date":"2022-09-05T07:09:14.000Z","updated":"2022-09-17T15:52:20.062Z","comments":true,"path":"2022/09/05/Object-Oriented-Design/","link":"","permalink":"https://umiao.github.io/2022/09/05/Object-Oriented-Design/","excerpt":"Important principles as well as solution to concrete interview questions.","text":"Important principles as well as solution to concrete interview questions. SOLID PrinciplesSOLID stands for: S (SRP): The Single Responsibility PrincipleThere should NEVER be more than one reason for a class to change.(A class should be changed and seperated, if it needs to undertake multiple type of responsibilities) O (OCP): The Open Closed PrincipleThe software should be expandable instead of immutable. That is to say, being open to extension and close to mutation. L (LSP): The Liskov Substitution PrincipleOnly when an instance of a child class is able to replace any instance of its super class, we can say there is an is-A relationship between the two classes. I (ISP): The Interface Segregation PrincipleWe should not force people to be dependent on the interfaces which they do not use.Using multiple specialized interfaces is better than a single universal interface. D (DIP): The Dependency Inversion PrincipleHigh level modules should not be dependent on low level modules. Both should be dependent on abstraction. Abstraction should not be dependent on details, details shoulde be dependent on abstraction. Design Patterns Observer Design Pattern: A subject corresponds to multiple observers, and notifies them of any state changes. Usually used for implementing event handling systems in “event driven” software. The observers are physically separated and have no control over the emitted events and their sources. Most implementations use background threads listening. The sole responsibility of a subject is to maintain a list of observers and to notify them of state changes by calling their update() operation. The responsibility of observers is to register (and unregister) themselves on a subject (to get notified of state changes) and to update their state (synchronize their state with the subject’s state) when they are notified. Composite Design Pattern: A part-whole hierarchy should be represented so that clients can treat part and whole objects uniformly. (Define a unified Component interface for different parts) A part-whole hierarchy should be represented as tree structure. (Requests can be forwarded to their child components) Try not to differentiate the leaf nodes and branch, have all the objects showing similar behavior. State Design Pattern: Allows an object to alter its behavior when its internal state changes. This can be a cleaner way for an object to change its behavior at runtime without resorting to conditional statements and thus improve maintainability. Aim to solve the tasks of: An object should change its behavior when its internal state changes. State-specific behavior should be defined independently. That is, adding new states should not affect the behavior of existing states. Essence: State should be defined as an object. A class delegate state-specific behavior to the state object, rather than implement it itself. Singleton Design Pattern: Only allow one “single” instance of a class to be initialized. The only instance can be stored as class static variable and use hidden &amp; private constructor. OOD Interview TopicsDesign an elevatorRequirement: an elevator would finally stopped at the required floor. It would head towards the same direction, until all the requests (to that direction) have been solved. We can try to abstract the following objects &#x2F; classes from the real-life elevator: elevator (define its state, properties and restrictions, etc) panel (serve as container of buttons) button request (signal triggered by button) controller (schedule &amp; control the elevator). Also, the following facts should be noted: Normally, the status of an elevator can only be: up, down and idle. There may be more states like under repair &amp; inspection. You should not abstract passengers into a class, because the elevator can be agnostic of certain passenger &#x2F; items. It only check the total weight of them to decide if they can fit into one single ride. Design a parking lotWe can try to abstract the following objects &#x2F; classes from the real-life parking lot: Parking spot Entrance Cashier &#x2F; Management System Timing System Ticket Printer Design a playlistThis is mainly about system design I believe. Comprehensively consider the data structure and pipeline to be applied to optimize the performance. Work on the requirement to decide which part would need random access ability and find suitable data structure.","categories":[{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"}],"tags":[{"name":"OOD","slug":"OOD","permalink":"https://umiao.github.io/tags/OOD/"},{"name":"Object Oriented Design","slug":"Object-Oriented-Design","permalink":"https://umiao.github.io/tags/Object-Oriented-Design/"},{"name":"Software Engineering","slug":"Software-Engineering","permalink":"https://umiao.github.io/tags/Software-Engineering/"}]},{"title":"Collection and Solution of Brainteasers - 1","slug":"brainteaser_1","date":"2022-07-29T07:35:43.000Z","updated":"2022-07-29T23:33:41.462Z","comments":true,"path":"2022/07/29/brainteaser_1/","link":"","permalink":"https://umiao.github.io/2022/07/29/brainteaser_1/","excerpt":"Strange questions you would expected to meet only in interviews of financial &#x2F; trading firms LOL!","text":"Strange questions you would expected to meet only in interviews of financial &#x2F; trading firms LOL! 1 - Screwy piratesQuestion: Five pirates looted a chest full of 100 gold coins. Being a bunch of democratic pirates, they agree on the following method to divide the loot:The most senior pirate will propose a distribution of the coins. All pirates, including themost senior pirate, will then vote. If at least 50% of the pirates (3 pirates in this case)accept the proposal, the gold is divided as proposed. If not, the most senior pirate will befed to shark and the process starts over with the next most senior pirate … The process isrepeated until a plan is approved. You can assume that all pirates are perfectly rational:they want to stay alive first and to get as much gold as possible second. Finally, beingblood-thirsty pirates, they want to have fewer pirates on the boat (kill as many as possible) if given a choice between otherwise equal outcomes.How will the gold coins be divided in the end? Solution: Infer from the most trivial case. One pirate: get all $100$ gold. (1 v 0) Two pirate: The 2nd pirate would deny the 1st’s plan anyway to get all $100$ golds. Thus, 1st pirate’s expectation is $0$. (1 v 1) Three pirate: Now, the pirate who’s the first to move only need to give $1$ gold to the one who is second to move. (2 v 1) Four pirate: In the three pirate case, the last pirate’s equity is denied, so the first to move give him $1$ gold. (2 v 2) Thus, for any $2n+1$ pirate case ($n &lt; 99$), the first-to-move pirate ($2n+1$) offer the pirate $1$, $3$, …, $2n-1$ one coin and keep the rest. 2 - Tiger and sheepQuestion: One hundred tigers and one sheep are put on a magic island that only has grass. Tigers can eat grass, but they would rather eat sheep. Assume: A. Each time only one tiger can eat one sheep, and that tiger itself will become a sheep after it eats the sheep. B. Alltigers are smart and perfectly rational and they want to survive. So will the sheep beeaten? Solution: Trace back from the trivial case, like $1$ tiger ($n&#x3D;1$) and $1$ sheep. The the tiger would eat the sheep. When $n&#x3D;2$, no tiger would eat the sheep, otherwise, the case would be transformed to $n&#x3D;1$ and the tiger who eat the sheep would be eaten. Similarly, we can have that when $n&#x3D;3$, or $n&#x3D;2k-1, \\forall k \\ge 1$, a tiger can eat the sheep. 3 - River crossingQuestion: Four people, A, B, C and D need to get across a river. The only way to cross the river is by an old bridge, which holds at most 2 people at a time. Being dark, they can’t cross the bridge without a torch, of which they only have one. So each pair can only walk at the speed of the slower person. They need to get all of them across to the other side asquickly as possible. A is the slowest and takes $10$ minutes to cross; B takes $5$ minutes; Ctakes $2$ minutes; and D takes $1$ minute.What is the minimum time to get all of them across to the other side?Solution:The key is to hide the latency lol. That is to say, A and B should cross in the same round and never get back. Thus, the plan would be like: ($\\rightarrow$, CD), ($\\leftarrow$, D), ($\\rightarrow$, AB), ($\\leftarrow$, C), ($\\rightarrow$, CD). The total time consumption would be $17$ mins. 4 - Birthday problemQuestion: You and your colleagues know that your boss A’s birthday is one of the following $10$ dates:Mar 4, Mar 5, Mar 8Jun 4, Jun 7Sep 1, Sep 5Dec 1, Dec 2, Dec 8A told you only the month of his birthday, and told your colleague Conly the day. Afterthat, you first said: “I don’t know A’s birthday; C doesn’t know it either.” After hearing what you said, C replied: “I didn’t know A’s birthday, but now I know it.” You smiled and said: “Now I know it, too.” After looking at the 10 dates and hearing your comments,your administrative assistant wrote down A’s birthday without asking any questions. So what did the assistant write? Solution: The key is like looking for a unique identifier. Since the days $2$ and $7$ are unique, then $C$ would be able to know the whole date if the day is $2$ or $7$. However, I believe $C$ does not have a chance to know it, thus, the month cannot be Jun or Dec. Then, $C$ learns the month is within Mar and Sep and know the whole date. Then, we can know that $C$’s day is within $\\{1, 4, 8\\}$ because if the day is $5$, $C$ still cannot tell. However, if the day is $4$ or $8$, I cannot figure out the date. Then, the month must be a unique identifier and can only be 1st Sep. 5 - Card gameQuestion: A casino offers a card game using a normal deck of $52$ cards. The rule is that you turn over two cards each time. For each pair, if both are black, they go to the dealer’s pile; if both are red, they go to your pile; if one black and one red, they are discarded. The process is repeated until you two go through all $52$ cards. If you have more cards in your pile, you win $100$ $; otherwise (including ties) you get nothing. The casino allows you to negotiate the price you want to pay for the game. How much would you be willing to pay to play this game? Solution: $0$ because you will never win. Whenever you take away two red cards, there are two black cards for the dealer. In case of a tie (one black and one red), the difference of black and red cards number would not change (both in the deck and in your piles). Consider the idea of symmetry. 6 - Burning ropesQuestion: You have two ropes, each of which takes $1$ hour to burn. But either rope has different densities at different points, so there’s no guarantee of consistency in the time it takes different sections within the rope to bum. How do you use these two ropes to measure $45$ minutes? Solution: For a rope that takes $x$ minutes to burn, if you light both ends of the rope simultaneously, it takes $\\frac{x}{2}$ minutes to burn. So we should light both ends of the first rope and light one end of the second rope. $30$ minutes later, the first rope will get completely burned, while that second rope now becomes a $30$-min rope. At that moment, we can light the second rope at the other end (with the first end still burning), and when it is burned out, the total time is exactly $45$ minutes. 7 - Defective ballQuestion: You have $12$ identical balls. One of the balls is heavier OR lighter than the rest (you don’t know which). Using just a balance that can only show you which side of the tray is heavier, how can you determine which ball is the defective one with $3$ measurements? Solution: Split the $12$ balls into groups of $4$, and measure the first $2$ groups. If it balances, the defective ball is in the rest group. If not, that ball is within the prior $2$ groups…The detailed strategy is shown in the following image: 8 - Trailing zerosQuestion: How many trailing zeros are there in $100!$ (factorial of $100$)? Solution: This is an easy problem. We know that each pair of $2$ and $5$ will give a trailingzero. If we perform prime number decomposition on all the numbers in $100!$, it is obvious that the frequency of $2$ will far outnumber of the frequency of $5$. So the frequency of $5$ determines the number of trailing zeros. Among numbers $1, 2, …, 99$, and$100$, $20$ numbers are divisible by $5$ ( $5, 10, …, 100$ ). Among these $20$ numbers, $4$ aredivisible by $5^2$ ( $25, 50, 75, 100$ ). So the total frequency of $5$ is $24$ and there are $24$trailing zeros. 9 - Horse raceQuestion: There are $25$ horses, each of which runs at a constant speed that is different from the other horses’. Since the track only has $5$ lanes, each race can have at most $5$ horses. If you need to find the $3$ fastest horses, what is the minimum number of races needed to identify them? Solution: It is natural for us to index the horses from $1$ to $25$ and split them into $5$ groups of $5$ to take the first race. Then, with the first round $5$ races, you can eliminate the last $2$ of each group. We can assume $1, 6, 11, 16, 21$ are the fastest within each group (and already sorted by their speed, though we do not know the speed now). We can find that for $16, 21$ cannot be in top $3$; $11$ may be within top $3$ but may not (because other members of top $3$ may be beat by $1$ and $6$). Then, we can race the $6$th time among $1, 6, 11, 16, 21$ to find out the local top $3$. Then, the pool of top $3$ can only be: $1, (2, 3), 6, (7), 11$ and $1$ is the absolute champion. Then we only need to give $7$th ride to $2, 3, 6, 7, 11$. 10 - Infinite sequenceQuestion: If x ^ x ^ x … ^ x&#x3D;2 and $x^y&#x3D;x^y$, find $x$.Solution: Since we can have $x &gt; 1$ and $\\lim_{n \\rightarrow \\inf}$ x ^ x ^ x … ^ x (a total of n) &#x3D;2 converges to a constant $2$, then another ^ x would not change the result. Then we can have x ^ (x ^ x ^ x … ^ x) &#x3D; $x^2 &#x3D; 2$, $x&#x3D;\\sqrt 2$.","categories":[{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"Financial Firm","slug":"Job-Search/Financial-Firm","permalink":"https://umiao.github.io/categories/Job-Search/Financial-Firm/"}],"tags":[{"name":"IQ","slug":"IQ","permalink":"https://umiao.github.io/tags/IQ/"},{"name":"Brainteasers","slug":"Brainteasers","permalink":"https://umiao.github.io/tags/Brainteasers/"},{"name":"Math","slug":"Math","permalink":"https://umiao.github.io/tags/Math/"}]},{"title":"NLP-2 Reading Note: A Survey on Text Classification","slug":"NLP-2","date":"2022-06-20T18:38:01.000Z","updated":"2022-07-29T07:09:48.412Z","comments":true,"path":"2022/06/20/NLP-2/","link":"","permalink":"https://umiao.github.io/2022/06/20/NLP-2/","excerpt":"Review the major NLP tasks as well as methods (in terms of text classification).","text":"Review the major NLP tasks as well as methods (in terms of text classification). AbstractThe methods discussed in this survey are proposed between 1961-2021. Traditional model: Preprocess and mannually extract features from the documents. Deep learning model: Learns and implements the feature extraction (as non-linear transformation). Better preserve the sequential structure and contextual info. Traditional models1 - Preprocessing:graph LR; A[word segmentation] --> B[data cleaning]; B --> C[statistics]; 2 - Word Representation:graph LR; A[Representation Methods] --> 1[\"Bag-Of-Words (BOW)\"]; A --> 2[N-gram]; A --> 3[\"Term Frequency-Inverse Document Frequency (TF-IDF)\"]; A --> 4[word2vec]; A --> 5[\"Global Vectors for word representation (GloVe)\"]; Bag-Of-Words (BOW): Representing each text with a dictionary-sized vector. The i-th element in the vector represents the frequency of the i-th word in themapping array of the sentence. N-gram: Considers the information of adjacent words to predict a word’s probability. Adopts the Markov hypothesis: word appears only concerning the words that preceded it. Counting and recording the occurrence frequency of all fragments (in a sliding window with size N), and predict the sentence (next word) based on the above frequency vector. TF-IDF: TF (Term Frequency) is the word frequency of a word in a specific article, and IDF (Inverse Document Frequency) is the reciprocal of the proportion of the articles containing this word to the total number of articles in the corpus. Use the multiplication of the two to represent a word. The importance of a word increases proportionally with the number of times it appears in a document. However, it decreases inversely with its frequency in the corpus as a whole. word2vec: Use local context information to obtain (fixed-length, real value) word vectors. Two essential models: CBOW and Skip-gram. The former is to predict the current word on the premise that the context of the current word is known. The latter is to predict the context when the current word is known. The two manners: GloVe: Global Vectors for Word Representation: Construct the co-occurrence matrix of words based on the corpus. Learn the word vector based on the co-occurrence matrix and GloVe model (fit the co-occurrence frequency by dot-product of vectors to measure the relevance). 3 - ClassifiersFinally, the represented text (vector) is fed into the classifier according to selected features. Probabilistic Graphical Models (PGMs) basedExpress the conditional dependencies among features in graphs, such as the Bayesian network. It is a combination of probability theory and graph theory. Naïve Bayes (NB) : Assumption: when the target value has been given, the conditions between text $T &#x3D; [T_1,T_2, . . . ,T_n]$ are independent. It uses the prior probability to calculate the posterior probability: $P(y|T_1, …, T_n)&#x3D;\\frac{p(y)\\prod_{j&#x3D;1}^np(T_j|y)}{\\prod_{j&#x3D;1}^n p(T_j)}$. The sturcture would be quite simple (though the assumption is not actual) and would be shown below: Naive Bayes Transfer Classification (NBTC): Use EM Algorithm to settle the different distribution between the training set and the target set (by obtaining a locally optimal posterior hypothesis on the target set). By assuming the distribution of variables, Bernoulli NB, Gaussian NB and Multinomial NB can be implemented. K-Nearest Neighbors (KNN) basedFinding the category with the most samples on the k nearest samples. Possible improvements: (metric of) feature similarity, K value, index optimization (accelerate the search of k nearest negihbours). However, this algorithm can be very SLOW on large datasets. The Neighbor-Weighted K-Nearest Neighbor (NWKNN) is proposed to improve classification on Unbalanced corpora (casting big weight for neighbors in a small category, a small weight for neighbors in a broad class). Support Vector Machine (SVM) basedChange the classification task into multiple binary classification tasks in essence (construct a optimal hyperplane to divide two classes with max distance of category boundary). Decision Trees (DT) basedSupervised tree structure learning including: Tree constrction and tree pruning. The key is to divide the dataset into diverse subsets, and every leaf node stands for a category. The Iterative Dichotomiser 3 (ID3) algorithm uses information gain as the attribute selection criterion to select node &#x2F; discriminant attribute. DT based models usually need to be train on each dataset (lack of generalization ability). Integration-based Methods.Aims at aggregating the results of multiple algorithms for better performance. Bootstrap aggregation: Random forest, Adaptive Boosting (AdaBoost), XGBoost. These classifiers are expected to be independent (combine different learners would be ideal). Summary:NB assume the features are independent. It is less sensitive to missing data and simple. The performance would drop with large number of features and high correlation between features. SVM can solve high-dimensional &#x2F; non-linear problems. It has a high generalization but sensitive to missing data. KNN depends on the finite, surrounding samples thus vulnearable in cases of crossover &#x2F; overlap of the class domain. DT is easy to explain, however, the feature engineering can be difficult. However, it works well on small datasets. Deep learning methodsMainstream methods include Multi-Layer Perceptron (MLP), Recursive Nerual Network (RNN), Convolutional Neural Network (CNN). Bert is able generate the contextualized word vectors and is a significant turning point. RNN: Usually used to learn a (latent) semantic vector representative automatically (each input word would be viewed as a leaf node of the entire tree like model structure. Finally, all nodes are combined into a parent node to represent the entire input text for prediction). It can also deal with input with variable length. It should be noted that it is a biased model as the following inputs profit over the former and decreasing the semantic efficiency (LSTM is proposed to partially alleivated this issue). It can also be realized in a bi-directional manner. CNN: It can be implemented both in character level and word level. It can also combined with pyramid structure, residual network, etc. Self-attention &#x2F; Transformer &#x2F; Bert: There are alread a lot of materials on this topic. Pre-trained Model: Including models like Embedding from Language Model (ELMo), OpenAI GPT, BERT, etc.We can clearly tell that ELMo is LSTM based while GPT and BERT are Transformer based. These methods comeup with larget training time, training data and resources. A typical setting is like over 200K training sets and 1.5B training data, batch size over 8K. We also have other models like BART (Seq2Seq based denoising autoencoder, introducing noise to the document and use Seq2Seq model to reconstruct) and SpanBERT (improved implementation of BERT, like mask a continuous paragraph, rather than word leveled. Span Boundary Objective (SBO) is added to predict span by the token next to the span boundary, and the NSP pre-training task is removed). GNN (Graph Neural Networks) based methodJust put a image here:","categories":[{"name":"AI","slug":"AI","permalink":"https://umiao.github.io/categories/AI/"},{"name":"NLP","slug":"AI/NLP","permalink":"https://umiao.github.io/categories/AI/NLP/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://umiao.github.io/tags/NLP/"}]},{"title":"NLP-1 Roadmap","slug":"NLP-1","date":"2022-06-20T17:23:43.000Z","updated":"2022-06-20T18:49:12.139Z","comments":true,"path":"2022/06/20/NLP-1/","link":"","permalink":"https://umiao.github.io/2022/06/20/NLP-1/","excerpt":"Have a glance on the NLP tasks and techniques. Will be discussed in a more detailed manner.","text":"Have a glance on the NLP tasks and techniques. Will be discussed in a more detailed manner. General Roadmap for NLP Studygraph LR; 1[Preliminary]--> 1.1[Linear Algebra]; 1 --> 1.2[Statistics and Probability]; 1--> 1.3[Metrics]; 2[Traditional model]; 2 --> 2.1[Statistical ML]; 2.1 --- 2.1.1(Linear Classification); 2.1 --- 2.1.2(SVM); 2.1 --- 2.1.3(Tree Model); 2.1 --- 2.1.4(\"HMM / CRF\"); 2 --> 2.2[Neural Network]; 2.2 --- 2.2.1(Word embedding); 2.2 --- 2.2.2(CNN); 2.2 --- 2.2.3(RNN/LSTM/GRU/Bidirectional); 2.2 --- 2.2.4(ELMo); 2.2 --- 2.2.5(BERT); 3[Paradigm and trick]; 3 --> 3.1[Document Classification]; 3.1 --- 3.1.1(ReNN); 3.1 --- 3.1.2(MLP); 3.1 --- 3.1.3(RNN); 3.1 --- 3.1.4(CNN); 3.1 --- 3.1.5(Attention); 3.1 --- 3.1.6(Transformer); 3.1 --- 3.1.7(GNN); 3 --> 3.2[Sentence Matching]; 3.2 --- 3.2.1(Representation based); 3.2 --- 3.2.2(Interaction based); 3 --> 3.3[Annotation]; 3.3 --- 3.3.1(Embedding Module); 3.3 --- 3.3.2(Context Encoder Module); 3.3 --- 3.3.3(Inference Module); 3 --> 3.4[Generation]; 3 --> 3.5[Language Model]; 3.5 --- 3.5.1(Word Level); 3.5 --- 3.5.2(Sentence Level); 4[Application Scenarios]; 4 --> 4.1[Single Task]; 4.1 --- 4.1.1(Lexical Analysis); 4.1 --- 4.1.2(Syntax Analysis); 4.1 --- 4.1.3(Semantic Analysis); 4.1 --- 4.1.4(Document Generation); 4 --> 4.2[Complex Task]; 4.2 --- 4.2.1(Search / Recommendation); 4.2 --- 4.2.2(Conversation); 4.2 --- 4.2.3(Knowledge Graph); By the wayOther roadmaps of the ML &#x2F; Statistical topics would be placed here (from reddit):","categories":[{"name":"AI","slug":"AI","permalink":"https://umiao.github.io/categories/AI/"},{"name":"NLP","slug":"AI/NLP","permalink":"https://umiao.github.io/categories/AI/NLP/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://umiao.github.io/tags/NLP/"}]},{"title":"DS-Study-Note-9 Gradient Boosting Machine Tree Model(s)","slug":"DS-Study-Note-9","date":"2022-05-26T02:54:23.000Z","updated":"2022-05-28T22:43:50.842Z","comments":true,"path":"2022/05/25/DS-Study-Note-9/","link":"","permalink":"https://umiao.github.io/2022/05/25/DS-Study-Note-9/","excerpt":"Gradient Boosting Machine TreeGBMTree stands for Gradient Boosting Machine Tree.","text":"Gradient Boosting Machine TreeGBMTree stands for Gradient Boosting Machine Tree. The idea is to train multiple serial weak learner, while the objective of each learner is to fit the negative gradient of the loss function of the previous cumulative model.Thus, after this weak learner is attached, the loss of the new cumulative model shall be maximally reduced. Also, each base (weak) learner can be linearly combined with different weights (so that those learners with higher performance would contribute more to the result). A common implementation of the base learner is Tree Model (e.g., Decision Tree). Primary FeatureThe primary feature of GBM (Gradient Bossting Machine) is that, it conduct Gradient Descent in the space of function, instead of the space of model parameters (e.g., Neural Network calculates the gradient of current loss to the model parameters for update). In Gradient Boosting, in each iteration, a weak learner is generated via fitting the negative gradient of loss function to the cumulative model formed by the learners generated before (this means that the previous model is left unchanged). Then, the current weak learner is added to the cumulative model to reduce the loss. Differences: Gradient descent of the parameter space would: use the gradient to update the parameters Gradient descent of the function space would: fit a new function with the gradient Math TheoryConsidering we have $n$ training samples $\\{x_i, y_i\\}$, and the cumulative model in the $k$th round is $F_{k-1}(x)$. Then, the model in the kth round should be:$$ F_k(x) &#x3D; F_{k-1}(x) + arg \\min_{h \\subset H} Loss(y_i, F_{k-1}(x_i) + h(x_i)) $$where $h(x)$ is the desired weak learner. In fact, after the $k-1$th round, we can have $\\hat y &#x3D; F_{k-1}(x)$ and the loss $Loss(y, \\hat y)$. Thus, in order to minimize the model’s loss after introducing the $k$th weak learner, the gradient of this learner $h(x)$ should be negative to the gradient of $F_{k-1}(x)$, so that:$$ Gradient \\ h(x) &#x3D; - \\frac{\\partial Loss(y, \\hat y)}{\\partial F_{k-1}(x)} $$ In the actual implementation of this algorithm, a concrete loss function and a learning rate $\\alpha$ should be appointed. The learning rate would be used when updating the model: $F_k(x) &#x3D; F_{k-1}(x) + \\alpha h(x)$.We also need to set a boundary condition (number of iteration, minimal improvement &#x2F; difference, etc) to decide when to terminate this algorithm. GBDT (Gradient Boosting Decision Tree) AlgorithmGBDT uses CART (Classifier And Regression Tree) as the weak learner.The benefit of such design is that, the Decision Tree itself is unstable, as minor fluctuation of training data can greatly influence the (inference) result (single Decision Tree has high variance). In Ensemble Learning, we expect weak classifiers to have high variance to achieve better generalization performance. Thus, CART is preferred to the more stable weak learners (e.g., linear regression). Implementation In regression task, the loss function would be ${(y - F_{k-1}(x))}^2$, then the negative gradient would be $2(y - F_{k-1}(x))$.Thus, we can find the negative gradient simply via $y - F_{k-1}(x)$. In classification task, we aims at fitting the logarithmic probability $\\log \\frac{p}{1-p}$ with linear model $Wx+b$. The loss function would be Cross Entropy Loss: $Loss &#x3D; -y \\log p - (1-y)log(1-p)$. Then, we can have$$ F_{k-1}(x) &#x3D; \\log\\frac{p_{k-1}}{1-p_{k-1}} \\Rightarrow p_{k-1} &#x3D; \\frac{1}{1 + e^{-F_{k-1}}} $$$$ Loss &#x3D; -y\\log \\frac{1}{1 + e^{-F_{k-1}}} - (1-y)\\log \\frac{e^{-F_{k-1}}}{1 + e^{-F_{k-1}}} $$ Simplify the expression, we have $Loss &#x3D; (1-y)F_{k-1} + \\log(1 + e^{-F_{k-1}})$. $-\\frac{\\partial Loss}{\\partial F_{k-1}} &#x3D; y - p_{k-1}$ XGBoostXGBoost stands for eXtreme Gradient Boosting, which is an algorithm based on GBDT. It makes multiple improments, including: Apply second-order Taylor Formula Expansion to better approximate various loss functions (and faster convergence). Introduce regularization term to prevent overfitting. Use Block to store the structure for parallel processing Math TheoryThe objective function of XGBoost consists of loss function and regularization term.The objective function can be written as:$$ Loss &#x3D; \\sum_{i&#x3D;1}^n l(y_i, \\hat y_i) + \\sum_{k&#x3D;1}^K \\Omega(f_k) $$We have $n$ pairs of training samples and a total of $K$ trees. $\\Omega(f_k)$ is the regularization term which measures the model’s complexity. Since XGB is implemented by Boosting, we also have: $\\hat y_i^t &#x3D; \\hat y_i^{t-1} + f_t(x_i)$. Here $f_t(x_i)$ is the most recently appended weak learner. Taylor ExpansionWe already know that $f(x) \\approx f(x_0) + f’(x_0)(x - x_0) + \\frac{f’’(x_0)}{2}(x - x_0)^2$.Then, let $l(x) &#x3D; l(y_i, x)$, find the 2nd order Taylor Expansion at $x_0$, we can have $l(y_i, x) \\approx l(y_i, x_0) + l’(y_i, x_0)(x - x_0) + \\frac{l’’(y_i, x_0)}{2}(x - x_0)^2$.Similarly, we have $l(y_i, x) \\approx l(y_i, \\hat y_i^{t-1}) + l’(y_i, \\hat y_i^{t-1})(x - \\hat y_i^{t-1}) + \\frac{l’’(y_i, \\hat y_i^{t-1})}{2}(x - \\hat y_i^{t-1})^2$. Note that we have $x &#x3D; \\hat y_i^{t-1} + f_t(x_i)$, denote $g_i &#x3D; l’(y_i, \\hat y_i^{t-1})$, $h_i &#x3D; l’’(y_i, \\hat y_i^{t-1})$, we have:$$ l(y_i, \\hat y_i^{t-1} + f_t(x_i)) \\approx l(y_i, \\hat y_i^{t-1}) + g_if_t(x_i) + \\frac{h_i}{2}f_t^2(x_i)$$Here we finish the derivation. Unfold regularization termNote that $l(y_i, \\hat y_i^{t-1})$ is a constant, we can simply remove it. Also, we unfold the regularization term $\\sum_{k&#x3D;1}^K \\Omega(f_k) &#x3D; \\Omega(f_t) + \\sum_{k&#x3D;1}^{t-1}\\Omega(f_k)$. The structure of the previous $t-1$ trees would not change, thus we can view their sum as a constant and remove, to have the polished loss function:$$ {Loss}^t &#x3D; \\sum_i^n[g_if_t(x_i) + \\frac{h_i}{2}f_t^2(x_i)] + \\Omega(f_t) $$ Organize the objective function In order to define a Tree, we need the leave nodes’ weight vector $\\omega \\subset R^T$ and the mapping relationship $q: R^d \\rightarrow 1,2,…,T$, T is the number of the leave nodes. Thus, we can express a tree as $f_t(x) &#x3D; \\omega_{q(x)}$. We then define the complexity, i.e., $\\Omega$. We define it by: the number of the leave nodes $T$, and the L2 Norm of the weight vectors of the leave nodes. Thus, we define $\\Omega(f_t) &#x3D; \\gamma T + \\frac{1}{2}\\lambda\\sum_{j&#x3D;1}^T\\omega_j^2$. Merge the terms according to their order: $$ {Loss}^{(t)} &#x3D; \\sum_{j&#x3D;1}^T[(\\sum_{i \\in I_j}g_i)w_j + \\frac{1}{2}(\\sum_{i \\in I_j}h_i + \\lambda)w_j^2] + \\gamma T$$ We can denote $G_j &#x3D; (\\sum_{i \\in I_j}g_i)$ and $H_j &#x3D; \\sum_{i \\in I_j}h_i$, these two stands for sum of the 1 &#x2F; 2 - order partial derivative of the samples contained by leave node $j$. Note that $G_j$, $H_j$ are constants. Optimal SolutionWe can tell that the objective function $f(w_j) &#x3D; G_jw_j + \\frac{1}{2}(H_j + \\lambda) w_j^2$ is a 2nd-order function about $w_j$. Thus, we can tell that the minial ($f(w_j) &#x3D; -\\frac{G_j^2}{2(H_j + \\lambda)}$) is reached at $w_j &#x3D; - \\frac{G_j}{H_j + \\lambda}$. LightGBMMotivation Reduce the occupancy of memory. Utilize as much as possible data on single machine, without sacrificing the speed. Reduce the overhead of communication, realize linear acceleration in case of Multiprocessor parallel. Difference from XGBoost These two alogrithms both use the negative gradient of the loss function as the approximation of the residual of current decision tree, to fit the new decision tree. LightGBM would grow in the vertical direction, and other algorithms would grow in the horizontal direction. LightGBM would grow in the leave node with maximal error, to reduce loss as possible. Histogram AlgorithmHistogram Algorithm is proposed to substitute the pre-sorted algorithm of XGBoost. The pre-sorted algorithm would first sort the samples according to their feature values, then find the optimal split point from all the possible feature values. Thus, for each feature, the number of candidate split points is proportional to the number of samples. At the same time, Histogram Algorithm would discretize the continuous feature values into a constant number (e.g., 255) of bins. Thus, the candidate split points would reduce to (num_bins - 1) from (num_unique_values - 1). With this manner, instead of storing the feature values with float_32, we can now use uint_8 to store the index of bucket (with hash algorithm). At the same time, there comes the tradeoff of losing accuracy to raise efficiency (However, single Decision Tree itself is a weak model, it is not that important for the split points to be accurate. Coarse split points may have the regularization effect and the result can stay robust under Gradient Boosting framework.).Acceleration of Histogram via finding differenceThe histogram of a leaf can be found by solving the difference of its father node’s histogram and its brother’s histogram. LightGBM can solve the histogram of a leaf node (with small sample number) and quickly find the histogram of its brother. It should be noted that XGB and LightGBM only consider non-zero values. Leaf-wise Algorithm with Depth LimitMost GBDT algorithms use the level-wise grow strategy, i.e., all the leave nodes in the same level would be splitted, under a single traverse (then post-pruning would be executed). It is friendly for multi-thread optimization, controlling of model complexity and resist over-fitting. However, for many leaves, the split gain is very low and result in useless computational overhead. LightGBM uses the leaf-wise grow strategy. At a time, a leaf with max split gain is selected and splitted. Thus, with the same number of splition, we can come up with better accuracy and lower error. However, it also has the advantage of possibility of overfitting, as a deep decision tree may be generated. Thus, a max_depth is introduced to control the depth of a tree. GOSS - Gradient Based One-Side SamplingGOSS aims at omitting most of the samples with small gradient and only use the remaining samples to compute the information gain. (We assume that the samples with smaller gradients are already well-trained.) However, simply dropping the data with small gradient would change the overall distribution of dataset. Thus, GOSS would sort all the possible values of the feature to be splitted order by the absolute value and select the $k$-largest ones. Then, $m-k$ samples and randomly selected from the remaining ones (and assigned with a constant $fact$ to scale the samples with small gradient). EFB - Exclusive Feature BundlingHigh-dimensional data is usually sparse. Such sparsity inspires us to develop a lossless method to reduce the dimension of features. If two features are exclusive, it means that their feature would not be non-zero values at the same time. Then, we can simply find the sum of these two (Bundling), without losing information. If they are not completely exclusive, we can measure the conflict ratio (ratio of having non-zero values at the same time) of a pair of features. If conflict ratio is small, we can still bundle them without impair the final accuracy. Exclusive Feature Bundling, EFB points out that if we conduct fusion and bundling on some features, we can reduce the number of features for better performance. Decide which features to bundleBundling pair-wise independent features is a NP-Hard problem. The EFB algorithm of LightGBM reduces this problem into Graph Coloring Algorithm. Each feature would be viewed as a vertex of the graph, and we draw an edge between two features which are NOT completely independent and the weight of the edge is the conflict ratio of these two features. Then our task is to color the vertices to be bundled in the same color. The heuristic alogrithm is like: Construct such a graph, sort the vertices by the degree (larger degree means higher degree of conflict). Traverse each vertex (feature), allocate it to one of the existing feature bundles or create a new one, to minimize the total conflict. When the scale of data is very large, the efficency of graph operation would get too low. LightGBM proposes a non-graph algorithm, which sort the features by the number of non-zero values. Implement the bundlingWe can distribute values of different features into different bins of a bundle (in order to make sure the values of raw features are recognizable before bundling, by adding a shifting constant to the feature value). SummaryIt is not recommended to use LightGBM on small dataset as it is sensitive to overfitting. (Recommend to use LightGBM on dataset with a size larger than 10K) Categorical FeatureIt should be noted that, one-hot encoding is not recommended for Decision Tree (especially when the number of classes is large). The split would be imbalanced, which results in very small splitting gain. Using one-hot encoding means that on each decision node, we can only use one vs rest splitting (e.g., decide a sample is dog or not). It is almost equivalent to not splitting at all. One-hot encoding would cut samples of the same class into many small subspaces &#x2F; groups (statistics on small space may be inaccurate). LightGBM use many-to-many to solve this issue (e.g., a single node is $X &#x3D; A || X &#x3D; C$). Acceleration method: Before traverse all the candidate splitting points, first sort the histogram by the mean value of the corresponding labels, then search the optimal splitting point according to the order. This method can easily overfitting, so it needs further constraints and regularization. Source CodeWhen running in parallel, LightGBM would store all the training data in each node (server) to save the communication cost (instead of dividing the data vertically and distributing to each node). LightGBM would also use Reduce Scatter to distribute the task of merging the histogram. Finally, it applies parallizing based on voting (only the Top K features of each node would be considered and merged). Also, the histogram algorithm LightGBM applied is naturally more friendly to Cache, because it reduce the random access and do not need a data structure to store the mapping of row indexes to leave indexes. Recommended web for source code studying: https://mp.weixin.qq.com/s/XxFHmxV4_iDq8ksFuZM02w Summary on Bagging and Boosting Sample Selection: Bagging: sampling with replacement, different training sets are independent from each other Boosting: The training set remains unchanged, only the weight of each sample alters (decided by the performance of last round). Weight of sample: Bagging: Each sample has the same weight. Boosting: Weight decided by the error rate (higher error rate corresponds to higher weight). Weight of learner: Bagging: Each learner has the same weight. Boosting: Each weak learner has different weight (more accurate one has higher weight). Parallel computing: Bagging: Able to parallel. Boosting: Each learner should be generated in sequential order. Essence: Bagging: Reduce variance (via voting). Boosting: Reduce bias.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"General Knowledge","slug":"Data-Science/General-Knowledge","permalink":"https://umiao.github.io/categories/Data-Science/General-Knowledge/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://umiao.github.io/tags/Machine-Learning/"},{"name":"GBM","slug":"GBM","permalink":"https://umiao.github.io/tags/GBM/"},{"name":"XGBoost","slug":"XGBoost","permalink":"https://umiao.github.io/tags/XGBoost/"}]},{"title":"SQL-Study-Note-16 Optimization Discussion 3 - Optimize the condition setting and table creating","slug":"SQL-Study-Note-16","date":"2022-05-14T21:02:18.000Z","updated":"2022-05-15T07:25:39.805Z","comments":true,"path":"2022/05/14/SQL-Study-Note-16/","link":"","permalink":"https://umiao.github.io/2022/05/14/SQL-Study-Note-16/","excerpt":"The setting of query conditions, together with table creating can be further optimized.","text":"The setting of query conditions, together with table creating can be further optimized. Optimization of conditionsFor complex query, you can use temporary table to store the intermediate result. (Maybe CTE) Optimize the GROUP BY clauseBy default, MySQL would sort all the values of the groups generated by GROUP BY.The result of GROUP BY col1, col2, ... is equivalent to ORDER BY col1，col2，... explicitly. If you do not want to sort the result of GROUP BY, you can specify ORDER BY NULL to ban the sorting: 12SELECT col1, col2, COUNT(*) FROM table GROUP BY col1, col2 ORDER BY NULL ; Optimize the JOINIn MySQL, you can use subquery to generate a single column result and use it as another query’s filtering condition.By applying subquery, a SQL operation with multiple logical steps can be done at a time, while avoiding the deadlock of transaction and table and being easy to write. However, sometimes, the subquery can be more efficiently replaced with JOIN (this is because MySQL does not have to create temporary virtual table and able to utilize the index). Optimize the UNIONMySQL execute the UNION command via creating and filling temporary table. UNION ALL is suggested, unless you do need to eliminate the duplicates of record.MySQL would defaultly add DISTINCT keyword to the temporary table (under the UNION command), which would be costly. Avoid large transaction You can split complex SQL into multiple small SQLs Simple SQL can better utilize the QUERY CACHE of MySQL Reduce the lasting time of locks. Especially for the tables stored under the MyISAM engine. Better utilize multi-core CPU. Use TRUNCATE to replace DELETEThe DELETE operation would be recorded in the undo block and the deleting record would be recorded in binlog. If you are deleting the entire table, large binlog would be created and mass undo data blocks would be occupied. Using TRUNCATE would not record the information used for recovery. Thus, it corresponds to very small cost of resources and high time efficiency. Also, it can make auto-increment attribute back to $0$. Improve the paging strategyReasonable paging strategy can raise the efficiency of paging:Example: 12select * from t where thread_id = 10000 and deleted = 0 order by gmt_create asc limit 0, 15; The above method would extract all the records which satisfy the condition and return.In this case, we have: Cost_of_visiting = IO_of_index + IO_of_all_the_records_dataThus, in case of table with large data, the efficiency would be very low with high latency. This SQL is suitable for application scenarios with small intermediate result set (less than 10000 lines) or with complex query condition (related with multiple attributes of multiple tables) Solution: 123select t.* from (select id from t where thread_id = 10000 and deleted = 0 order by gmt_create asc limit 0, 15) a, t where a.id = t.id; In this case, we require the id to be the primary key and we have a overlap index of (thread_id, deleted, gmt_create). We first extract the primary key id for sorting, and then extract the other attributes with JOIN.we have: Cost_of_visiting = IO_of_index + IO_of_the_records_data_after_paging (15 lines)This works well in the scenarios where we have the needed overlap index and a large intermediate result set. Optimization of table creatingSet up indexFirst consider the attributes which would be used in WHERE &#x2F; ORDER BY, … to create index on. Use numerical attribute as possible E.g., you can map male &#x2F; female to 0 &#x2F; 1.If an attribute only contains numerical information, do not store it as a string. Otherwise, the performance of querying and connecting would drop and the storage overhead would increase. This is because for string, the engine would conduct char-wise comparison, while numerical data only needs one comparison. Divide’n Conquer table with large dataQuery over large table can be very slow due to too many lines to scan. Thus, we can use window function to split and iterate the entire table, and union the query result for display: 123SELECT * FROM (SELECT ROW_NUMBER() OVER(ORDER BY ID ASC) AS rowid,* FROM infoTab)t WHERE t.rowid &gt; 100000 AND t.rowid &lt;= 100050-- Specify the range of line number Use VARCHAR &#x2F; NVARCHAR to replace CHAR &#x2F; NCHARUse VARCHAR &#x2F; NVARCHAR to replace CHAR &#x2F; NCHAR as possbile, this is because: The varaible type (which takes up smaller space) can save the cost of memory &#x2F; type. It is more efficent to search in a smaller search space (less thing to compare). NULL also takes up space, if the length is already given, e.g., CHAR(100). However, for VARCHAR, the NULL would not occupy space. Design of related tables One to Many (1 to N): Use Foreign Key Many to Many (N to N): Create a new table to decomposite Many to Many to two One to One One to One (1 to 1): Usually use the same Primary Key, or add a Foregin Key","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"SQL-Study-Note-15 Optimization Discussion 2 - Optimize the SELECT and Data Manipulation","slug":"SQL-Study-Note-15","date":"2022-05-14T18:14:50.000Z","updated":"2022-05-15T17:12:02.143Z","comments":true,"path":"2022/05/14/SQL-Study-Note-15/","link":"","permalink":"https://umiao.github.io/2022/05/14/SQL-Study-Note-15/","excerpt":"Other issues which deserve attention when writing SELECT clause.","text":"Other issues which deserve attention when writing SELECT clause. Optimize the SELECTAvoid SELECT *It is not a good habit of writing SQL, since it would disable the optimizer from conducting optimization like scanning of overlay index, influencing the selection of execution plan, increasing the overhead of bandwith, IO, memory and CPU. It is suggested to specify the required columan names only. Avoid the use of functions with undefined resultIn business scenarios such as master-slave replication, the slave would only duplicate the statements executed by the master.Thus, applying functions such as now(), Rand(), sysdate(), current_user() would generate different result on the master and slave. In addition, for functions with uncertain outputs, the generated SQL statements cannot utilize the query cache. Place small table ahead of big tableWhen conducting relationship query, MySQL would scan the tables after the FROM keyword from left to right (For Oracle, it is right to left).Thus, there would be a full table scan for the first table, so it would be faster and more efficient if the first table has fewer records. Use alias of tableWhen connecting multiple tables in SQL, you should assign alia for each table and add the table alia prefix to the column names. Then the time of parsing together with the syntax error raised by ambiguity can be reduced. Use WHERE instead of HAVINGYou should avoid using HAVING, as it only filters the result after all the records are selected. However, WHERE would filter records before aggregating to reduce the number of kept records and overhead.HAVING should be used for the filtering of aggregate results only. Modify the connection order of WHERE clauseMySQL would parse the WHERE clause from left to right, up to down. Thus, we should place the condition which can filter out the most data to the very beginning. Do not use ORDER BY RAND()In this case, a random number would be generate for each row and then conduct sorting using the random number as key.E.g., select * from student order by rand() limit 5.Thus, it results in super low efficiency. It is recommended to generate primary key at random, and filter by primary key. Optimization of Data Manipulation Language statementsInsert data in batchIn case of large scale data insertion, INSERT clause with multiple values is recommended as it is faster: 1234567insert into T values(1,2); insert into T values(1,3); insert into T values(1,4);-- Insert one value at a time, slowerInsert into T values(1,2),(1,3),(1,4);-- Insert multiple values Reasons of applying the latter one: Reduce the parsing operation of SQL Reduce the number of connecting to the DB Shorter SQL clause can reduce the Network IO cost.Proper use of COMMITCOMMIT is able to release some resources (after the transaction is finished) to save the cost: The undo data block of transaction The data block recorded in redo log Release the lock of transaction to alleviate the contention of lock. Avoid query the updated data repeativelyMySQL does not support PostgreSQL’s syntax of UPDATE RETURNING.This can be realized with variable. An example of query the updated data: 12Update t1 set time=now() where col1=1; Select time from t1 where id =1; Optimized using variable: 12Update t1 set time=now () where col1=1 and @now: = now (); Select @now; Both the two methods require 2 Network IO. However, the latter avoids visiting the table again, which is much more efficient. Setting the priority of query &#x2F; updateMySQL allows to change the priority of different statements for better collaboration of multiple clients (reduce the waiting casued by lock).We should first find out the type of the application, i.e., query based or update based so that we can sacrifice one’s efficiency to speed up the other’s. The default scheduling strategy is : Write has higher priority over read. At a moment, the write operation for a certain table can happen only ONCE. The write requests are handled in the arriving order. Multiple read operations can happen at the same time. MySQL allows you to edit its scheduling via: LOW_PRIORITY: used for DELETE &#x2F; INSERT &#x2F; LOAD DATA &#x2F; REPLACE &#x2F; UPDATE HIGH_PRIORITY: used for SELECT &#x2F; INSERT DELAYED: used for INSERT and REPLACE It should be noted that if write becomes a LOW_PRIORITY request, then it may be blocked forever if read requests continue to come. The modification can also be only temporarily (appended to the end of SQL).","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"SQL-Study-Note-14 Optimization Discussion 1 - Always use index","slug":"SQL-Study-Note-14","date":"2022-05-14T06:31:36.000Z","updated":"2022-05-14T18:14:23.730Z","comments":true,"path":"2022/05/13/SQL-Study-Note-14/","link":"","permalink":"https://umiao.github.io/2022/05/13/SQL-Study-Note-14/","excerpt":"The optimization of SQL script can be extremely important, just as the importance of algorithm theory &amp; analysis to programming.","text":"The optimization of SQL script can be extremely important, just as the importance of algorithm theory &amp; analysis to programming. IntroductionThe optimization of SQL is the most significant way of leveraging the system’s performance (with the lowest cost and the best effect). The cost of optimization: hardware &gt; system configuration &gt; table structure of database &gt; SQL &amp; index Effect of optimization: hardware &lt; system configuration &lt; table structure of database Principles of MySQL Optimization Reduce the visiting of data: Setting reasonable type for each attribute; Apply compression and index to reduce the drive’s IO Return less data: Only return the needed attributes (avoid using the wildcard *); use paging of data to reduce the IO of drive and network Reduce the number of interaction: Batch DML operation; Reduce the number of connection to database via function &#x2F; stored procedure Reduce the CPU overhead: Reduce the sorting and scan of full table; to reduce the occupancy of CPU Fully use the resources: Introduce parallel processing and table partitioning Order of syntax and executionOrder of syntax123456789101. SELECT 2. DISTINCT &lt;select_list&gt;3. FROM &lt;left_table&gt;4. &lt;join_type&gt; JOIN &lt;right_table&gt;5. ON &lt;join_condition&gt;6. WHERE &lt;where_condition&gt;7. GROUP BY &lt;group_by_list&gt;8. HAVING &lt;having_condition&gt;9. ORDER BY &lt;order_by_condition&gt;10.LIMIT &lt;limit_number&gt; Order of execution FROM: Select table. Would use Cartesian Product to merge multiple tables into one. ON: Filtering based on the virtual table generated by Cartesian Product JOIN: Specify the type of JOIN. E.g., LEFT JOIN would add the remaining data of the left table into the virtual table. WHERE: Filtering based on the above virtual table. GROUP BY: Grouping the data. HAVING: Filtering after the exection of GROUP BY. Logical judgement on the aggregate result can only be placed here. SELECT: Select the desired attributes from the filtered result. DISTINCT: Keep distinct result only. Modify the name of attribute (column). ORDER BY: Sorting the result. LIMIT: Limit the row number of the returned result. Avoid the scenario which invalidaes the indexDo not use fuzzy matching at the beginning of textThis would result in a full table scanning, rather than a query with index. 1234SELECT * FROM t WHERE name LIKE &#x27;%J&#x27;-- This would result in full table scanningSELECT * FROM t WHERE name LIKE &#x27;J%&#x27;-- In this case, the index would be used If you MUST use the fuzzy matching at the very beginning, you should: Use function INSTR(str, substr) for matching, which is similar to indexOf() (return the position of queried string) Use FullText index, and query with MATCH AGAINST. Apply ElasticSearch &#x2F; solr, for big data scenario Simply use LIKE &#39;%J&#39; in small data scenario Avoid using IN &#x2F; NOT INThis would also result in the DB engine conduct a full table scan. 12345678910SELECT * FROM t WHERE id IN (2,3)-- Original querySELECT * FROM t WHERE id BETWEEN 2 AND 3-- If the value is continuousselect * from A where A.id in (select id from B);-- Index deactivatedselect * from A where exists (select * from B where B.id = A.id);-- Index activated Avoid using ORThis would also result in the DB engine conduct a full table scan. 12345678SELECT * FROM t WHERE id = 1 OR id = 3-- Original querySELECT * FROM t WHERE id = 1 UNIONSELECT * FROM t WHERE id = 3-- Optimized with UNION Avoid comparing with NULLThis would also result in the DB engine conduct a full table scan. 123456SELECT * FROM t WHERE score IS NULL-- Original querySELECT * FROM t WHERE score = 0;-- Set a default value 0 for this attribute, and filter by value 0 Avoid calculating expression &#x2F; function in the left part of the condition of WHEREThis would also result in the DB engine conduct a full table scan. 123456SELECT * FROM T WHERE score/10 = 9-- Full table scanSELECT * FROM T WHERE score = 10*9-- Use index Avoid using WHERE 1&#x3D;1 with big dataFor the convenience of adding condition to the query, we would use WHERE 1&#x3D;1 defaultly. In this case, the DB engine would scan the whole table. 12SELECT username, age, sex FROM T WHERE 1=1-- Original query We can further optimize this query. If we do not need the WHERE condition, we can remove WHERE 1=1 and if we need, we can add AND to the condition part. Avoid &lt;&gt; &#x2F; !&#x3D; in the filter conditionWhen an index column is used in the condition expression, we should avoid using &lt;&gt; or !=. If this query (and Not Equal operator) is really required in business, we should evaluate the setup of index and avoiding building index on this attribute (and build index on other attributes used in the query instead). Leftmost Prefix Matching PrincipleFor example, if a composite (union) index includes three columns, key_part1, key_part2, key_part3 but the SQL query does not include the leftmost column key_part1, then the index would not be used. This is because the Leftmost Prefix Matching Principle of SQL. 12select col1 from table where key_part2=1 and key_part3=2-- key_part1 is missing Implicit type conversionIn the following example, the type of this column is varchar while the given value 123 is a number. This would introduce Implicit type conversion and prevent the DB engine from using index. 1select col1 from table where col_varchar=123; The condition of ORDER BY should be the same as WHERE123456SELECT * FROM t order by age;-- age&#x27;s index is NOT usedSELECT * FROM t where age &gt; 0 order by age;-- age&#x27;s index is used For the above SQL, the processing order of DB is: Fetch the data according to the execution plan described by WHERE condition. Sort the data. When processing ORDER BY clause, the DB would check if the attribute used for sorting uses index in the execution plan. If yes, the data is sorted by the index. Otherwise, external sorting is applied. Return the sorted data. Thus, we require the attribute specified by ORDER BY appears in the WHERE condition, to avoid external sorting. More specifically, this attribute should use the index during execution. Similar conclusion holds for other keywords requiring sorting: GROUP BY, UNION, DISTINCT, etc. Proper use of HINTIn MySQL, you can use HINT to require the optimizer to select &#x2F; omit certain index during execution. Generally, we recommend using ANALYZE table to collect statistical information. However, sometimes specify the HINT can result in better execution plan. Appoint the index you would like to use with USE INDEX (to the end of the query). Omit certain index with IGNORE INDEX. Force MySQL to use certain index with FORCE INDEX. Generally, the DB system would auto select a proper index but it is not guaranteed to be optimal. If we know how to select the index, we can use FORCE INDEX to specify the index to use.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"SQL-Study-Note-13 Window Function","slug":"SQL-Study-Note-13","date":"2022-05-05T21:26:39.000Z","updated":"2022-05-10T19:35:30.267Z","comments":true,"path":"2022/05/05/SQL-Study-Note-13/","link":"","permalink":"https://umiao.github.io/2022/05/05/SQL-Study-Note-13/","excerpt":"Window function is also known as Online Analytical Processing function (LAP), which is able to conduct realtime processing and analyzing on the database data.","text":"Window function is also known as Online Analytical Processing function (LAP), which is able to conduct realtime processing and analyzing on the database data. MotivationIn order to solve the following problems: Ranking: rank each department with its own performance TOP N: find out the top $N$ department interms of performance Syntax12345678SELECT *, &lt;window function&gt; OVER(PARTITION BY &lt;col_name used for grouping&gt;ORDER BY &lt;col_name used for sorting&gt;)AS rankingFROM table Types of window functionsThe above can be replaced by two types of functions: Specialized Window Functions: RANK, DENSE_RANK, ROW_NUMBER Aggregate Window Function: SUM, AVG, COUNT, MAX, MIN Specialized Window FunctionsConsidering that we have a column A with value of $\\{3,3,3,7\\}$ and we would get the ranking in the form of RANK() OVER A. RANK: Multiple rows with the same value would occupy their following ranking place. Result: $\\{1,1,1,4\\}$2. DENSE_RANK: Multiple rows with the same value woud NOT occupy their following ranking place.Result: $\\{1,1,1,2\\}$3. ROW_NUMBER: Multiple rows with the same value woud NOT share the same ranking. The tie would be broken via a lexicographical order or something.Result: $\\{1,2,3,4\\}$ Example of these three functions shown below: Summary It should be noted that, the window function operates on the intermediate result after WHERE &#x2F; GROUP BY. Thus, it should be written in the SELECT clause. The PARTITION can be ignored (means sorting without grouping). Difference between PARTITION BY and GROUP BY: GROUP BY has the summary functionality, which would summarize multiple records in a group into one (reducing the number of rows), while PARTITION would keep all these records (leave the number of rows unchanged). Specialized Window Functions realize the functionality of sorting and grouping at the same time, and would NOT reduce the number of rows. Examples: Get each student’s ranking by class: 123SELECT *, DENSE_RANK() over (order by 成绩 desc) as dese_rank from 班级表; Solve TOP N question: 123456789SELECT * FROM(SELECT *, ROW_NUMBER() OVER (PARTITION BY col_name_to_be_partORDER BY col_name_to_be_sort DESC) AS rankingFROM table ) AS AWHERE ranking &lt;= N; Find the grades which are above average: 1234567SELECT * FROM(SELECT *, AVG(grade) OVER (PARTITION BY course_id) AS avg_gradeFROM table ) AS AWHERE grade &gt; avg_grade; This is solved with a 2-step manner. For each row, the corresponding average value is found. Then, filter those courses whose grades are above average. Find the cumulative SUM &#x2F; AVG &#x2F; MAX … of each course: 1234567SELECT *,SUM(grade) over w as curr_sum,AVG(grade) over w as curr_avg,MAX(grade)over w as curr_max,MIN(grade) over w as curr_minFROM scoreWINDOW w AS (partition by course_id order by student_id) If the aggregate functions of AVG() BIT_AND() BIT_OR() BIT_XOR() COUNT() JSON_ARRAYAGG() JSON_OBJECTAGG() MAX() MIN() STDDEV_POP(), STDDEV(), STD() STDDEV_SAMP() SUM() VAR_POP(), VARIANCE() VAR_SAMP() is followed by OVER() clause, it would become aggregate window function.However, if you want to get cumulative result rather than a constant result of the entire group, you must specify PARTITION and ORDER BY at the same time. It should be noted that the WINDOW keyword can apply an alia to a window, so that it can be referred for multiple times.The expected result is shown below: Sliding WindowThe range of Sliding Window can be specified either by ROWS or by RANGE. By ROWS123456789SELECT *,AVG(grade) OVER (ORDER BY id ROWS 2 PRECEDING ) -- Ways to decide the range of window by specifying rows(ORDER BY id ROWS 2 FOLLOWING ) -- Two end interval:(ORDER BY id ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) AS current_avgFROM class The PRECEDING &#x2F; FOLLOWING clause is also known as Frame clause. By RANGESometimes, the range cannot be represented by neighbor rows. E.g., when you want to select the orders within a time frame of a given date (rathe than a row frame). 123456SELECT *,AVG(price) OVER (ORDER BY id INTERVAL BETWEEN 7 DAY PRECEDING AND 7 DAY FOLLOWING) AS current_avgFROM orders Window Functions with constant frame cume_dist() &#x2F; dense_rank() &#x2F; lag() &#x2F; lead() &#x2F; ntile() &#x2F; percent_rank() &#x2F; rank() &#x2F; row_number() In these cases, built-in rules would specify the frame. MySQL 8.0 source code analysis (on execution process)stage of optimization setup windows: during the optimization process, if select_lex-&gt;m_windows is not NULL, then first call Window::setup_windows; The crucial interface would be Window::check_window_functions(THD *thd, SELECT_LEX *select). a. First judge the current window is dynamic or static. Static window (m_static_aggregates=True) would judge if the lower and upper bound of the window are defined. b. If the conditon is not satisfied, i.e., m_static_aggregates=False. Then further decide if it is based on range (m_range_optimizable) or rows (m_row_optimizable). Then decide if a row_buffer is required to calculate the result (if we need neighbor rows, no matter whether the window is static &#x2F; dynamic). Use Optimize-&gt; make_tmp_tables_info to decide if a temporary table is required as a windodw frame buffer. c. Stack calling: unit-&gt;first_select()-&gt;join-&gt;exec()-&gt;evaluate_join_record()-&gt;sub_select_op() -&gt;QEP_tmp_table::put_record()-&gt;end_write_wf() Code example:SUM(A+FLOOR(B)) OVER (ROWS 2 FOLLOWING) First finish the function calculation (FLOOR) not related with the window frame formulation. Then the result is put into the frame buffer, and frame buffer decides if the set of rows within the frame is already calculated (done in process_buffered_windowing_record). If the result does not satisfy the definition of window frame, then the calculation continues. Otherwise, the result is put into frame buffer and continue on processing the non-window function operators. process_buffered_windowing_record has two strategies of moving the sliding window, native strategy and optimizable strategy.The prior would go through all the rows within the row buffer. However, the latter would find the inverse function to eliminate the out-of-frame rows’ contribution made to the aggregation. Then, a normal aggregation function would add the contribution made by the row which just entered the frame.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"SQL-Study-Note-12 Common Table Expression and Discussion on UNION","slug":"SQL-Study-Note-12","date":"2022-05-04T19:17:03.000Z","updated":"2022-05-05T23:11:11.470Z","comments":true,"path":"2022/05/04/SQL-Study-Note-12/","link":"","permalink":"https://umiao.github.io/2022/05/04/SQL-Study-Note-12/","excerpt":"Common Table Expression (CTE) is viewed as a better way to realize the functionality of subquery.","text":"Common Table Expression (CTE) is viewed as a better way to realize the functionality of subquery. Supported by MySQL &gt;&#x3D; 8.0 Generate a named temporary table, only survives during the query Comparing with subquery: CTE can be referred multiple times within one query, and is able to refer itself (in recursive manner) Syntax (of Common Table Expression)12345678WITH cte(col1, col2) AS -- Name the temporary table here, col_name is brackets(SELECT 1, 2 UNION ALL SELECT 3, 4)SELECT col1, col2 FROM cteUNION ALLSELECT * FROM cte -- Can be referred for multipletimes, subquery can be used only onceORDER BY col1 Recursively generate sequence12345678WITH RECURSIVE test as -- USE recursive keyword to call itself(SELECT 1 AS UNIONUNION ALLSELECT 1 + n FROM test -- call itselfWHERE n &lt; 10 -- break when n &gt; 10)SELECT * FROM test The script above would generate results like: 12345n12...10 Another example about querying the quest &#x2F; reply pairs recursively 12345678910111213141516171819202122232425WITH RECURSIVE replay ( quest_id, quest_title, user_id, replyid, path ) AS ( SELECT -- Select all the answers without reply quest_id, quest_title, user_id, replyid, cast( quest_id AS CHAR ( 200 ) ) AS path FROM imc_question WHERE course_id = 59 AND replyid = 0 -- 0 means that there does not exist reply UNION ALL-- search the reply / comments recursively SELECT a.quest_id, a.quest_title, a.user_id, a.replyid, CONCAT( b.path, &#x27; &gt;&gt; &#x27;, a.quest_id ) AS path FROM imc_question a -- table a stores the reply of table b JOIN replay b ON a.replyid = b.quest_id -- recursively join on table &#x27;reply&#x27;, that is this very CTE ) SELECT * FROM replay UNION V.S. UNION ALLAs discussed previously, MYSQL UNION is able to combine the results of multiple queries: 123SELECT column, ... FROM table 1UNION \\ [ALL\\]SELECT column, ... FROM table 2 In these SELECT clauses, the corresponding columns should have the same attributes (column names), and the attribute name in the FIRST appearing clause would be used as the result’s attribute name. The main difference of UNION &#x2F; UNION ALLWhen using UNION, MySQL would remove the duplicates in the query result. When using UNION ALL, MySQL would return all the results, with a higher efficiency comparing with UNION. Using ORDER BY in UNION sub clauseIf ORDER BY is used in the sub clause (of SELECT), the result of the sub clauses would first be sorted, before combined.Besides, the entire sub clause should be wrapped with brackets, including LIMIT: 123(SELECT aid,title FROM article ORDER BY aid DESC LIMIT 10) UNION ALL(SELECT bid,title FROM blog ORDER BY bid DESC LIMIT 10) Using ORDER BY in entire query with UNIONIf you want to use ORDER BY &#x2F; LIMIT to restrict or classify the combined result of UNION, you should add brackets to each single SELECT clause: 1234(SELECT aid,title FROM article) UNION ALL(SELECT bid,title FROM blog)ORDER BY aid DESC When alia is usedIf alia is used, then ORDER BY clause MUST refer the alia: 1(SELECT a AS b FROM table) UNION (SELECT ...) ORDER BY b","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"DS-Study-Note-8 Random Forest","slug":"DS-Study-Note-8","date":"2022-05-04T05:48:16.000Z","updated":"2022-05-04T17:42:41.778Z","comments":true,"path":"2022/05/03/DS-Study-Note-8/","link":"","permalink":"https://umiao.github.io/2022/05/03/DS-Study-Note-8/","excerpt":"Random Forest inherits the idea of bagging, which is part of Ensemble Learning paradigm.","text":"Random Forest inherits the idea of bagging, which is part of Ensemble Learning paradigm. Introduction to Ensemble Learning It can be simply categorized into Boosting, Bagging and Stacking. Stacking: use Logistics Regression to integrate multiple prediction results and output one single prediction. It can be viewed as a more complicated form of voting (most commonly appeared in classification tasks, take the result with most votes). Bagging and Boosting both somehow combine existing classification &#x2F; regression methods to form a stronger classifier (utilize some sort of group intelligence). The difference lies in the combining method. BaggingAlso known as Bootstrap aggregating. ProcedureThe idea is to : 1. sample $n$ data samples with bootstraping method (sample with replacement) from the dataset, to form $k$ training sample sets by repeating $k$ times. That is to say, some samples can be contained in multiple training sample sets, while some samples may not be contained by any training sample sets. 2. We can tell that the $k$ training sets are independent with each other. 3. Execute learning algorithm on the $k$ training sets to achieve $k$ models. 4. Receive the classification &#x2F; regression results by integrating the $k$ outputs of these models (with voting &#x2F; averaging). Characteristics: Highly Parallelizable. The generated models are highly independent with each other. All the models have the same significance (equally important). Representative Work: Random Forest BoostingThe idea is to combine multiple ‘weak’ classifiers to form only ONE ‘strong’ classifier to generate ONE prediction, rather generating and processing $k$ individual predictions. This method runs under the Approximately Correct (PAC) framework. This theory supports that you are bound to leverage multiple weak classifiers into a stronger one.Implementation In each round, the weight distribution of the training data would be altered. The weight of samples which are falsely classified would increase while the weight of the correctly classified samples would be reduced to accelerate the iteration &#x2F; convergence. Different ways of combining the weak classifiers: Use additive model to generate a linear combination. AdaBoost: Through the weighted majority voting method, the weight of classifiers with less error rate would be increased. Weight of classifiers with high error rate would decrease. Boosting Tree: The classifers establish series connection and each classifier fits the residual of the prior one. Use the additive sum of all the classifiers as the predicted value. Representative Work: GBDT &#x2F; XGBoost Summary on Bagging and Boosting Bagging: would generate multiple training sets with replacement. Each sample &#x2F; classifier has the same weight. Can be easily Parallelization. Boosting: would not modify the training set but the samples’ weight. The weight of each sample &#x2F; classifier would be changed according to error rate. The training of classifiers should be in sequential order. Model setup &#x2F; combination Bagging + Decision Tree &#x3D; Random Forest AdaBoost + Decision Tree &#x3D; Boosting Tree Gradient Boosting + Decision Tree &#x3D; Gradient Boosted Decision Trees (GBDT) Decision TreeHow it is built General description: Randomly sample from the dataset to train decision tree. (First select $N$ samples to train a decision tree which is to be placed at the root node) Randomly select attribute (feature) used for training. (If the data has $M$ attributes, then randomly select a subset with size of $m$) Decide the attribute used for node splitting. (Use some metirc (information gain, gini impurity) to select one attribute as the splitting attribute of this node) Repeat such process untill reach the preset boundary (Unable to split &#x2F; reach required depth). (If the selected splitting attribute is already used by the father node, it also indicates that we have reached the leaf node and no further splitting is required) Build massive Decision Trees to form a forest. Pros and ConsAdvantages Able to process data with high dimensions without dimensional reduction and feature selection. Indicates the importance and correlation between features. Resist overfitting. High training speed, easy to parallelize. Easy to implement. Able to alleviate the imbalance in dataset distribution. Able to resist missing data partially.Disadvantages Overfitting on some regression and classification tasks with high noise volumn. In favor of attributes with more possible ways of splitting (in discrete cases).With more choices of splitting &#x2F; more unique values, an attribute can cast greater impact on the Random Forest classifier, making the result unreliable.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"General Knowledge","slug":"Data-Science/General-Knowledge","permalink":"https://umiao.github.io/categories/Data-Science/General-Knowledge/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://umiao.github.io/tags/Machine-Learning/"},{"name":"Random Forest","slug":"Random-Forest","permalink":"https://umiao.github.io/tags/Random-Forest/"}]},{"title":"DS-Study-Note-7 L1 & L2 Regularization","slug":"DS-Study-Note-7","date":"2022-05-02T23:37:57.000Z","updated":"2022-05-04T05:48:01.710Z","comments":true,"path":"2022/05/02/DS-Study-Note-7/","link":"","permalink":"https://umiao.github.io/2022/05/02/DS-Study-Note-7/","excerpt":"The essence of L1 and L2 regularization (with corresponding L1 &#x2F; L2 Norm): the projection of a vector to the domain of positive real number. They can both be viewed as metrics of distance.","text":"The essence of L1 and L2 regularization (with corresponding L1 &#x2F; L2 Norm): the projection of a vector to the domain of positive real number. They can both be viewed as metrics of distance. Summary L1 normalization would make many parameters become zero (equivalent of removing these parameters) due to the property of sparsification. L2 normalization is easier to calculate and avoid the issue of discussion on the absolute value function. Only one optimal prediction exists with L2 while multiple optimal solutions may exist with L2 (bacause of the non-linear point at $0$). TheoryWhenever we apply Gradient Descend algorithm for parameter optimization, we need to find the gradient (derivative) and use the result for parameter update:$$ \\theta &#x3D; \\theta - \\alpha \\frac{\\partial}{\\partial \\theta} J(\\theta) $$Here, $\\theta$ stands for the model parameters, $\\alpha$ stands for the learning rate and $J$ stands for the objective &#x2F; loss function. The function and derivative of L1 &#x2F; L2 norm is shown in the above image. We can tell that $$ \\frac{dL_1(w)}{dw} &#x3D; sgn(w) \\\\ \\frac{dL_2(w)}{dw} &#x3D;w $$ It is easy to find that, whenever the gradient is computed and used for update, the gradient of L1 function (if not equals zero), can only be $1$ or $-1$. Thus, for some parameters, they would head towards $0$ with steady pace (this is the cause of sparsity).However, for L2 function, the gradient’s value would vanish when a certain parameter $w$ becomes closer to $0$. This means that with L2 regularization, some parameters may become close to $0$ but would never reach $0$.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"General Knowledge","slug":"Data-Science/General-Knowledge","permalink":"https://umiao.github.io/categories/Data-Science/General-Knowledge/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://umiao.github.io/tags/Machine-Learning/"},{"name":"Regularization","slug":"Regularization","permalink":"https://umiao.github.io/tags/Regularization/"}]},{"title":"DS-Study-Note-6 Naive Bayesian modeling","slug":"DS-Study-Note-6","date":"2022-05-02T16:39:44.000Z","updated":"2022-05-02T23:36:40.323Z","comments":true,"path":"2022/05/02/DS-Study-Note-6/","link":"","permalink":"https://umiao.github.io/2022/05/02/DS-Study-Note-6/","excerpt":"Naïve Bayesian Classifier: is a typical learning based method which make hypothesis on the distribution of prediction target.","text":"Naïve Bayesian Classifier: is a typical learning based method which make hypothesis on the distribution of prediction target. The discrete caseLet $X$ be the input feature vector and $Y$ be the labels, then our target is to find out the $Y$ which maximizes the conditional probability $P(Y|X)$, with given $X$.In the discrete case, we assume the conditional probabilities (of different channels of the input feature vector) are independent from each other, $$P(X^1, …, X^D|Y) &#x3D; \\prod_{d&#x3D;1}^DP(X^d|Y)$$Then, we can simply use the total probability formula to traverse the existing data to find out the desired $Y$ which corresponds to the maximal conditional probability.$$ P(Y&#x3D;k|x^1, …, x^D) &#x3D; \\frac{\\prod_{d&#x3D;1}^DP(x^d|Y&#x3D;k)P(Y&#x3D;k)}{\\sum_j\\prod_{d&#x3D;1}^DP(x^d|Y&#x3D;j)P(Y&#x3D;j)} $$ $$ Y&#x3D;{argmax}_k \\frac{\\prod_{d&#x3D;1}^DP(x^d|Y&#x3D;k)P(Y&#x3D;k)}{\\sum_j\\prod_{d&#x3D;1}^DP(x^d|Y&#x3D;j)P(Y&#x3D;j)} \\\\ &#x3D;{argmax}_k \\prod_{d&#x3D;1}^D P(x^d|Y&#x3D;k)P(Y&#x3D;k) $$ Apply to limited dataset$$ P(X^i&#x3D;v_j|Y&#x3D;k)&#x3D;\\frac{samples \\quad with \\quad X^i&#x3D;v_j \\quad and \\quad Y&#x3D;k}{samples \\quad with \\quad Y&#x3D;k} $$ For some outliers, i.e., for given input $X$, there does not exist such data (Y), you can use smoothing method for Interpolation (e.g., set a default value which equals the mean). Scaling factorYou can introduce a scaling factor $I$ to adjust the weight between the training data and the default mean value: $$ P(X^i&#x3D;v_j|Y&#x3D;k)&#x3D;\\frac{(samples \\quad with \\quad X^i&#x3D;v_j \\quad and \\quad Y&#x3D;k) + l}{(samples \\quad with \\quad Y&#x3D;k) + lM} $$$$ P(Y&#x3D;k)&#x3D;\\frac{(samples \\quad with \\quad label \\quad k) + l}{(data \\quad samples) + lK} $$Here $M$ stands for the number of unique values of $X$ (input) and $K$ stands for the number of unique values of $Y$ (output). The continuous caseWhen processing continuous functions, we need to assume the distribution of the target function, and Normal Distribution is most commonly used. Normal Distribution can be described with two parameters, the expectation (mean) $\\mu$ and the variance $\\sigma$. These two can be estimated with statistics, so we select Normal Distribution to describe the conditional probability.Of course, we use assumption not only about distribution but also about independence. We assume $P(Y|x_1, x_2)&#x3D;P(x_1|Y) \\times P(x_2|Y)$. $$ \\mu_j^i &#x3D; E[X^i|Y&#x3D;j] \\\\ \\sigma_j^{2^i} &#x3D; E[(X^i - \\mu_j^i)^2|Y&#x3D;j]$$ Generally, the essence is to determine a distribution relying on the statistics and then use the determined distribution to fit the real distribution. Pros and Cons Simple and straightforward. Provide the probabilistic distribution function Explainable &#x2F; interpretable Require domain knowledge. Require dataset for learning Performs well even the i.i.d assumption is not satisfied Using the normal distribution for modeling provides some good properties, but may be contrary to the facts Correction of distribution modeling with Gaussian distributionApplying the Gaussian &#x2F; Normal distribution can introduce good properties but may not be reasonable. E.g., the Gaussian distribution has two long tails at the left and right, and it has exactly one peak.In order to model the distribution which may have multiple peaks, you can use the sum of multiple Gaussian functions for modeling. You can still use the training data to estimate the parameters of these Gaussian distributions. EM (Expectation Maximization) Algorithm for parameter estimationIt includes two steps: the expection-step (E-step) and maximization-step (M-step). The prior estimate the parameters by observing the data and existing model, and then computes the expection of the likelihood function with the parameters. The latter find the parameters which maximizes the likelihood function. The algorithm assures that after each iteration, the value of likelihood function would increase, so that the function is bound to converge. Instance of EM algorithmGiven two coins with different distribution, we need to estimate their expection of head-up probability after flipping. However, the experiment records do not specify which coin a single record corresponds to. Thus, we need to estimate the coin an experiment record corresponds to, and the expection of getting a head after flipping at the same time. We first initialize the expection of the two coins to be different values. For each experiment record, find out the distribution of the mapping relationship $Z$, to complete a E-Step (e.g., 0.7 belongs to coin A, 0.3 belongs to coin B). Use the achieved $Z$ to assign weights to the experiment records. (View each record as a mixture of using coin A and B). Update the experiment results to: the estimate experiment result when flipping coin A &#x2F; B. (times the distribution of $Z$ with the experiment results) Base on the priciple of maximize the likelihood, use the updated experiment records to calculate the expectation of the two coins iteratively, as the M-Step Apply EM algorithm to solve Gaussian mixture modelEach observation (experimente record) corresponds to the overlap of multiple normal distributions and the parameters are unkown. Then, we can randomly initialize the parameters for the distributions, and find out how each data point can be decomposed into these distributions. Then, we can get the datapoints weighted so that it only includes one distribution. Then we can correct the parameters of normal distribution in an iterative manner. It should be noted that the guarantee of convergence does not mean that the EM algorithm can converge at the global optimal point, because the given initial parameters decide the upper-bound of performance to a large extent. AppendixThis is about the deduction of EM algorithm’s property of convergence guarantee. Jensen Inequityif $f$ is a Concave function, $X$ is a random variable, then $E[f(X)]\\le f(E[x])$.Similar conclusion holds, with the unequal sign in the opposite direction, when $f$ is a Convex function. Let $P(x,z)$ be the distribution with latent variable $z$ (the weight &#x2F; contribution of a certain Gaussian distribution made to a datapoint).$$ \\sum_{i&#x3D;1}^M\\sum_{z&#x3D;1}^NlnP(x,z) &#x3D; \\sum_{i&#x3D;1}^Mln\\sum_{z&#x3D;1}^NQ(z) \\frac{P(x,z)}{Q(z)}$$With Jensen inquity (log is a concave function):$$ \\sum_{i&#x3D;1}^Mln\\sum_{z&#x3D;1}^NQ(z) \\frac{P(x,z)}{Q(z)} \\ge \\sum_{i&#x3D;1}^M\\sum_{z&#x3D;1}^NQ(z) ln\\frac{P(x,z)}{Q(z)} $$ The key step is to adjust $Q(z)$ so that makes the right part equals the left part (reach the $&#x3D;$). The $&#x3D;$ can be reached when the input function is a constant function, i.e., $\\frac{P(x,z)}{Q(z)}&#x3D;c$.Then we have $\\sum_z P(x,z) &#x3D; c\\sum_z Q(z) \\Rightarrow Q(z) &#x3D; \\frac{P(x,z)}{\\sum_zP(x,z)} &#x3D; \\frac{P(x,z)}{P(x)} \\Rightarrow P(z|x)$ Then, we solved the issue of how to select $Q(z)$ – by selecting the posterior probability. This is the essence of E-step which is to build lower-bound for the likelihood function $L(\\theta) &#x3D; \\sum_{i&#x3D;1}^M\\sum_{z&#x3D;1}^NlnP(x,z)$, where $\\theta$ is the parameter set of these distributions.The following M-step aims at adjusting $\\theta$ to maximize the lower-bound of $L(\\theta)$.It should be noted that this process is guaranteed to converge, but it may get into local optimal rather than the real parameter values (reach the global optimal). This is determined by the initialization of parameters.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"General Knowledge","slug":"Data-Science/General-Knowledge","permalink":"https://umiao.github.io/categories/Data-Science/General-Knowledge/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://umiao.github.io/tags/Machine-Learning/"},{"name":"Naive Bayes","slug":"Naive-Bayes","permalink":"https://umiao.github.io/tags/Naive-Bayes/"}]},{"title":"Missing Values Patterns in Time Series Data","slug":"Missing-Values-Patterns-in-Time-Series-Data","date":"2022-05-02T03:36:26.000Z","updated":"2022-05-02T05:18:55.133Z","comments":true,"path":"2022/05/01/Missing-Values-Patterns-in-Time-Series-Data/","link":"","permalink":"https://umiao.github.io/2022/05/01/Missing-Values-Patterns-in-Time-Series-Data/","excerpt":"It is meaningful and believed to be possible to discover the pattern of the missing parts of the time series data. Such patterns may vary in different scenarios and sources and may be related with physical devices and configurations.","text":"It is meaningful and believed to be possible to discover the pattern of the missing parts of the time series data. Such patterns may vary in different scenarios and sources and may be related with physical devices and configurations. Algorithms for pattern detection In the above image, we can find out the distribution and comparison between the imputed values VS the known patterns. A pipeline of missing data pattern detecting is proposed in this paper.1 Create base matrices to represent data. An algorithm to quantify and categorize missing values slots. Evaluate the frequencies of time attributes to determinate the most crucial time scenarios to analyze. Use this time attributes to find patterns over classified slots applying Kernel Density Estimation (KDE) that is considered as a statistical model to understand the shape and features of data. Data missing Around 48% of studies process dataset with missing values.2 You can discard the records with missing values, or use data imputation methods to recover the missing values. (this can be especially common in time series data) It would be helpful if you can find out the mechanism of data missing so that you can select a suitable imputation method. Avoid the missing of values during collection would always be the best solution! Missing value mechanism Missing At Random (MAR): (may work well with statistical based methods) Missing Completely At Random (MCAR): (may work well with Hot-Deck) Missing Not At Random (MNAR): (may works well with learning algorithm, like Random Forest) Problem formulationRepresentationConsidering $n$ IOT devices (sensors), each report one attribute and they monitor in a period of $t$ time slots. The, we can denote the data with $x$ and specify a value with $x(t, n)$ (at the $t$th time and collected by the $n$th device). The $t$ is expected to be in the form of a timestamp and $x$ can be viewed as a 2D matrix. We can also introduce a binary matrix (as an indicator) to mark if an element of the above matrix is missing (equals null). We can define$$BM &#x3D; X(t, n) &#x3D; \\begin{cases} 0,\\quad (x(t, n) \\quad is \\quad null) \\\\ 1 \\quad (otherwise) \\end{cases} $$ Feature selectionA series of papers propose different feature selection strategies based on the matrix $x$ to formulate new feature sequences, including: finding the cumsum, finding the indexes of missing values, transpose of the missing value indexes, record the missing values’ count and span, etc. The missing value spans can also be pre-categorized into different levels (e.g., minute &#x2F; hour &#x2F; day level). KDE: Kernel Density EstimationSelect a bandwidth parameter $h$ (may be viewed as the window’s length) and a kernel function $K(x;h)$. The function $K$ can be selected from: gaussian, tophat, epanechnikov, exponential, linear or cosine. The 1-D time series case:$$ \\hat f_K(x)&#x3D;\\sum_{i&#x3D;1}^n K_{h_x}(x-x_i) $$The 1-D time series case: (the timestamp may have 2 or more channels)$$ \\hat f_K(x,y)&#x3D;\\sum_{i&#x3D;1}^n K_{h_x}(x-x_i) K_{h_y}(y-y_i)$$Use the above function to retrieves the points to make a bivariate scatter diagram to visualizeand understand the shape and features of missing data periods. Here $x$ and $y$ may represent the start and end time of a missing part. The mined patterns would be like the visualization results shown above. We can find the common parts shared by the periodical data and view the rest parts as random noise. References [1] [Lima, Juan-Fernando, Patricia Ortega-Chasi, and Marcos Orellana Cordero]”A novel approach to detect missing values patterns in time series data.” Conference on Information Technologies and Communication of Ecuador. Springer, Cham, 2019. [2] [Dong, Y., Peng, C.Y.J.]Principled missing data methods for researchers. Springer-Plus 2(1), 222 (2013).","categories":[{"name":"UCLA","slug":"UCLA","permalink":"https://umiao.github.io/categories/UCLA/"},{"name":"Course Study","slug":"UCLA/Course-Study","permalink":"https://umiao.github.io/categories/UCLA/Course-Study/"},{"name":"ECE209 in 2022 spring","slug":"UCLA/Course-Study/ECE209-in-2022-spring","permalink":"https://umiao.github.io/categories/UCLA/Course-Study/ECE209-in-2022-spring/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"UCLA","slug":"UCLA","permalink":"https://umiao.github.io/tags/UCLA/"}]},{"title":"DS-Study-Note-5 Support Vector Machine (SVM)","slug":"DS-Study-Note-5","date":"2022-04-27T06:51:47.000Z","updated":"2022-05-01T08:11:21.602Z","comments":true,"path":"2022/04/26/DS-Study-Note-5/","link":"","permalink":"https://umiao.github.io/2022/04/26/DS-Study-Note-5/","excerpt":"SVM is a machine learning model which aims at finding a Decision Boundary with a subset of the training set. The SVM is a non-probabilistic binary classifier.","text":"SVM is a machine learning model which aims at finding a Decision Boundary with a subset of the training set. The SVM is a non-probabilistic binary classifier. Linear SeparableLinear Separable stands for an attribute that two class of points can be completely divided by a hyperplane (more specifically, a line in the 2-D space).A hyperplance can be determined with normal vector $W$ and intercept $b$, i.e., $$ X^TW + b&#x3D;0 $$. For the two separable groups, they would satisfies $X^TW + b&gt;0$ and $X^TW + b&lt;0$, respectively. In order to enhance the robustness, we additionally require the best-fit hyperplane to separate these two classes with maximum margin &#x2F; interval, which is called Maximum Margin Hyperplane. SVM (Support Vector Machine)In the training set, the points which are nearest to the hyperplane are named Support Vector. After generalized to $n$ dimensional space, a point $x &#x3D; (x_1, …, x_n)$’s distance to the hyperplane $w^Tx+b&#x3D;0$ is $\\frac{|w^Tx+b|}{||w||}$ (the denominator corresponds to 2-norm). We are interested with these support vectors and optimize the hyperplane in order to maximize the margin between the support vectors which belong to different classes. If the maximized distance equals $d$, for all the support vectors, we further unfold the absolute value expression to have:$$ \\frac{w^Tx+b}{||w||} \\ge d, y&#x3D; 1; \\quad \\frac{w^Tx+b}{||w||} \\le -d, y&#x3D; -1$$while $y$ marks different classes.Ignore the constant factor, we can have $$ w^Tx+b \\ge 1 (y&#x3D;1); \\quad w^Tx+b \\le -1 (y&#x3D;-1)$$which equals $$ y(w^Tx+b) \\ge 1 $$, so that we use hyperplanes $w^Tx+b&#x3D; \\pm 1$ to seperate the two classes. Replace the numerator of the distance expression with the two hyperplanes to have our optimization target:$$ \\max_{w, b} margin \\Leftrightarrow \\max(\\frac{2}{||w||}) \\Leftrightarrow \\max(\\frac{2}{||w||})^2 \\Leftrightarrow \\min({||w||}^2), \\quad y(w^Tx+b) \\ge 1 $$ We can add a constant factor of $\\frac{1}{2}$ to absorb the constant factor after get derivative of $w^2$. Support VectorsNow we can tell that support vectors are all the vectors on the lines of $wx^T +b &#x3D; \\pm 1$. Only the support vectors would contribute to the classification. Primal-Dual TransformationIn order to solve the primal problem of $\\frac{1}{2} {||w||}^2$, we can use Method of Lagrange Multiplier to solve its Dual Problem. Solving the corresponding dual problem has the advantages of: Easier to solve with simpler constraints and only need to optimize one variable $\\alpha$ Able to introduce kernel function to generalize to non-linear cases Method of Lagrange Multiplier Problem formulation$$ \\min f(x_1, …, x_n) s.t. h_k(x_1, …, x_n)&#x3D; 0, k&#x3D;1,2,…,l $$That is to say, we decide to optimize function $f(x_1, …, x_n) $ with $l$ extra constraints. We can let $$ L(x, \\lambda &#x3D; f(x) + \\sum_{k&#x3D;1}^l \\lambda_k h_k(x) $$, where $L(x, \\lambda)$ is named Lagrange Function, and $\\lambda$ is NOT required to be non-negative. When solving the problem, we need to find $\\frac{\\partial L(x, \\lambda)}{\\partial x_i}$ for each $i \\in {1, …, n}$ and let them be $0$. This is called necessary condition of equality constraints (in order to get extremum). Strong DualityWe want to transform $$ \\min_w \\max_\\lambda L(w, \\lambda) \\Rightarrow \\max_\\lambda \\min_w L(w, \\lambda), \\quad (\\lambda_i \\ge 0)$$For function $f$, if we have $\\min \\max f \\ge \\max \\min f$, that is, the minimum of the possible maximums is still greate than the maximal possible minimums, we say there exists weak duality. If $f$ is convex optimization problem, we have strong duality.In order to handle the SVM problem, we require the Karush-Kuhn-Tucker (KTT) condition as the necessary and sufficient condition of strong duality. Karush-Kuhn-Tucker (KTT) conditionWe need to transform the optimization of inequality into optimization of equality:$$ \\min f(w)&#x3D;\\min \\frac{1}{2} {||w||}^2, g_i(w) &#x3D; 1 - y_i(w^Tx_i+b)\\le 0 $$into$$ L(w, \\lambda, a) &#x3D; f(w) + \\sum_{i&#x3D;1}^n \\lambda_i h_i(w) &#x3D; f(w) + \\sum_{i&#x3D;1}^n \\lambda_i [g_i(w) + \\alpha_i^2], \\lambda_i \\ge 0$$$g_i(w)$ is guaranteed to be $\\le 0$ for $\\forall i$, we can solve a series of non-zero values (a_i^2) to make every term of constraints $0$. Then we require $L$’s’ partial derivative to $w$, $\\lambda$ and $a$ equals $0$ to derive the KKT condtion:$$\\min L(w, \\lambda, a) \\Rightarrow \\min L(w, \\lambda) &#x3D; f(w) + \\sum_{i&#x3D;1}^n \\lambda_i g_i (w)$$The achieved minima must be greater than $0$, we have $\\sum_{i&#x3D;1}^n\\lambda_i g_i(w)\\le 0 $, $L(w, \\lambda) \\le p$ as a natural upper bound. So that our objective can be changed to $\\max_\\lambda L(w, \\lambda)$.The dual optimization problem after transformation: $$\\min_w \\max_\\lambda L(w, \\lambda) , \\lambda_i \\ge 0$$ Procedure of solving SVM problem Construct Lagrange Function $\\min_{w,b}\\max_{\\lambda}L(w,b,\\lambda)&#x3D; \\frac{1}{2} {||w||}^2 + \\sum_{i&#x3D;1}^n \\lambda_i [1 - y_i(w^Tx_i+b)] $ Transform to dual problem $\\max_\\lambda \\min_{w,b}L(w,b,\\lambda)$Find derivatives: $\\frac{\\partial L}{\\partial w}&#x3D;w - \\sum_{i&#x3D;1}^n \\lambda_i x_iy_i&#x3D;0$,$\\frac{\\partial L}{\\partial b}&#x3D;\\sum_{i&#x3D;1}^n \\lambda_i y_i&#x3D;0$ to get $\\sum_{i&#x3D;1}^n \\lambda_i x_iy_i&#x3D;w$, $\\sum_{i&#x3D;1}^n \\lambda_i y_i&#x3D;0$.Substitue into function:, that is, $$ \\min_{w,b}L(w,b,\\lambda)&#x3D;\\sum_{j&#x3D;1}^n\\lambda_i - \\frac{1}{2} \\sum_{i&#x3D;1}^n\\sum_{j&#x3D;1}^n \\lambda_i\\lambda_jy_iy_j(x_i \\cdot x_j) $$ We can tell that the above question is a quadratic programming problem and its scale is proportional to the number of training samples. It can be solved with Sequential Minial Optimization (SMO) algorithm. It optimizes one parameter a time and fixed the others. We just mentioned that SMO algorithm optimize only one parameter a time. However, we have a constraint $\\sum_{i&#x3D;1}^n \\lambda_i y_i&#x3D;0$ which must be satisfied. Thus, we update two parameters a time to solve this issue. With two selected parameters $\\lambda_i$ and $\\lambda_j$, we fix the other parameters and we have $$\\lambda_i y_i + \\lambda_j y_j &#x3D; c, \\lambda_i \\ge 0, \\lambda_j \\ge 0, c&#x3D; -\\sum_{k \\ne i,j}\\lambda_ky_k $$ Then we can have $\\lambda_j &#x3D; \\frac{c-\\lambda_iy_i}{y_j}$ so that we can replace $\\lambda_j$ with expression of $\\lambda_i$, find the partial derivative for $\\lambda_i$ and update the two paramteres, keep iterating until converge. By find partial derivative, we can have $w&#x3D;\\sum_{i&#x3D;1}^m \\lambda_i y_i x_i$. All the points corresponding to a $\\lambda_i &gt; 0$ are support vectors (non-support vectors correspond to $\\lambda_i&#x3D;0$), and we can find an arbitary support vector $x_s$ and substitute x to have $y_s(wx_s+b)&#x3D;1 \\Rightarrow y_s^2(wx_s+b)&#x3D;y_s$ to find $b$. $y_s^2&#x3D;1$, thus $b&#x3D;y_s - wx_s$. We can also find b via the mean value of the support vectors: $$b&#x3D;\\frac{1}{|S|}\\sum_{s\\in S}(y_s - wx_s) $$ With $w$ and $b$ found, we can construct the hyperplane $w^Tx+b&#x3D;0$ and use $f(x)&#x3D;sign(w^Tx+b)$ to decide the classification result. Parameters to be searched (tuned) in SVM C: decide the level of regularization. In fact, $C&#x3D;\\frac{1}{\\lambda}$ and $\\lambda$ is the regularization coefficient. (Is a squared l2-penalty) kernel: The kernel function adopted by SVM model. degree: The highest degree, when adapting Polynomial kernel function. gamma: Coefficent of the kernel function.Beside, you can decide whether to use heuristic shrinking, tolerance (the desired accuracy at convergence), size allocated to kernel cache, iteration times. You can also appoint random state for reproducing results. Soft MarginThe original SVM require the problem to be solved is linear separable. When this prerequisite is not satisfied, we can use soft margin to solve this problem. Originally, we require the two classes correspond to different signs, in a hyperplane expression. Now, we can allow SVM to misclassify on a handful of vectors.$$ y_i(w^Tx_i+b) - \\xi_i \\ge 1$$ which means that for such $i$, they do not meet the constraint and need the help of relax coefficient $\\xi_i$. In order to minimize the number of misclassified vectors, we would fix our objective (loss) (by appending a regularization term for the relax coefficients):$$ \\min_w \\frac{1}{2}{||w||}^2 + C \\sum_{i&#x3D;1}^m \\xi_i, \\quad g_i(w,b)&#x3D;1-y_i(w^Tx_i+b)-\\xi_i \\le 0, \\xi_i \\ge 0 $$Here, $C &gt; 0$ and can be understood as the penalty to the misclassified samples. If $C \\rightarrow \\infty$, then $\\xi_i \\rightarrow 0$ and the SVM (with soft margin) becomes linear separable SVM. Kernel FunctionThe essence of kernel function $K$ is to map vectors from low-dim Hilbert Space to high-dim. By this mean, we expect the samples which are not linear separable become separable, in the high-dim space.$$K(x,z)&#x3D;\\phi(x) \\cdot \\phi(z)$$Prerequisite: The Gram matrix formed by the set of all the points of the function is semi-positive definite. (a.k.a. complete inner product space) The introduction of kernel function is to solve the case of linear unseparable. It is costly to map samples to high-dimensional space before calculating dot product, thus kernel function can realize effectively calculating the dot product result of high-dimensional space, in low-dimensional space. Types of kernel functions Linear kernel: $x_i^Tx_j$ Polynomial Kernel: $(x_i^Tx_j+c)^d$ Radial basis function kernel: $exp(-\\frac{||x_i-x_j||_2^2}{2\\sigma ^2})$ Hyperbolic tangent kernel: $tanh(Kx_i^Tx_j+c), \\quad K&gt;0, c&lt;0$It is clear that only the latter three needs parameter tuning. Pros and ConsAdvantages: Supported by math theory, highly interpretable Do not rely on statistical method, simplify the regular classification and regression problem (solely rely on the support vectors, which are deterministic and crucial samples) Applying of kernel function can handle the unlinear tasks Computational complexity relies on the number of support vectors(rather than the dimensional of the sample space), avoid the curse of dimensionDisadvantages: Long training time (every time select a pair of parameters for optimization to maintain the sum unchanged). The complexity would be $\\mathcal{O}(n^2)$ where $N$ equals the number of training samples. If applied kernel function and need to store the matrix, extra space of $\\mathcal{O}(n^2)$ is required. The prediction rate is inversely proportional to the number of support vectors, leading to high complexity. Not suitable for scenarios which have millions, or even hundreds of millison of samples.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"General Knowledge","slug":"Data-Science/General-Knowledge","permalink":"https://umiao.github.io/categories/Data-Science/General-Knowledge/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://umiao.github.io/tags/Machine-Learning/"},{"name":"SVM","slug":"SVM","permalink":"https://umiao.github.io/tags/SVM/"}]},{"title":"DS-Study-Note-4 Metrics","slug":"DS-Study-Note-4","date":"2022-04-27T05:28:19.000Z","updated":"2022-04-27T06:47:18.035Z","comments":true,"path":"2022/04/26/DS-Study-Note-4/","link":"","permalink":"https://umiao.github.io/2022/04/26/DS-Study-Note-4/","excerpt":"Metrics are used for model training and evaluation. It reveals a model’s performance on a given dataset.","text":"Metrics are used for model training and evaluation. It reveals a model’s performance on a given dataset. PreliminaryWe use: $f$ to denote our model (function) $D$ to denote the dataset used, $m$ as the sample it contains. Error rateDefined as the number of incorrectly classified samples divide the number of total samples.$$ E(f;D) &#x3D; \\frac{1}{m}\\sum_{i&#x3D;1}^m \\mathbb{I}(f(x_i) \\ne y_i) $$Similarly, the continuous form:$$E(f;D) &#x3D;\\int_{x \\sim D} \\mathbb{I}(f(x_i) \\ne y_i)p(x)dx $$ AccuracyDefined as the number of correctly classified samples divide the number of total samples.$$ acc(f;D) &#x3D; 1 - E(f;D) &#x3D; \\frac{1}{m}\\sum_{i&#x3D;1}^m \\mathbb{I}(f(x_i) &#x3D; y_i) $$Similarly, the continuous form:$$acc(f;D) &#x3D; \\int_{x \\sim D} \\mathbb{I}(f(x_i) &#x3D; y_i)p(x)dx $$ Recall and Precision RateWhen the distribution of different classes are NOT balanced, we may be interested in how many samples of interest (positive samples) is found (Recall Rate) and how many of the filter samples of interest are correct (Precision Rate). We use TP &#x2F; FP &#x2F; TN &#x2F; FN to denote the frequency of True Positive &#x2F; False Positive &#x2F; True Negative &#x2F; False Negative. Precision Rate$$ P &#x3D; \\frac{TP}{TP + FP} $$ (TP + FP stands for all the samples classified as positive) Recall Rate$$ R &#x3D; \\frac{TP}{TP + FN} $$ (TP + FN stands for all the samples whose labels are positive) Tradeoff Pursuing Recall Rate and Precision Rate at the same time is often contradictory. This is because a high Precision means that the model would be cautious as possible so that many positive samples with lower confidence would be classified as negative. PR CurveIn order to unify the Precision and Recall rate and compare the performance of different model, PR Curve is proposed. The model is required to sort all the samples by the confidence (of being positive sample). Normally, for the first sample, the Precision would be 1 while the Recall would be the minima (close to 0). Predict the current sample to be positive, one-by-one, in the sorted order and keep calculating the corresponding Precison and Recall Rate. When all the samples are predicted to be positive, we have a Recall of 1 and the Precision reaches the minima. Break-Event Point of PR CurveIn order to compare the performance of two models, we can use the area under the PR curve as the metric. However, this value can be hard to calculate. Thus, we use the Break-Event Point to reach a balance between the Recall and Precision.It is calculated by the x-coordinate of the intersection point of the PR Curve and function $y&#x3D;x$.Ideally, we want both the Precision and Recall to be as high as possible. F1-ScoreF1-Score is a more commonly used metric to reach a balance between Precision and Recall, it is defined as the Harmonic Mean of these two:$$ \\frac{1}{F1} &#x3D; \\frac{1}{2} \\times (\\frac{1}{P} + \\frac{1}{R}) $$ Macro scope (Macro-F1) For N-class classification problem, first calculates $N$ F1-Scores on $N$ Confusion Matrix. Find the arithmetic mean of the $N$ F1-Scores. Micro scope (Micro-F1) Find $N$ groups of $TP$, $FP$, $TN$, $FN$ and find the arithmetic mean for each of the four. Calculate F1-Score with the mean $TP$, $FP$, $TN$, $FN$. GeneralizationWith the method describe above, you can similarly define micro-P, macro-P, micro-R, macro-R. Receiver Operating Characteristic (ROC) Curve Similar to PR Curve, sort all the samples by the confidence of predicting as positive. Find $$ TPR &#x3D; \\frac{TP}{TP + FN} $$, $$ FPR &#x3D; \\frac{FP}{TN + FP} $$.These two values stand for the ratio of correctly classified positive samples and incorrectly classified negative samples. For a 0-1 Classification task, a model based on random guess‘s ROC Curve should correspond to $y&#x3D;x$ and an accuracy of $0.5$. Area Under ROC Curve (AUC) With finite samples, you can draw the coordinate $(TPR, FPR)$ for each point and find the area under the ROC Curve. This is a common metric in evaluating models. Find a confidence threshold for ROC Curve Set the threshold to $\\inf$ which exceeds the confidence (score) of all the positive samples and they are all predicted as negative. In this case, both the TPR and FPR equal $0$. With the threshold goes down, these two would gradually raise to $1$. Define Sorting Loss based on AUC For each pair of positive-negative samples, if the positive sample achieves a score lower than the negative sample, add $1$ to the Loss. If equals, add $0.5$. This corresponds to the area above the ROC Curve and thus should be minimized. $AUC &#x3D; 1- L_{rank}$ Similarly, we can assign different weights to different types of mistakes a classifer made and draw the cost curve. We can find the weighted sum and for each point, draw the curve of expected cost (line determined by $(0, FPR)$ and $(1, FNR)$. Then we find the lower bound of all such lines, the area under this curve stands for the cost.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"General Knowledge","slug":"Data-Science/General-Knowledge","permalink":"https://umiao.github.io/categories/Data-Science/General-Knowledge/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://umiao.github.io/tags/Machine-Learning/"}]},{"title":"DS-Study-Note-3 Dimension Curse","slug":"DS-Study-Note-3","date":"2022-04-26T22:19:27.000Z","updated":"2022-04-27T05:26:41.457Z","comments":true,"path":"2022/04/26/DS-Study-Note-3/","link":"","permalink":"https://umiao.github.io/2022/04/26/DS-Study-Note-3/","excerpt":"DefinitionDimension curse stands for the troubles you would meet when processing high-dimensional data. E.g., computation of similarity, distance, neighbour or any metric based processing.","text":"DefinitionDimension curse stands for the troubles you would meet when processing high-dimensional data. E.g., computation of similarity, distance, neighbour or any metric based processing. The reason is that for high-dimensional space, the concept of distance will gradually fail, and even make any two points infinitely inseparable, even if they look different &#x2F; correspond to very different categories. As the dimension (of vector representation) grows, the data becomes more and more sparse and may correspond to higher variance and bias error. These phenomena are essentially caused by the same theory. However, the theoretical derivation is omitted here. Specifically, dimension curse can be categorized into the following more concrete problems: Distance concentration Combinational explosion Hubness Distance concentration As the number of dimension raises, for the queried point, the distance to its closest neighbour would converge to the distance to the furthest point. This also means that the difference of distance between arbitrary two points would be negligible. The difference of distance may be small enough, for a dimension of 20. The distance may remain effective, if there lies inherent clusters in the data and these clusters are distant from each other. Or, if many of the data’s dimensions are redundant (the data can be embedded into a space with much smaller dimension). In high-dimensional space, small change of the neighbourhood radius determines the difference of selecting only ONE point or selecting ALL the datapoints. This is because the volume ratio of a fixed radius hypersphere to a unit radius hypersphere will be close to 1. Increase of relevant features would be beneficial to the model. Increase of irrelevant features would impair the model’s performance. Distance under different dimension and space CANNOT be compared with each other. Combinational explosion As the dimensionality increases, a larger percentage of the training data resides in the corners of the feature space. It is also much more difficult to traverse the increasing search space as the size of the search space grows exponentially, as shown in the above image. A complex search space may correspond to the same configuration (object &#x2F; training &#x2F; testing sample) in the low-dimensional case, resulting in overfitting. Training samples with larger scale are required to suppress overfitting (on data with high dimensions). Algorithms like Random Forest can restrict the number of features used (in one tree). Hubness With the increase of dimension, a handful of points are significantly more frequent to become the nearest neighbour of other points. This is also called Hubness. If we record the frequency of each point becoming the nearest neighbour of another point, we can find that is frequency follows Zipf’s Law and is right-skewed heavily. Generally, points close to the mean value of the entire dataset become hubs more easily. When cluster exists, data points close to the mean value of the cluster are more likely to become hubs as well. Outlier &#x2F; Anti-hubs These points are most distant from the majority of other points. Anti-intuitive things may happen, i.e., hubs exist in the low-density region of the high-dimensional space, but the hubs are close to the other points. At the same time, anti-hubs may exist in the high-density region of the high-dimensional space, but the anti-hubs are distant to the other points (which makes them outliers). This can be viewed as a mismatch between the probabilistic density and distance distribution. Solution (to dimension curse) For the cases of Multicollinearity and Duplication of Components(redundancy) happen, we can simply remove the redundant variables after evaluation. In the case when each dimension (feature) contributes equally to the model’s result, methods like Dimensional Reduction (including CNN, Convolutional Neural Network) can be applied. Dimensional Reduction’s Disadvantages: Converted data points DO NOT represent original features. Less interpretable, hard to visualize, weaker theoretical support.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"General Knowledge","slug":"Data-Science/General-Knowledge","permalink":"https://umiao.github.io/categories/Data-Science/General-Knowledge/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://umiao.github.io/tags/Machine-Learning/"}]},{"title":"DS-Study-Note-2 Bias VS Variance","slug":"DS-Study-Note-2","date":"2022-04-23T07:41:01.000Z","updated":"2022-04-26T22:18:22.843Z","comments":true,"path":"2022/04/23/DS-Study-Note-2/","link":"","permalink":"https://umiao.github.io/2022/04/23/DS-Study-Note-2/","excerpt":"The target of Machine Learning is to fit an (unknown) distribution. There lies three possible error: bias, variance and irreducible error.","text":"The target of Machine Learning is to fit an (unknown) distribution. There lies three possible error: bias, variance and irreducible error. The irreducible error CANNOT be avoided with any algorithm as it can be viewed as the result of unknown factor, noise, accidents, etc. Thus, we would focus on the bias and variance error. Definition Bias can be understood as the accuracy of the model, i.e., the ability to estimate the output value accurately. Variance can be understood as the stability of the model, i.e., the ability of resisting the noise and disturbance contained by the input. I also understood this ability as being able to recognize similar inputs and generate similar results for them. Example Applying K-fold cross validation can reduce the influence casted by the outliers and enhance the generalization ability, which reduces the variance error. At the same time, part of the data is not used for training, which impairs the model’s fitting ability and increase the bias error. An intuition is that, a more complex model is more sensitive to the noise contained by the input, which makes the output less stable (higher variance error). At the same time, a simpler model more easily ignores the random noise and difference of distribution between the training set and the testing set. Tradeoff and AnalysisThe variance of the parameters you are estimating can be reduced, at the cost of increasing bias. If you would like to generalize a model trained on a certain training set, then you CANNOT minimize the bias and variance error at the same time. Bias: Bias error comes from the erroneous assumptions of the learning algorithm. High bias error corresponds to underfitting. Bias error measures the closeness between the distribution you modeled and the expectation of the real distribution. Introduction of bias term in linear models aims at simplying the learning process without introducing way more complicated distribution which makes the model hard to generalize. At the same time, such models would fail to solve the more complicated models which do not meet the assumption (that this problem can be approximated by linear model). Variance: Variance error comes from the noise &#x2F; fluctuation &#x2F; disturbance of the training set. High variance error probably means that you are modeling on the random noise of the training set, which cannot be generalized, and this means overfitting. Variance error reveals the level of concentration of your model.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"General Knowledge","slug":"Data-Science/General-Knowledge","permalink":"https://umiao.github.io/categories/Data-Science/General-Knowledge/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://umiao.github.io/tags/Machine-Learning/"}]},{"title":"DS-Study-Note-1 Overfitting and Brief Introduction on Decomposition and Regularization","slug":"DS-Study-Note-1","date":"2022-04-22T16:23:14.000Z","updated":"2022-04-23T07:37:49.936Z","comments":true,"path":"2022/04/22/DS-Study-Note-1/","link":"","permalink":"https://umiao.github.io/2022/04/22/DS-Study-Note-1/","excerpt":"Overfitting is a modeling error in statistics that occurs when a function is too closely aligned to a limited set of data points. —- Definition ranked 1st in Google","text":"Overfitting is a modeling error in statistics that occurs when a function is too closely aligned to a limited set of data points. —- Definition ranked 1st in Google Definition Overfitting stands for making excessive learning steps on the training set, which results in the model extracting noise of the training set as valid pattern to fit the training set’s distribution. In this case, the trained model would show low performance on the testing set and new data (real-world data). Possible Causes: The volume of the training data is too small. The data distribution of the training set data does NOT subject to the testing and real business data. This also means that the iid (identically and independent distributed) assumption is not satisfied. There exists noise in the training set. Too many iteration times. Fail to learn correct features with ability of generalization and representative. The overfitting MAY be phenomenon of information leakage, i.e., too complicated model remembers the training which makes the inference equivalent to the table look-up. Solutions: Discard some features. This can be implemented by Feature Selection or simply randomly discard a subset. This process can be: Conducted manually. Randomly. (Random Forest) Decided by Model Selection Algorithm. E.g., PCA(Principal component analysis). PCA: Solve the eigen-vector of the covariance matrix. It is obvious that the larger the covariance is, the more useful the corresponding eigen-value is. Find the largest k eigen-values and use their corresponding eigen-vectors to form a matrix as the PCA output. (You can also use SVD for such decomposition.) You can also use dimensional reduction tools like LR(lower–upper) decomposition, SVD(Singular Value Decomposition). Introduce regularization. Introduce drop-out layer when training network. Use Early-Stop to achieve the tradeoff the generalization ability and convergence on the training set. (In this case, evaluation set is required for observation.) A combination of methods above. Adopt models like Random Forest. Any method which is believed to be able to control the model’s complexity. E.g., control the depth, number of trees. Selection Bewteen the L1 and L2 Regularization Term L1 - LASSO (Least Absolute Shrinkage and Selection Operator): cast penalty on the sum of the absolute value of the model parameters.Regularize all the parameters equally. Able to transform some parameters into 0. (make the model sparse)$${L1}_{reg} &#x3D; \\lambda \\sum_{j&#x3D;1}^p |\\beta _j|$$ L2 - Ridge Regression: cast penalty on the sum of the square value of the model parameters.$${L2}_{reg} &#x3D; \\lambda \\sum_{j&#x3D;1}^p \\beta _j^2$$ In essence, these two regularization method is to conduct L1 &#x2F; L2 Normalization on the model parameters and add the normalized term to the Loss Function for optimization.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"General Knowledge","slug":"Data-Science/General-Knowledge","permalink":"https://umiao.github.io/categories/Data-Science/General-Knowledge/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://umiao.github.io/tags/Machine-Learning/"}]},{"title":"SQL-Study-Note-11 User and Privilege Management","slug":"SQL-Study-Note-11","date":"2022-04-21T19:39:39.000Z","updated":"2022-04-21T22:06:33.950Z","comments":true,"path":"2022/04/21/SQL-Study-Note-11/","link":"","permalink":"https://umiao.github.io/2022/04/21/SQL-Study-Note-11/","excerpt":"Most data science practitioners would not be granted the privilege of managing the database system (not even the privilege to update &#x2F; delete), so…","text":"Most data science practitioners would not be granted the privilege of managing the database system (not even the privilege to update &#x2F; delete), so… Create and Manage User1234567891011121314CREATE USER join@&#x27;%.google.com&#x27; IDENTIFIED BY &#x27;1234&#x27;;-- You can restrict the domain of user with this method.-- You can also specify an ip after &#x27;@&#x27;.-- &#x27;1234&#x27; Stands for the password.SELECT * FROM mysql.USER;-- Retrieve the information of all the users.-- It is also supported to use GUI interface to manage the user,\\-- their host to log in with, etcDROP USER bob@gmail.com;-- Drop a user.SET PASSWORD (john) = &#x27;1234&#x27;;-- Reset a user&#x27;s password.-- It is also supported to EXPIRE PASSWORD for one user.-- So that he would be required to change the password by next log in. Privilege Management12345678GRANT SELECT, INSERT, UPDATE, DELETE, EXECUTE ON sql_store.* TO user_a;-- An example of granting the privileges.SHOW GRANTS;-- Show all the grantsREVOKE privilege .. ;-- Revoke specified granted privilege. Refer documentation for more related instructions.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"SQL-Study-Note-10 Index","slug":"SQL-Study-Note-10","date":"2022-04-21T17:53:59.000Z","updated":"2022-04-21T19:55:39.737Z","comments":true,"path":"2022/04/21/SQL-Study-Note-10/","link":"","permalink":"https://umiao.github.io/2022/04/21/SQL-Study-Note-10/","excerpt":"Index can be used to find the row (line) numbers corresponding to the value being queried. Index is added to certain columns and is stored in memory (RAM) for most times.","text":"Index can be used to find the row (line) numbers corresponding to the value being queried. Index is added to certain columns and is stored in memory (RAM) for most times. Create IndexIndex can speed up the query, however, it would also increase the size (memory comsuption) of database as well as the cost of maintenance. It is usually implemented by binary tree in database systems. 12CREATE INDEX idx_state ON customers (state);-- Specify a column of a table to create an index. Explain and ANALYZE1EXPLAIN SELECT * FROM ... Add EXPLAIN before a sql query would get explanatory information instead of the query result. E.g., which information is used and how many records are went through, for performance evaluation. 123456SHOW INDEXES IN customers;-- Reveal all the indexes in the customers table.-- You can find out the indexes added, the cardinality and name.ANALYZE TABLE customers;-- Indexes includes primary index, secondary index, etc.-- You can use the ANALYZE command to view the statistics and values of a table Different Index Types prefix indexYou should create prefix index, instead of index on the entire column, for acceleration.12345CREATE INDEX idx_n ON customers (last_name(20));-- In this case, the last_name column would be grouped and indexed according to the first 20 characters only.COUNT (DISTINCT LEFT(last_name, 10));-- You can use this way to analyze the performance of creating prefix index on the first 10 characters. -- If the number of distinct values is large enough, then this prefix is able to differentiate the possible values. Full-Text IndexThe idea of such index is similar to the implementation principles of Search Engines. For all the non-stop words, record the corresponding passages (rows) and the position where they appear.1234567CREATE FULLTEXT INDEX idex_t_body ON posts (title, body);-- FULLTEXT INDEX can monitor multiple columns.SELECT * FROM posts WHERE MATCH(title, body) AGAINST (&#x27;react redux&#x27;);-- Search &#x27;react redux&#x27; in the two columns: title and body. They are viewed as TWO words.MATCH(title, body) AGAINST (&#x27;react -redux +form&#x27; IN BOOLEAN MODE);-- It is possible to exclude some words by adding a &#x27;-&#x27;.-- So the above query matches contents including react/form and without redux. Composite indexesEnable indexing on multiple columns in order to solve the issue that too many results are returned after filtering with the primary index. You can use the appearing order of the columns to decide the filtering priority of the Composite indexes.1CREATE INDEX idx_n ON customers (last_name, state, points); MySQL supports Composite index to include at most 16 columns. However, including 4-6 columns is fairly enough in practice. At the same time, Composite index supports sorting on the columns so that the more frequently used columns appear ahead. In fact, you can decide the priority of the indexes being used, by changing the appearing order of the conditions of the WHERE clause. You can use the command of: 1USE INDEX idx_name; to force MySQL to use certain index, even it is not optimal. You can NOT make full use of the index, if column is included in the sql expression (e.g. in WHERE condtion). So extract the required column first before conducting transformation. Sorting and PerformanceSorting should be avoided as possibleIt is because it is costy. You should utilize Indexes for the purpose of sorting, as possible. 1234SHOW STATUS;-- Show the variables being used in MySQL server.last_query_cost;-- You can measure the cost of last query by this mean. It should be noted that, for column a and b, ORDER BY clause can use the Index for sorting if the order is among one of these: ORDER BY a ORDER BY b ORDER BY a, b ORDER BY a DESC, b DESCAny other order would introduce external sorting operation (which means Scaning the Entire Table!).There is an exception that if WHERE clause can locate to a Single Column, then external sorting would not happen. If you can achieve the result of query solely rely on the index, then it is a Overlay Index. Duplicate index and Redundant Index Duplicate Indexes: Repeatly create the same index, e.g. on columns (a, b, c). Redundant Indexes: One index’s functionality is completely covered by the other. E.g., creating index for column (a) and (a, b) at the same time. (The latter can cover the prior)","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"SQL-Study-Note-9 Data Modeling, Constraint and Normalization Form","slug":"SQL-Study-Note-9","date":"2022-04-20T22:22:22.000Z","updated":"2022-04-21T17:53:30.704Z","comments":true,"path":"2022/04/20/SQL-Study-Note-9/","link":"","permalink":"https://umiao.github.io/2022/04/20/SQL-Study-Note-9/","excerpt":"Data Modelling Pipeline Understand the requirements; Build a conceptual Model; Build a logical model; Build a physical model.","text":"Data Modelling Pipeline Understand the requirements; Build a conceptual Model; Build a logical model; Build a physical model. Foreign Key ConstraintAlthough modify the primary key IS NOT recommended, we would consider the update to the foreign key caused by the primary key anyway. Option (strategy of updating): restrict: restrict modification cascade: update the foreign keys according to the primary key set null: set the corresponding foreign key in the foreign table into NULL no action: reject the update It is highly not recommended to use set null, as it would result in organ record in the corresponding tables (no idea which id it belongs to). Dataset NormalizationNF-1 (First Normal Form):Each record unit (element specified by row and column index) should contain a single value only and contain NO duplicate column.E.g., if you want to add tags to the Course table, you should extract tags into an independent table (and use id mapping to retrieve the tags) for it to be extendable. NF-2 (Second Normal Form):Frist is to satisfy NF-1. Also, every non candidate-key attribute depends on the whole candidate keys (that is to say, they must not depend on a true subset of the candidate keys). That is to say, each table should contain exactly one entity category only. E.g., a table stores course information should NOT contain information like the enroll time of each student. If there is an attribute which does not belong to the entity represented by this table, create a new table to store it. NF-3 (Third Normal Form):Frist is to satisfy NF-2.Also, all the attributes of the table should be determined by candidate key (for example, id) and should NOT be determined by the other non-primary attributes.That is to say, all the columns of the table should NOT be generated &#x2F; derived by the other columns, to avoid errors caused by duplicate storage and erroneous update. Data Model $ \\leftrightarrow$ TableIn MySQL, you can use forward engineer to convert data model into actual tables. Its essence is to generate sql script for database &#x2F; table generation, with the specified data model &#x2F; structual graph.The script would be like: 123CREATE schema IF NOT EXISTS … USE schema … CREATE TABLE … It is also supported to regenerate and fix the table via modifying the data model. In this case, you should select synchronize model instead of forward engineer. For those table with foreign keys, if you want to update the table, foreign keys would prevent you from doing so (as a constraint). You should first drop all the foreign keys and then reconstruct it to link to correlated tables. Reverse EngineerIt is also supported to generate data model (graphs) from existing tables. That is reverse engineering.It is highly recommended that you only put ONE database into a single data model, unless these databases are really highly correlated. Data Management Opertion (Via SQL Script)Create of Database: 12CREATE DATABASE IF NOT EXISTS name;-- Make well use of EXISTS clause to avoid error. Create Table:1234567CREATE TABLE cus ( c_id, INT PRIMARY KEY AUTO_INCREMENT, first_name VARCHAR(50) NOT NULL, points INT NOT NULL DEFAULT 0, email VARCHAR(250) NOT NULL UNIQUE ) Use Alter Table to update Table:1234567ALTER TABLE customers ADD last_name VARCHAR(50) NOT NULL AFTER first_name-- You can also use MODIFY / DROP instead of ADD to edit and delete the existing and known columns. Add Constraints (e.g., Foreign Key):12345FOREIGN KEY fk_col_name (c_id 表中列名) REFERENCES customers(c_id) ON UPDATE NO ACTION ON DELETE NO ACTION -- You cannot drop a table without droping its foreign key constraints in advance. Charset1234567SHOW CHARSET; -- You can use this command to show the charset.CREATE / ALTER DATABASE db_name CHARACTER SET lain1;-- About modifying / altering the charset (for a dataset).CREATE / ALTER TABLE () CHARACTER SET latin1;-- You can set the charset in the table level, too.-- Also, charset can be set in column level, just like adding constriants like &#x27;NOT NULL&#x27; Database Engine &#x2F; Storage Engine1234SHOW ENGINES; -- Show all the engines.ALTER TABLE customers ENGINE = InnoDB;-- Specify the engine for a table.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"Missing Value Imputation in Traffic Data","slug":"Missing-Value-Imputation-in-Traffic-Data","date":"2022-04-19T21:52:23.000Z","updated":"2022-05-05T23:35:56.102Z","comments":true,"path":"2022/04/19/Missing-Value-Imputation-in-Traffic-Data/","link":"","permalink":"https://umiao.github.io/2022/04/19/Missing-Value-Imputation-in-Traffic-Data/","excerpt":"Lost of sensor-generated data can be very common. The methods of imputation can be coarsely categorized into: 1. Prediction methods; 2. Interpolation methods; 3. Statistical Learning methods.","text":"Lost of sensor-generated data can be very common. The methods of imputation can be coarsely categorized into: 1. Prediction methods; 2. Interpolation methods; 3. Statistical Learning methods. Imputation problem &amp; Model Formulation1Let $Y_c$ be the traffic dataset persists for $N$ consecutive days,$$ Y_c &#x3D; [Y(1), …, Y(N)] $$in which the ith $Y(i)$ be noted as 1-D vector$$ Y(i) &#x3D; [y_i(1), …, y_i(D)]^T, i \\in [1, N] $$.Concatenate all the vectors we have together, we would have:$$ Y_{series} &#x3D; [y(1), …, y(D \\times N)]^T $$. Typical traffic data includes the speed and number of vehicles on a certain lane at a time. These data can form a numerical time sequence. Such data can be collected by sensor installed on the roads or along the roadsides. ARIMA-based method2ARIMA stands for Autoregressive Integrated Moving Average. In ARIMA(p, d, q), p denotes the order of the autoregressive part, d is the degree of differencing and q is the order of moving average part: $$ (1 - \\sum_{i&#x3D;1}^p \\alpha_i L^i) (1 - L)^d y(t) &#x3D; (1 + \\sum_{i&#x3D;1}^q \\beta_i L^i) y(t) \\xi(t) $$ and L is the backshift operator, $L_y(t) &#x3D; y(t- 1)$. $\\xi(t)$ is white Gaussian noise. First train this model with the known series and impute missing data one by one. The imputed data would be used as known data for next prediction. We use Akaike information criterion to determine $p$ and $q$. $d$ is suggested to be set to $1$. BNs-based imputation method3BN stands for Bayesian Netowrk. Based on the known dataset, learn the distribution model of multivariable variants $Y_{mv}(t) &#x3D; [y(t-m), …, y(t)]^T$.Assume this learning target as Gaussian mixture model (GMM).Use split and merge expectation maximisation algorithm to determine the model parameters. With a learnt GMM, missing data of $y(t)$ can be estimated as the expectation foregoing value from the latest $m$ items, as: $$ \\hat y(t) &#x3D; E[y(t) | y(t-m), …, y(t-1)] $$ k-NN based imputation method4Weighted k-NN is a non-parametric estimation method. Selection S-step (Selection Step):Use a metric to find $k$ nearest traffic daily flow vectors to the corrupted vector $Y(i)$ in pattern-similar from the dataset $Y_c$.Metrics can be Euclidean distance and Pearson correlation, for examples. Then, the lost entry &#x2F; dimension of $Y(i)$ can be imputed with the mean value of the (entries of the) $k$ vectors. Imputation I-step: (Imputation Step)Because our k-NN algorithm is weighted, then we need to find the weighted average of the k entries (averaged by the correlation coefficent given by the selected metric). Grid search and other optimization methods may be applied to determine best $k$. LLS-based imputation method5Selection S-step (Selection Step):Exactly the same as the above k-NN method. Imputation I-step: (Imputation Step)Decompose k selected vectors dataset into matrix $A$ and $B$. The dimension should be corresponding to the missing part of $Y_{mis}(i)$ and observed part $Y_{obs}(i)$ of $Y(i)$. We would have $$ \\hat Y_{mis}(i) &#x3D; B((A^TA)^{-1}A^T Y_{obs}(i)) $$. This is just a pseudo-inverse, $A$ should be full-ranked&#x2F; MCMC-based imputation method 6, 7First assume the entire data sequence $Y$ follows a certain distribution, e.g., Gaussian distribution.The conditional expectation $E[Y_{mis}|Y_{obs}, \\Phi]$ would be approximated by MCMC(Markov chain Monte Carlo) with DA(Data Augmentation), since the expectation is hard to be solved precisely due to its high dimension. Here $\\Phi$ stands for the parameter of the selected distribution. The MCMC with DA is a special case of Gibbs sampler described as follows: Imputation I-step: (Imputation Step)Given a current estimated model parameter $\\Phi ^k$, this step uses the conditional probability $Y_{mis}^{k+1}&#x3D;p(Y_{mis}|Y_{obs}, \\Phi)$ to simulate missing values for each observation independently. Posterior P-step:Use $p(\\Phi | Y_{obs}, Y_{mis}^{k+1})$ to update model parameter $\\Phi$. In this manner, a Markov chain of $(Y_{mis}^{1},\\Phi ^{1} )$, …, $(Y_{mis}^{N},\\Phi ^{N} )$ should be constructed. The missing data is estimated as$$ \\hat Y_{mis} &#x3D; \\frac{1}{N_{sample} - N_{burn-in}} \\sum_{t &#x3D; N_{burn-in + 1}}^{N_{sample}} Y_{mis}^t $$. The first $N_{burn-in}$ samples would be discarded. One feasible parameter setting is $N_{sample}&#x3D;1500$ and $N_{burn-in}&#x3D;500$. I believe the introduction of burn-in is to allow the model sometime to converge. PPCA-based imputation method8PPCA stands for Probabilistic Principal Component Analysis. It assumes that the observed data depends on latent varaiables $$ Y &#x3D; Wx + \\mu + \\epsilon $$ where $Y$ is a D-dimensional vector of observed data, $x$ is a q-dimensional latent varaible defined Gaussian distribution and $\\epsilon$ is isotropic noise. $x \\sim N(0,1)$, $\\epsilon \\sim N(0, \\sigma ^2 I)$ and $\\mu$ stands for a base mean value. Use Expectation Maximisation (EM) method to find a set of imputed data which best fit the above distribution. Concrete steps of EM method: Expectation E-step:Find out the expectation of completed log-likelihood function with previous estimated parameters $\\Phi ^k$ and observed data part $Y_{obs}$:$$ Q(\\Phi | \\Phi ^k) &#x3D; E_{X, Y_{mis} | Y_{obs}, \\Phi ^k}[log p_c (Y_c, X|\\Phi ^k)] $$We can use this to update our guess of the missing data part $Y_{mis}^k$ and latent data $X^k$. Maximisation M-step:Computiong parameter space $\\Phi$ by maximising the expectation of log-likelihood in E-step:$$ \\Phi ^{k+1} &#x3D; \\arg \\max_{\\Phi} Q(\\Phi | \\Phi ^k) $$ Conditional expectation $E[Y_{mis} | Y_{obs}, \\Phi]$ can be difficult to calculate and can approximate with MCMC with DA. Dataset Intended to Use: ComparisonPrediction and interpolation methods mentioned cannot capture stochastic variations in daily traffic flow. On the contrary, statistical learning methods could achieve traffic flow information by emphasising the statistical characteristics of traffic flow. Shorting coming of such methods: Cannot handle the situations in which neighbour (data points) DO NOT even exist. References [1] [Li, Yuebiao, Zhiheng Li, and Li Li]”Missing traffic data: comparison of imputation methods.” IET Intelligent Transport Systems 8.1 (2014): 51-57. [2] [Ahmed, Mohammed S., and Allen R. Cook]Analysis of freeway traffic time-series data by using Box-Jenkins techniques. No. 722. 1979. [3] [Ueda, Naonori, et al.] “Split and merge EM algorithm for improving Gaussian mixture density estimates.” Journal of VLSI signal processing systems for signal, image and video technology 26.1 (2000): 133-140. [4] [Troyanskaya, Olga, et al.] “Missing value estimation methods for DNA microarrays.” Bioinformatics 17.6 (2001): 520-525. [5] [Kim, Hyunsoo, Gene H. Golub, and Haesun Park.] “Missing value estimation for DNA microarray gene expression data: local least squares imputation.” Bioinformatics 21.2 (2005): 187-198. [6] [Ni, D., Leonard II, J.D.] “Markov chain Monte Carlo multiple imputation using Bayesian networks for incomplete intelligent transportation systems data”, Transp. Res. Rec., 2005, 1935, (1), pp. 57–67 [7] [Gilks, W.R., Richardson, S., Spiegelhalter, D.J.]”Markov chain Monte Carlo in practice” (Chapman &amp; Hall, London, 1996) [8] [Tipping, M.E., Bishop, C.M.]”Mixtures of probabilistic principalcomponent analyzers”, Neural Comput., 1999, 11, (2), pp. 443–482","categories":[{"name":"UCLA","slug":"UCLA","permalink":"https://umiao.github.io/categories/UCLA/"},{"name":"Course Study","slug":"UCLA/Course-Study","permalink":"https://umiao.github.io/categories/UCLA/Course-Study/"},{"name":"ECE209 in 2022 spring","slug":"UCLA/Course-Study/ECE209-in-2022-spring","permalink":"https://umiao.github.io/categories/UCLA/Course-Study/ECE209-in-2022-spring/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"UCLA","slug":"UCLA","permalink":"https://umiao.github.io/tags/UCLA/"}]},{"title":"SQL-Study-Note-8 - Data Type of MySQL","slug":"SQL-Study-Note-8","date":"2022-04-17T05:47:13.000Z","updated":"2022-04-20T22:19:31.193Z","comments":true,"path":"2022/04/16/SQL-Study-Note-8/","link":"","permalink":"https://umiao.github.io/2022/04/16/SQL-Study-Note-8/","excerpt":"Data Type of MySQL Suggestion for Data Type Selection","text":"Data Type of MySQL Suggestion for Data Type Selection VARCHAR: For short string set to 50, for long string set to 255. Maximum length is 65535, 64KB. (Note that Char Type is fix-lengthed). MEDIUMTEXT: 16M document &#x2F; LONGTEXT: 4GB document &#x2F; TINYTEXT: 255 Bytes &#x2F; TEXT: equal to VARCHAR, 64KB Note that Chinese charcter takes 3 Byte each, so we allocate $3n$ Bytes to string with length of $n$ (pick the upper bound). Integer Type: TINYINT: 1 Byte, UNSIGNED TINYINT and SMALLINT : 2 Byte, MEDIUMINT: 3 Bytes, INT: 4 Bytes, BIGINT: 8 Bytes. Leading zero filling is supported: INT(4) -&gt; ‘0003’ Float Type: DECIMAL(p,s) defines the int length and decimal length, it can be viewed as a fixed-point decimal (int). DECIMAL &#x3D; DEC &#x3D; NUMERIC &#x3D; FIXED FLOAT: 4 Bytes, DOUBLE: 8 Bytes (Expressed in exponential form) Boolean: BOOL &#x2F; BOOLEAN, 1 bit Enumerate Type: ENUM(‘a’, ‘b’, ‘c’): value must be selected from the given set. This is not a good design as it is complex to change the domain of legal values, you may even need to rebuild the entire table. It is not reusable itself. Creating a table to store the mapping relationship is recommended. Time: Timestamp can only store date up to 2038 AD as it takes 4 Bytes. To store later time, use Datastamp. BLOB for Binary Large Object: TINYBLOB &#x2F; BLOB &#x2F; MEDIUMBLOB &#x2F; LONGBLOB takes 255 Bytes &#x2F; 65KB &#x2F; 16 MB &#x2F; 4 GB, respectively. Store files in the FileSystem as possible rather than store them in the database. Otherwise, you may come into problems like high memory usage, slow copying, low performance, indirect and complicated IO, etc. JSON: In order to set JSON object, you can use string form like: ‘{ “k”:v}’. You can also create the object with function:.1JSON_OBJECT(&#x27;weight&#x27;, 10, &#x27;dimensions&#x27;, JSON_ARRAY(1, 2, 3)); In order to extract the attributes included in JSON, you can use 12345678JSON_EXTRACT(properties, ‘$.weight’);-- while the properties stand for the desired column name / key of the JSON object. JSON_EXTRACT(properties, ‘$.weight.data.sub.time’);-- You can use multiple dots to get the nested attributes.properties -&gt; ‘$.weight’; -- Semi-CPP syntax is also supportedproperties -&gt; ‘$.weight[idx]’;-- Can specify a certain element of the JSON list use &#x27;[]&#x27; At the same time, it should be noted that the returned results are still in JSON format. 1234properties -&gt; ‘$.weight’;-- would return something like &quot;sony&quot;, which leads to problem when comparing with other resultsproperties -&gt;&gt; ‘$.weight’;-- is able to return sony, without &quot;&quot; In terms of updating partial attributes, you can use JSON_SET. Here SET stands for the motion of setting. 123SET properties = JSON_SET / JSON_REMOVE(properties, ‘$.weight’, 30, ‘$.age’, 10) WHERE id=1-- JSON_REMOVE is used to remove attributes The above example can be used to set part of the attributes in JSON object properties.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"SQL-Study-Note-7 - Transactions","slug":"SQL-Study-Note-7","date":"2022-04-17T05:07:50.000Z","updated":"2022-04-17T05:47:05.601Z","comments":true,"path":"2022/04/16/SQL-Study-Note-7/","link":"","permalink":"https://umiao.github.io/2022/04/16/SQL-Study-Note-7/","excerpt":"Transactions Principles of Transaction (ACID) Atomicity Consistency Isolation Durability","text":"Transactions Principles of Transaction (ACID) Atomicity Consistency Isolation Durability 12345START TRANSACTION;// Instruction block to be executed-- If only part of the transaction is done and the connection to server is lost, the finished part would be rolled backCOMMIT; A transaction would lock the lines and tables to be updated so that they are untouchable to other transactions.If one transaction comes into locked resources, it would wait the owner of the lock to finish, or until it expires the time limit itself.At the same time, ROLLBACK is also a SQL instruction and keyword. Different level of transaction isolation: 12SET TRANSACTION ISOLATION LEVEL SERIALIZABLE;-- Note that this setting is session-leveled. It comes with the tradeoff that the higher the isolation level is, the lower performance it reaches. (In extreme situation, serializable level would not benefit from distributed architecture.) Read Uncommitted: May read uncommited data (dirty read). Rarely used in actual application as the performance improvement is very limited. Read Committed: Default level for most DBMS (Oracle, SQL server). However, it may counter Nonrepeatable Read: same select may have different result within one transaction. This is due to the UPDATE operation made by other transactions and can be solved by Line Level Lock. Repeatable Read: Applied with Line Level Lock. However, it can still encounter Phantom Reads (caused by the delete &#x2F; insert operations made by other transactions). Require Table Level Lock to solve. Serializable: Ban the parallel processing and sort all the transactions.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"SQL-Study-Note-6 - Trigger and Events","slug":"SQL-Study-Note-6","date":"2022-04-17T04:46:07.000Z","updated":"2022-04-17T05:05:21.886Z","comments":true,"path":"2022/04/16/SQL-Study-Note-6/","link":"","permalink":"https://umiao.github.io/2022/04/16/SQL-Study-Note-6/","excerpt":"TriggerTriggers are the code blocks executed automatically before insertion &#x2F; update &#x2F; delete take effect.","text":"TriggerTriggers are the code blocks executed automatically before insertion &#x2F; update &#x2F; delete take effect. 123456789DELIMITER $$CREATE TRIGGER payments_after_insert AFTER/BEFORE INSERT/UPDATE/DELETE ON payments FOR EACH ROWBEGIN -- You can either write SQL codes here or call the existing procedure...END $$DELIMITER ; Extensive syntax123NEW -- Return the just inserted line.OLD -- Return the just deleted line.NEW.amount -- Can use . to specify a column These two (NEW &#x2F; OLD) are keywords of MySQL.Triggers can be used to modify data of any table EXCEPT the table which is being listened by the trigger. This is because trigger can trigger itself and results in infinite loop. 12SHOW TRIGGERS -- Show all the created triggers.ShOW TRIGGERS LIKE &#x27;c%&#x27; -- Filter the triggers. Triggers can be also used for auditing purpose, i.e., record the executor, attribute and timestamp when an operation is done. EVENTEvents are periodically triggered codes for multiple tasks. 12SHOW VARIABLES -- Show all system variables.SET GLOBAL event_scheduler=ON / OFF -- Switch the event_scheduler Create Event: 12345678DELIMITER $$CREATE EVENT yearly_delete_state_audit_rows ON SCHEDULEEVERY 1 YEAR STARTS ‘2020-01-01’ ENDS ’2029-01-01’DO BEGIN -- Note that DO is required here....END $$DELIMITER ; Two ways to calculate the diff of time: 12NOW() – INTERVAL 1 YEAR;DATESUB(NOW(), INTERVAL 1 YEAR); Activate &#x2F; Deactivate Events: 1ALTER EVENT e_name DISABLE;","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"SQL-Study-Note-5 - Stored Procedure and User Defined Functions","slug":"SQL-Study-Note-5","date":"2022-04-17T02:01:32.000Z","updated":"2022-04-17T04:48:52.424Z","comments":true,"path":"2022/04/16/SQL-Study-Note-5/","link":"","permalink":"https://umiao.github.io/2022/04/16/SQL-Study-Note-5/","excerpt":"Stored ProcedureMotivationGenerally, developers prefer not to interpret string as SQL codes &#x2F; instructions due to security concerns.","text":"Stored ProcedureMotivationGenerally, developers prefer not to interpret string as SQL codes &#x2F; instructions due to security concerns. You can wrap the query and update functionality with Stored Procedure. DBMS is able to further optimize the stored procedure and enhance the security. The stored procedure itself is similar to a function implementation. Syntax Implementation123456DELIMITER $$CREATE PROCEDURE get_clients()BEGIN SELECT * FROM clients;END $$DELIMITER ; The reason of changing DELIMITER is that, we need to use ; to seperate SQL statements within the stored procedure. (We are forced to do so!)This will damage the integrity of our BEGIN-END statement block. Thus, we should temporarily change the DELIMITER (into $$) and change back to ; after the definition of our stored procedure. In order to avoid conflict of naming, MySQL would add a pair of &#96; to the names of databases, tables and columns. 1Student -&gt; `Student` -- Auto-Renamed to avoid conflicts. You can use 1CALL procedure_name() to call the defined stored procedure. If you are using the workbench of MySQL, you can simply right click Store Procedures to create one. In this case you do not need to worry about the delimiter and MySQL would help you with the transformation (Some sort of Syntactic sugar). Delete Stored Procedure1DROP PROCEDURE (IF EXISTS) get_clients; Parameter Setting of Stored Procedure12CREATE PROCEDURE get_clinets ( state CHAR(2) );-- You must specify the size and type of the passed parameter. After setting the parameter declaration, the parameters can be then used in the procedure for program logic building.All the parameters are REQUIRED. However, they can have default values.Even if you want to use the default value, you should pass a NULL to the procedure as a placeholder. 123456IF state IS NULL THEN SET state = ‘CA’; END IF;-- By using such statement, you can realize default value de facto.table.col_name = IFNULL(para, table.col_name);-- This one is more concise and thus recommended.-- If para is NULL, then use the default value. Parameter Verification and Constraints12345IF payment_amount &lt;= 0 THEN SIGNAL SQLSTATE &#x27;22003&#x27; SET MESSAGE_TEXT = ‘Invalid payment amount’;END IF-- payment_amount is required to &gt; 0. If not satisfied, a error code of &#x27;22003&#x27; is raised and the prompting MESSAGE_TEXT is written. SIGNAL is similar to throw exception in other languages. SQLSTATE is the predetermined error code (corresponding to ‘out of range’ here, refer the documentation). Provide Return Value for Stored Procedure1234CREATE PROCEDURE `name` (client_id INT, OUT invoices_count INT)BEGIN SELECT COUNT(*) INTO invoices_count FROM invoices;END This is a syntactic sugar anyway. What MySQL actually does is define two parameters and pass them to the PROCEDURE for updating. Then SELECT the updated parameter.At the same time, MySQL uses a ‘@’ prefix to identify variables. For an example, 1SET @a = 0; User Variable and Local VariableSET is used to assign the User Variable while DECLARE is used to assign the Local Variable used within a stored procedure. 1DECLARE risk_factor DECIMAL(9, 2) DEFAULT 0; You can assign the declared variable with the following code: 1SELECT COUNT(*) INTO risk_factor FROM table; User-Defined FunctionsThe only difference between the procedure and function is that function can only return one single value. 12345678CREATE FUNCTION get_risk_f ( client_id INT) -- Must specify the type for the input parameterRETURNS INTEGER -- The return parameter must have a type too. Function always returns a single value rather than a query result.DETERMINISTIC -- Optional attribute. Always return the same result for the same id.READS SQL DATA -- Optional. This function can read SQLMODIFIES SQL DATA -- Optional. This function can modify table.BEGIN...RETURN 1; Functions can be deleted with DROP keyword as well.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"SQL-Study-Note-4 - View","slug":"SQL-Study-Note-4","date":"2022-04-17T01:39:43.000Z","updated":"2022-04-17T02:00:31.084Z","comments":true,"path":"2022/04/16/SQL-Study-Note-4/","link":"","permalink":"https://umiao.github.io/2022/04/16/SQL-Study-Note-4/","excerpt":"ViewIntroduction to ViewWith the introduction of View, middle &#x2F; query results can be stored for further query and use, just like a real table.","text":"ViewIntroduction to ViewWith the introduction of View, middle &#x2F; query results can be stored for further query and use, just like a real table. Creation and Manipulation on View Creation of View: 1CREATE VIEW view_name AS (SELECT …) Created Views would NOT be stored with the tables. It would be stored in ‘Views’ instead. Alter &#x2F; Drop of View: 1234DROP VIEW sales_by_client;-- Drop / delete Operation.CREATE / REPLACE VIEW AS ...;-- REPLACE has the advantage against CREATE that it does not require the view to be dropped in advance. Created Views would NOT be stored with the tables. It would be stored in ‘Views’ instead. In fact, the Views should be viewed as stored query code. When you need to use them, you can simply rerun the script to retrieve the result. So you can use version control tools to store and share them. Updatable Views: Updatable Views stand for those Views who DO NOT contain the keywords of DISTINCT &#x2F; aggregate functions &#x2F; GROUP BY &#x2F; HAVING &#x2F; UNION. In this case, these Views can be updated via CREATE &#x2F; REPLACE. Update Opertion: 1UPDATE view_name SET due_date = DATE_ADD(due_date, INTERVAL 2 DAY) WHERE invoice_id = 1l Point of updatable views: If you DO NOT have the authorization to modify a table, you can still create a View based on that table and update the View, as long as it is Updatable. WITH CHECK OPTION: If the UPDATE operation may cause some rows to be deleted, you can add WITH CHECK OPTION to the end of the UPDATE code to prevent this from happening.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"SQL Study Note - 3 - Function and the Aggregate Function","slug":"SQL-Study-Note-3","date":"2022-04-17T01:34:31.000Z","updated":"2022-04-17T01:48:08.598Z","comments":true,"path":"2022/04/16/SQL-Study-Note-3/","link":"","permalink":"https://umiao.github.io/2022/04/16/SQL-Study-Note-3/","excerpt":"Function and the Aggregate Function","text":"Function and the Aggregate Function Aggregate Function: 1COUNT(), MAX(), MIN(), AVG(), SUM() It should be noted that COUNT(row_name) only returns the number of the non-empty records. If you need to find out the total number of rows, you shoud use COUNT(*). You can use COUNT (DISTINCT client_id) to find out the number of unique client_id, too. Non-Aggregate Function: 1234RAND() -- Generate a random number within (0, 1)RAND(seed) -- Specify the rand seedSQRT() -- Find the square root for each valueCONCAT(a, b) -- Concat two strings into one Non-aggregate function would return a same-lengthed sequence for the input values. These functions are element-wised. GROUP BY clause: Group the rows according to a given column name. Rows with the same value in the very column would be aggregated together. The order of query clause: IMPORTANT: SELECT -&gt; FROM -&gt; WHERE -&gt; ORDER BY Grouping based on multiple attributes: 1GROUP BY state, city In this case, the grouping would based on the tuple: (state, city) Having: 1SELECT SUM(res) AS aggregated_res FROM t GROUP BY state HAVING aggregated_res &gt; 100; You cannot use WHERE clause to filter the result of the aggreate function, because at that time, the query is not yet completed and the aggregate result is not yet calculated. It should be noted that HAVING clause supports the filtering on Composite Condition, e.g. HAVING aggre_res1 &gt; 10 AND aggre_res2 &lt; 10. The HAVING clause is execute after the query is finished. So that it can only filter the selected columns. The columns not selected cannot be used in the filtering constraint. HAVING does NOT support alias. WITH ROLLUP: 1SELECT SUM(res) AS aggregated_res FROM t GROUP BY state HAVING aggregated_res &gt; 100; Conduct an extra aggregate operation for all the aggregated results. E.g., the SUM &#x2F; MEAN of the aggregated results. This is only supported by MySQL. If composite grouping is applied, e.g., GROUP BY col_a, col_b, each group identified by a unique (col_a, col_b) would conduct an extra aggregation. Nesting Sub-query: 12SELECT * FROM (SELECT ...);SELECT * FROM table WHERE col_name IN / NOT IN (SELECT ...); I.E., select from the result of another select operation. ALL VS ANY &#x2F; SOME 1234SELECT * FROM invoices WHERE invoice_total &gt; ALL(SELECT invoice_total FROM invoices WHERE client_id = 3);-- Require the records to be greater than ANY of the sub-query results in order to be selectedSELECT * FROM invoices WHERE invoice_total &gt; ANY / SOME(SELECT invoice_total FROM invoices WHERE client_id = 3);-- Require the records to be greater than ONE of the sub-query results in order to be selected ANY &#x2F; SOME are completely equivalent. If one value of the sub-query satisfies the logical expression, the ‘&gt; ANY’ expression is satisfied. For ALL, only if all the values of the sub-query satisfies the logical expression, the ‘&gt; ALL’ expression can be satisfied. ‘&#x3D; ANY’ also equals to ‘IN’. Correlated sub-query Code-writing Order: SELECT, FROM, WHERE, GROUP BY, HAVING, ORDER BY Executing Order: FROM, WHERE, GROUP BY, HAVING, SELECT, ORDER BY The marked line is crucial, as the logic of correlated subquery would retrive one value from the main query at a time, working in an iterative manner, input the value into the sub-query to ge the result and pass the result back to the main query. The main query would check the constraint of WHERE clause and return the result; This means that different alias are required even they points to the same table. Sub-query can touch the alias of the main &#x2F; outer query. EXISTS 1SELECT * FROM c WHERE EXISTS (SELECT id FROM I WHERE c_id = c.c_id) EXISTS keyword has advantage against the ‘IN (sub-query)’ manner. This is because the ‘IN (sub-query)’ needs to finish the sub-query first before the sub-query can return any result to the outer query. However, EXISTS can work as a short-circuiting operator which stops immediately when the first results is found. Write Sub-query in SELECT &#x2F; FROM clause Use SELECT to duplicate the result of aggregate function1SELECT (SELECT AVG() FROM t) AS average FROM t; This query makes sense because, the aggregate function would only return one single value. If you want to conduct row-wise calculation on the aggregate value, you have to duplicate it. You can also SELECT FROM a sub-query like it is a real-table. However, an alias is required. This may make the query too complicated and should be used with care. Numerical Function123456789ROUND(num, mantissas_n); -- Round the given number, and mantissas_n decimal places are preserved.TRUNCATE(num, mantissas_n); -- Truncate the given number with the decimal setting.--Simiarly, we have:CEILING()FLOOR()ABS()RAND() Other available functions can be found in the MySQL documentation. String Function1234567891011121314151617181920LENGTH(&#x27;sky&#x27;); -- Return the length of the input string.UPPER(&#x27;sky&#x27;);--transform to upper caseLOWER(&#x27;sky&#x27;);--transform to lower caseLTRIM() / RTRIM() / TRIM();-- Remove the left / right / both side of spaces.LEFT(string, n);-- Return the left n chars;RIGHT(string, n);-- Return the right n chars;SUBSTRING(string, start, n);-- Return n chars starts from start.LOCATE(&#x27;n&#x27;, &#x27;kinter&#x27;);-- Find the smallest index where the pattern (&#x27;n&#x27;) occurs in the searched string (&#x27;kinter&#x27;). Index starts from 1.REPLACE(string, source, target);-- Replace all the pattern of source to target, in string.CONCAT(a, b);-- Concat strings into 1. Refer the documentation for more string functions. Time Function1234567891011121314NOW();-- Return the current time and date.CURDATE();-- Return the current date.CURTIME():-- Return the current time.YEAR(time) / MONTH() / DAY() / HOUR() / MINUTE() / SECOND() ...-- Extract the year... of a time / date. DAYNAME(time);-- Return the name of day, like FridayMONTHNAME(time);-- Return sth like DECEMBER...EXTRACT( YEAR FROM NOW() )-- Personalize a date / time.. Year can be substitute with other keywords E.g., extract order of this year: YEAR(date) &#x3D; YEAR(NOW()); Fomat Time &#x2F; Date123456789101112DATE_FORMAT(NOW(), &#x27;%y&#x27;);-- %y for 2-digit year, and %Y for 4-digit year;-- Similar for D/d/M/m, ...-- Can be use to format time as well.SELECT DATE_ADD( NOW(), INTERVAL 1 DAY );-- Return the date of 1 day later. Accept negative value like -1.-- Can Use DATE_SUB instead and the behavior is very similar.DATEDIFF(d_1, d_2);-- Find the diff of two dates. (by days, the input accepts DATE only)-- Result can be negative, calculated by d_1 - d_2.TIME_TO_SEC(time);-- Transform time to second, starts from 12 am. IFNULL &amp; COALESCE123SELECT order_id IFNULL(shipper_id, &#x27;Not Assigned&#x27;) AS s;-- If a shipper_id is NULL, return &#x27;Not Assigned&#x27; instead.COALESCE(shipper_id, comments, ..., &#x27;Not assigned&#x27;); The COALESCE is the generalization of IFNULL. The principle is, by offering a bunch of column_name &#x2F; values, return the first one which IS NOT NULL. IF &#x2F; CASE (Conditional Statement)1234567891011IF (expression, first, second);-- If the expression is TRUE, then return first; else, return second.SELECT order_id CASE WHEN YEAR(order_date) = YEAR(NOW()) THEN ‘Active’ WHEN YEAR(order_date) = YEAR(NOW()) - 1 THEN ‘Last Year’ ELSE ‘Other cases’END AS categoryFROM orders-- The case statement is quite similar with IF statement.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"SQL Study Note - 2 - The Update / Delete / Insert Syntax","slug":"SQL-Study-Note-2","date":"2022-04-17T01:29:50.000Z","updated":"2022-04-17T01:48:03.088Z","comments":true,"path":"2022/04/16/SQL-Study-Note-2/","link":"","permalink":"https://umiao.github.io/2022/04/16/SQL-Study-Note-2/","excerpt":"The update &#x2F; delete &#x2F; insert syntax","text":"The update &#x2F; delete &#x2F; insert syntax The column attributes of table: PK： primary key NN： Not Null UQ：Unique Index B： binary UN： unsigned data type ZF： zero filled AI： Auto incremental G： Generated column Data Type in MySQL: INT (11): integer with a length of 11 VARCHAR(50): array of char with a size $\\le 50$. CHAR(50): array of char with a size $&#x3D; 50$. DEFAULT: Use the given default value to fill this column. Insert data to table: 1INSERT INTO customers VALUES (DEFAULT, ‘John’, ‘Smith’, ‘1990-01-01’, NULL, ‘address’, ‘city’, ‘CA’, DEFAULT); At the same time, MySQL allows to specify the column names to be assigned values. 1INSERT INTO customers (first_name, …, state, points) VALUES (…); In this case, you do not need to assign the values to the order (of columns) defined by the table. The number of affected rows would be returned after successful insertion. Insert Multi-rows: 1INSERT INTO shippers (name) VALUES (&#x27;S1&#x27;), (&#x27;S2&#x27;), (&#x27;S3&#x27;); LAST_INSERT_ID: Returns the most recently generated Auto Incremental ID. Enable hierarchical data insertion. I.E., find the ID of the latest inserted data, and then use the id to associate &#x2F; update other tables. This syntax feature can eliminate ambiguity, and it is also convenient to correspond a main table record to multiple sub table records. Duplicate Table: 1CREATE TABLE orders_archived AS SELECT * FROM orders; Use the selected partial &#x2F; entire data of other table to create a duplicate. However, column attributes (constraints) like PK, AI would be ignored. Batch Insertion with Select: 1INSERT INTO orders_archived SELECT * FROM orders WHERE order_date &lt; ’2019-01-01’; Column attributes (constraints) like PK, AI would be ignored as well. Truncate Table in workbench: Right click a table and select ‘truncate table’ would remove all the data (records) but would not remove the table itself. Update a single row: 1UPDATE invoices SET payment_total = 10, payment_date=’2019-03-01’ WHERE invoice_id = 1; Filter the results (single line) that meet the conditions and update the filtered results. It is also allowed to use default, arithmetic expression, etc. as the new value of the selected row. Note that even if multiple statements are selected for your filter criteria, MySQL workbench runs in the security update mode by default, allowing you to update only one row at a time. However, in other environments, there is no such problem. You can drag it to the bottom of SQL editor and choose to uncheck the safe updates option. After changing the settings, you need to reconnect for the settings to take effect. WHERE attribute IN (1,2,3) can be used to filter multiple records. Use SELECT to UPDATE (apply sub-query in UPDATE): 1UPDATE invoices SET payment_total = 10, payment_date=’2019-03-01’ WHERE client_id = ( SELECT client_id FROM clients WHERE name=’Myworks’ ); In this manner, a single logical judgement is replaced with a sub-query to update multiple values at a time. If multiple values would be returned in your sub-query, the WHERE clause should be changed to WHERE client_id IN (sub-query). You should test the sub-query before update the table. You CANNOT update the same table where you conduct your sub-query —- If you have to do so, create a duplicate and give it an alias. DELETE:1DELETE FROM invoices WHERE (invoice_id=1); Obviously, the condition within the parenthesis can be a sub-query, too. Rebuild the Database:12DROP DB If EXISTS DB;-- Conduct the script of DB building then.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"SQL Study Note - 1 - Syntax Basics","slug":"SQL-Study-Note-1","date":"2022-04-16T21:05:53.000Z","updated":"2022-04-17T01:31:33.088Z","comments":true,"path":"2022/04/16/SQL-Study-Note-1/","link":"","permalink":"https://umiao.github.io/2022/04/16/SQL-Study-Note-1/","excerpt":"Overview The core of database system is to interact with DB (DataBase) with DBMS (Database Management System).","text":"Overview The core of database system is to interact with DB (DataBase) with DBMS (Database Management System). The DB can be generally divided into: Relational DB NoSQL DB (e.g., KV (key-value) based DB) Recommended code style of SQL (Structed Query Language): Capitalize all keywords and reserved words, and lowercase all other contents. Each statement should end with ;. Keywords and Syntax RulesThe query syntax SHOW DATABASES 1SHOW DATABASES; List all the names of exisitng databases (under the current schema). USE 1234USE database_name;SELECT * from table;SELECT * from database_name.table; -- Must specify the database_name for the DBs which are not in use Select one DB as the default one. SELECT 1SELECT (column_name) FROM (table_name) WHERE (condition) ORDER BY (col_name). Select the desired data from a table. It should be noted that you should use &#x3D; in SQL to determine equivalence. (It DOES NOT mean assignment.) You can conduct calculation on the selected result. AS 1SELECT (column_name) FROM (table_name) WHERE (condition) ORDER BY (col_name). For each col_name, table_name to be queried and the queried results, you can always to set alias (surname) for them with AS. – &#x2F; comment 12-- This a comment.// This is also acceptable. DISTINCT 1SELECT DISTINCT column_name FROM t Add DISTINCT ahead of the queried target (column) to receive all the unique values. AND &#x2F; OR &#x2F; NOT 1SELECT * FROM t WHERE col_a &gt; 10 and col_b = &#x27;CA&#x27; Logical operator to be used with the WHERE clause. IN 1SELECT * FROM t WHERE col_a IN (&#x27;A&#x27;, &#x27;B&#x27;) Determine if the queried value belongs to a set. BETWEEN 123SELECT * FROM t WHERE point BETWEEN 100 AND 300;SELECT * FROM t WHERE point &gt;= 100 AND point &lt;= 300;-- These two are equivalent Determine if the queried value within a given interval. Both ends of the interval are closed ([beg, end]). LIKE12SELECT * FROM t WHERE name like &#x27;b%&#x27;-- Able to match &#x27;Bob&#x27; and &#x27;bike&#x27; Provide functionality similar to Regular Expression. % can match arbitrary string, _ can match arbitrary char. Not sensitive to the case. REGEXP1SELECT * FROM t WHERE name REGEX &#x27;^f[a-z]+d&#x27; Match a given Regular Expression pattern. IS NULL12SELECT * FROM t WHERE name IS NULL;SELECT * FROM t WHERE name IS NOT NULL; Query all the records which are (not) null in a given column. ORDER BY1SELECT * FROM t ORDER BY col_name DESC / AESC Decide the sorting order of the returned records. LIMIT12SELECT * FROM t LIMIT offset, tot_numSELECT * FROM t LIMIT tot_num Restrict the number of the returned records. Skip the first $n&#x3D;$offset records and then return tot_num lines of records. Return all the records if number of matched records fewer than tot_num. INNER JOIN123SELECT * FROM t_a JOIN t_b on t_a.col_1 = t_b.col_2;SELECT * FROM t_a JOIN t_b on t_a.col_1 = DB_2.t_b.col_2;-- You can conduct crossed-DB join by specifying the name of the DB not in use. Can be simply writtene as JOIN. Concat two tables based on the given condition. Conduct Cartesian product implicitly. SELF JOIN1SELECT * FROM t_a AS a JOIN t_a AS b on a.col_1 = b.col_2; A table can join with itself, but different alias are required. Multi-table JOIN1SELECT * FROM t_a JOIN t_b on a.col_1 = b.col_2 JOIN t_c on b.col_2 = c.col_3; Multiple (n) tables can be joined but this is not recommended when $ n &gt; 3 $ due to performance concern. Composite JOIN &#x2F; Implicit JOIN1SELECT * FROM t_a JOIN t_b on a.col_1 = b.col_2 JOIN t_c on b.col_2 = c.col_3; Sometimes only a tuple of multiple attributes can uniquely identify a row of the table. In this case, these attributes become Composite Primary Key. 1SELECT * FROM t_a, t_b; Implicit JOIN is conducted in the above example but this is not recommended, too. OUTER JOIN1SELECT * FROM t_a AS a JOIN t_b AS b on a.col_1 = b.col_2 JOIN t_c AS c on b.col_2 = c.col_3; When using INNER JOIN, some records of the left table cannot match with the right table because the condition of the ON clause is not satisfied. However, if we want to return all the records of the left (right) table regardless of the boolean value of the ON clause, we can use LEFT (RIGHT) OUTER JOIN. You should use RIGHT JOIN instead of LEFT JOIN as possible. SELF OUTER JOIN is similar, and alias is still required. USING123SELECT * FROM t_a JOIN t_b on t_a.col_1 = t_b.col_1;SELECT * FROM t_a JOIN t_b USING (col_1);-- These two are equivalent. Can be used to simplify the code, if the column names of the to-be-joined tables are exactly the same. You can Join on a tuple like ‘USING (id1, id2, id3)’ and these column names should be exactly the same as well (in the two tables). NATURAL JOIN1SELECT * FROM t_a NATURAL JOIN t_b; Let the compiler (DBMS) to decide the way of join. Not recommended to use! CROSS JOIN1SELECT * FROM t_a CROSS JOIN t_b; Conduct Cartesian Product. UNION1SELECT * FROM a UNION SELECT * FROM b; Concatenate multiple queried results together (on the direction of row). The column names should be exactly the same. IT SHOULD BE NOTED that ORDER BY can be set only once, so union all the results before setting the ORDER clause.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]}],"categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Data System","slug":"Data-Science/Data-System","permalink":"https://umiao.github.io/categories/Data-Science/Data-System/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"Software Engineering","slug":"Job-Search/Software-Engineering","permalink":"https://umiao.github.io/categories/Job-Search/Software-Engineering/"},{"name":"Research","slug":"Research","permalink":"https://umiao.github.io/categories/Research/"},{"name":"Paper Read","slug":"Research/Paper-Read","permalink":"https://umiao.github.io/categories/Research/Paper-Read/"},{"name":"Productivity","slug":"Productivity","permalink":"https://umiao.github.io/categories/Productivity/"},{"name":"Investment","slug":"Investment","permalink":"https://umiao.github.io/categories/Investment/"},{"name":"Financial Firm","slug":"Job-Search/Financial-Firm","permalink":"https://umiao.github.io/categories/Job-Search/Financial-Firm/"},{"name":"AI","slug":"AI","permalink":"https://umiao.github.io/categories/AI/"},{"name":"NLP","slug":"AI/NLP","permalink":"https://umiao.github.io/categories/AI/NLP/"},{"name":"General Knowledge","slug":"Data-Science/General-Knowledge","permalink":"https://umiao.github.io/categories/Data-Science/General-Knowledge/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"},{"name":"UCLA","slug":"UCLA","permalink":"https://umiao.github.io/categories/UCLA/"},{"name":"Course Study","slug":"UCLA/Course-Study","permalink":"https://umiao.github.io/categories/UCLA/Course-Study/"},{"name":"ECE209 in 2022 spring","slug":"UCLA/Course-Study/ECE209-in-2022-spring","permalink":"https://umiao.github.io/categories/UCLA/Course-Study/ECE209-in-2022-spring/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Cyber Security","slug":"Cyber-Security","permalink":"https://umiao.github.io/tags/Cyber-Security/"},{"name":"Data System","slug":"Data-System","permalink":"https://umiao.github.io/tags/Data-System/"},{"name":"Data Security","slug":"Data-Security","permalink":"https://umiao.github.io/tags/Data-Security/"},{"name":"Git","slug":"Git","permalink":"https://umiao.github.io/tags/Git/"},{"name":"Version Control","slug":"Version-Control","permalink":"https://umiao.github.io/tags/Version-Control/"},{"name":"Research","slug":"Research","permalink":"https://umiao.github.io/tags/Research/"},{"name":"Designing Data-Intensive-Applications","slug":"Designing-Data-Intensive-Applications","permalink":"https://umiao.github.io/tags/Designing-Data-Intensive-Applications/"},{"name":"Keyboard","slug":"Keyboard","permalink":"https://umiao.github.io/tags/Keyboard/"},{"name":"Term","slug":"Term","permalink":"https://umiao.github.io/tags/Term/"},{"name":"Linux","slug":"Linux","permalink":"https://umiao.github.io/tags/Linux/"},{"name":"Vim","slug":"Vim","permalink":"https://umiao.github.io/tags/Vim/"},{"name":"CLI","slug":"CLI","permalink":"https://umiao.github.io/tags/CLI/"},{"name":"English","slug":"English","permalink":"https://umiao.github.io/tags/English/"},{"name":"Vocabulary","slug":"Vocabulary","permalink":"https://umiao.github.io/tags/Vocabulary/"},{"name":"Word Frequency","slug":"Word-Frequency","permalink":"https://umiao.github.io/tags/Word-Frequency/"},{"name":"options","slug":"options","permalink":"https://umiao.github.io/tags/options/"},{"name":"futures","slug":"futures","permalink":"https://umiao.github.io/tags/futures/"},{"name":"trading","slug":"trading","permalink":"https://umiao.github.io/tags/trading/"},{"name":"investment","slug":"investment","permalink":"https://umiao.github.io/tags/investment/"},{"name":"Productivity","slug":"Productivity","permalink":"https://umiao.github.io/tags/Productivity/"},{"name":"Apple Watch","slug":"Apple-Watch","permalink":"https://umiao.github.io/tags/Apple-Watch/"},{"name":"Tips","slug":"Tips","permalink":"https://umiao.github.io/tags/Tips/"},{"name":"Software Engineering","slug":"Software-Engineering","permalink":"https://umiao.github.io/tags/Software-Engineering/"},{"name":"Certificate","slug":"Certificate","permalink":"https://umiao.github.io/tags/Certificate/"},{"name":"OOD","slug":"OOD","permalink":"https://umiao.github.io/tags/OOD/"},{"name":"Object Oriented Design","slug":"Object-Oriented-Design","permalink":"https://umiao.github.io/tags/Object-Oriented-Design/"},{"name":"IQ","slug":"IQ","permalink":"https://umiao.github.io/tags/IQ/"},{"name":"Brainteasers","slug":"Brainteasers","permalink":"https://umiao.github.io/tags/Brainteasers/"},{"name":"Math","slug":"Math","permalink":"https://umiao.github.io/tags/Math/"},{"name":"NLP","slug":"NLP","permalink":"https://umiao.github.io/tags/NLP/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://umiao.github.io/tags/Machine-Learning/"},{"name":"GBM","slug":"GBM","permalink":"https://umiao.github.io/tags/GBM/"},{"name":"XGBoost","slug":"XGBoost","permalink":"https://umiao.github.io/tags/XGBoost/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"},{"name":"Random Forest","slug":"Random-Forest","permalink":"https://umiao.github.io/tags/Random-Forest/"},{"name":"Regularization","slug":"Regularization","permalink":"https://umiao.github.io/tags/Regularization/"},{"name":"Naive Bayes","slug":"Naive-Bayes","permalink":"https://umiao.github.io/tags/Naive-Bayes/"},{"name":"UCLA","slug":"UCLA","permalink":"https://umiao.github.io/tags/UCLA/"},{"name":"SVM","slug":"SVM","permalink":"https://umiao.github.io/tags/SVM/"}]}