{"meta":{"title":"The blog of Blur","subtitle":"Hearing the fall of snow.","description":"Personal blog of blur.","author":"Blur - Shenghui Xu","url":"https://umiao.github.io","root":"/"},"pages":[{"title":"About Me","date":"2022-04-16T17:45:16.000Z","updated":"2022-04-21T18:32:14.752Z","comments":true,"path":"about/index.html","permalink":"https://umiao.github.io/about/index.html","excerpt":"","text":"Who am i This is Shenghui Xu and welcome to my blog. I am currently a year-one MS student with the Electrical Computer Engineering department of University of California, Los Angeles. I am now focusing on the track of Signals &amp; Systems and have a GPA of 4.0. I am also an incoming applied researcher intern at ebay. Before I went to UCLA, I was an undergraduate student majored in Computer Science with Soochow University. I graduated with honor in Jun. 2020 (with a GPA of 3.83). Skills and Tools Languages Python &#x2F; Matlab &#x2F; R &#x2F; Java &#x2F; JavaScript &#x2F; C++ Bash &#x2F; CMD &#x2F; Code Climate Frameworks TensorFlow &#x2F; Torch &#x2F; Keras &#x2F; Theano &#x2F; CUDA &#x2F; CUDNN Scikit-learn &#x2F; nltk &#x2F; Numpy &#x2F; Spark (PySpark) Qt Web HTML&#x2F;HTML5 &#x2F; CSS &#x2F; Node.js jQuery &#x2F; Vue.js &#x2F; quasar Tornado &#x2F; Chrome Dev Tools Editor &amp; IDE VIM &#x2F; Sublime Text &#x2F; Notepad++ &#x2F; Lime Text Visual Studio &#x2F; Qtcreator &#x2F; Pycharm &#x2F; Eclipse Version Control &amp; Deployment Git &#x2F; SVN &#x2F; Github &#x2F; GitLab &#x2F; Gitee &#x2F; Anaconda &#x2F; npm Testing Jenkins &#x2F; Lint &#x2F; Pytest &#x2F; Docker &#x2F; Unit Testing Data Management MySQL &#x2F; MongoDB &#x2F; Redis &#x2F; Memcached Projects Experiences"}],"posts":[{"title":"SQL-Study-Note-13 Window Function","slug":"SQL-Study-Note-13","date":"2022-05-05T21:26:39.000Z","updated":"2022-05-05T21:29:01.936Z","comments":true,"path":"2022/05/05/SQL-Study-Note-13/","link":"","permalink":"https://umiao.github.io/2022/05/05/SQL-Study-Note-13/","excerpt":"","text":"","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"SQL-Study-Note-12 Common Table Expression and Discussion on UNION","slug":"SQL-Study-Note-12","date":"2022-05-04T19:17:03.000Z","updated":"2022-05-05T23:11:11.470Z","comments":true,"path":"2022/05/04/SQL-Study-Note-12/","link":"","permalink":"https://umiao.github.io/2022/05/04/SQL-Study-Note-12/","excerpt":"Common Table Expression (CTE) is viewed as a better way to realize the functionality of subquery.","text":"Common Table Expression (CTE) is viewed as a better way to realize the functionality of subquery. Supported by MySQL &gt;&#x3D; 8.0 Generate a named temporary table, only survives during the query Comparing with subquery: CTE can be referred multiple times within one query, and is able to refer itself (in recursive manner) Syntax (of Common Table Expression)12345678WITH cte(col1, col2) AS -- Name the temporary table here, col_name is brackets(SELECT 1, 2 UNION ALL SELECT 3, 4)SELECT col1, col2 FROM cteUNION ALLSELECT * FROM cte -- Can be referred for multipletimes, subquery can be used only onceORDER BY col1 Recursively generate sequence12345678WITH RECURSIVE test as -- USE recursive keyword to call itself(SELECT 1 AS UNIONUNION ALLSELECT 1 + n FROM test -- call itselfWHERE n &lt; 10 -- break when n &gt; 10)SELECT * FROM test The script above would generate results like: 12345n12...10 Another example about querying the quest &#x2F; reply pairs recursively 12345678910111213141516171819202122232425WITH RECURSIVE replay ( quest_id, quest_title, user_id, replyid, path ) AS ( SELECT -- Select all the answers without reply quest_id, quest_title, user_id, replyid, cast( quest_id AS CHAR ( 200 ) ) AS path FROM imc_question WHERE course_id = 59 AND replyid = 0 -- 0 means that there does not exist reply UNION ALL-- search the reply / comments recursively SELECT a.quest_id, a.quest_title, a.user_id, a.replyid, CONCAT( b.path, &#x27; &gt;&gt; &#x27;, a.quest_id ) AS path FROM imc_question a -- table a stores the reply of table b JOIN replay b ON a.replyid = b.quest_id -- recursively join on table &#x27;reply&#x27;, that is this very CTE ) SELECT * FROM replay UNION V.S. UNION ALLAs discussed previously, MYSQL UNION is able to combine the results of multiple queries: 123SELECT column, ... FROM table 1UNION \\ [ALL\\]SELECT column, ... FROM table 2 In these SELECT clauses, the corresponding columns should have the same attributes (column names), and the attribute name in the FIRST appearing clause would be used as the result’s attribute name. The main difference of UNION &#x2F; UNION ALLWhen using UNION, MySQL would remove the duplicates in the query result. When using UNION ALL, MySQL would return all the results, with a higher efficiency comparing with UNION. Using ORDER BY in UNION sub clauseIf ORDER BY is used in the sub clause (of SELECT), the result of the sub clauses would first be sorted, before combined.Besides, the entire sub clause should be wrapped with brackets, including LIMIT: 123(SELECT aid,title FROM article ORDER BY aid DESC LIMIT 10) UNION ALL(SELECT bid,title FROM blog ORDER BY bid DESC LIMIT 10) Using ORDER BY in entire query with UNIONIf you want to use ORDER BY &#x2F; LIMIT to restrict or classify the combined result of UNION, you should add brackets to each single SELECT clause: 1234(SELECT aid,title FROM article) UNION ALL(SELECT bid,title FROM blog)ORDER BY aid DESC When alia is usedIf alia is used, then ORDER BY clause MUST refer the alia: 1(SELECT a AS b FROM table) UNION (SELECT ...) ORDER BY b","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"DS-Study-Note-8 Random Forest","slug":"DS-Study-Note-8","date":"2022-05-04T05:48:16.000Z","updated":"2022-05-04T17:42:41.778Z","comments":true,"path":"2022/05/03/DS-Study-Note-8/","link":"","permalink":"https://umiao.github.io/2022/05/03/DS-Study-Note-8/","excerpt":"Random Forest inherits the idea of bagging, which is part of Ensemble Learning paradigm.","text":"Random Forest inherits the idea of bagging, which is part of Ensemble Learning paradigm. Introduction to Ensemble Learning It can be simply categorized into Boosting, Bagging and Stacking. Stacking: use Logistics Regression to integrate multiple prediction results and output one single prediction. It can be viewed as a more complicated form of voting (most commonly appeared in classification tasks, take the result with most votes). Bagging and Boosting both somehow combine existing classification &#x2F; regression methods to form a stronger classifier (utilize some sort of group intelligence). The difference lies in the combining method. BaggingAlso known as Bootstrap aggregating. ProcedureThe idea is to : 1. sample $n$ data samples with bootstraping method (sample with replacement) from the dataset, to form $k$ training sample sets by repeating $k$ times. That is to say, some samples can be contained in multiple training sample sets, while some samples may not be contained by any training sample sets. 2. We can tell that the $k$ training sets are independent with each other. 3. Execute learning algorithm on the $k$ training sets to achieve $k$ models. 4. Receive the classification &#x2F; regression results by integrating the $k$ outputs of these models (with voting &#x2F; averaging). Characteristics: Highly Parallelizable. The generated models are highly independent with each other. All the models have the same significance (equally important). Representative Work: Random Forest BoostingThe idea is to combine multiple ‘weak’ classifiers to form only ONE ‘strong’ classifier to generate ONE prediction, rather generating and processing $k$ individual predictions. This method runs under the Approximately Correct (PAC) framework. This theory supports that you are bound to leverage multiple weak classifiers into a stronger one.Implementation In each round, the weight distribution of the training data would be altered. The weight of samples which are falsely classified would increase while the weight of the correctly classified samples would be reduced to accelerate the iteration &#x2F; convergence. Different ways of combining the weak classifiers: Use additive model to generate a linear combination. AdaBoost: Through the weighted majority voting method, the weight of classifiers with less error rate would be increased. Weight of classifiers with high error rate would decrease. Boosting Tree: The classifers establish series connection and each classifier fits the residual of the prior one. Use the additive sum of all the classifiers as the predicted value. Representative Work: GBDT &#x2F; XGBoost Summary on Bagging and Boosting Bagging: would generate multiple training sets with replacement. Each sample &#x2F; classifier has the same weight. Can be easily Parallelization. Boosting: would not modify the training set but the samples’ weight. The weight of each sample &#x2F; classifier would be changed according to error rate. The training of classifiers should be in sequential order. Model setup &#x2F; combination Bagging + Decision Tree &#x3D; Random Forest AdaBoost + Decision Tree &#x3D; Boosting Tree Gradient Boosting + Decision Tree &#x3D; Gradient Boosted Decision Trees (GBDT) Decision TreeHow it is built General description: Randomly sample from the dataset to train decision tree. (First select $N$ samples to train a decision tree which is to be placed at the root node) Randomly select attribute (feature) used for training. (If the data has $M$ attributes, then randomly select a subset with size of $m$) Decide the attribute used for node splitting. (Use some metirc (information gain, gini impurity) to select one attribute as the splitting attribute of this node) Repeat such process untill reach the preset boundary (Unable to split &#x2F; reach required depth). (If the selected splitting attribute is already used by the father node, it also indicates that we have reached the leaf node and no further splitting is required) Build massive Decision Trees to form a forest. Pros and ConsAdvantages Able to process data with high dimensions without dimensional reduction and feature selection. Indicates the importance and correlation between features. Resist overfitting. High training speed, easy to parallelize. Easy to implement. Able to alleviate the imbalance in dataset distribution. Able to resist missing data partially.Disadvantages Overfitting on some regression and classification tasks with high noise volumn. In favor of attributes with more possible ways of splitting (in discrete cases).With more choices of splitting &#x2F; more unique values, an attribute can cast greater impact on the Random Forest classifier, making the result unreliable.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"General Knowledge","slug":"Data-Science/General-Knowledge","permalink":"https://umiao.github.io/categories/Data-Science/General-Knowledge/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://umiao.github.io/tags/Machine-Learning/"},{"name":"Random Forest","slug":"Random-Forest","permalink":"https://umiao.github.io/tags/Random-Forest/"}]},{"title":"DS-Study-Note-7 L1 & L2 Regularization","slug":"DS-Study-Note-7","date":"2022-05-02T23:37:57.000Z","updated":"2022-05-04T05:48:01.710Z","comments":true,"path":"2022/05/02/DS-Study-Note-7/","link":"","permalink":"https://umiao.github.io/2022/05/02/DS-Study-Note-7/","excerpt":"The essence of L1 and L2 regularization (with corresponding L1 &#x2F; L2 Norm): the projection of a vector to the domain of positive real number. They can both be viewed as metrics of distance.","text":"The essence of L1 and L2 regularization (with corresponding L1 &#x2F; L2 Norm): the projection of a vector to the domain of positive real number. They can both be viewed as metrics of distance. Summary L1 normalization would make many parameters become zero (equivalent of removing these parameters) due to the property of sparsification. L2 normalization is easier to calculate and avoid the issue of discussion on the absolute value function. Only one optimal prediction exists with L2 while multiple optimal solutions may exist with L2 (bacause of the non-linear point at $0$). TheoryWhenever we apply Gradient Descend algorithm for parameter optimization, we need to find the gradient (derivative) and use the result for parameter update:$$ \\theta &#x3D; \\theta - \\alpha \\frac{\\partial}{\\partial \\theta} J(\\theta) $$Here, $\\theta$ stands for the model parameters, $\\alpha$ stands for the learning rate and $J$ stands for the objective &#x2F; loss function. The function and derivative of L1 &#x2F; L2 norm is shown in the above image. We can tell that $$ \\frac{dL_1(w)}{dw} &#x3D; sgn(w) \\\\ \\frac{dL_2(w)}{dw} &#x3D;w $$ It is easy to find that, whenever the gradient is computed and used for update, the gradient of L1 function (if not equals zero), can only be $1$ or $-1$. Thus, for some parameters, they would head towards $0$ with steady pace (this is the cause of sparsity).However, for L2 function, the gradient’s value would vanish when a certain parameter $w$ becomes closer to $0$. This means that with L2 regularization, some parameters may become close to $0$ but would never reach $0$.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"General Knowledge","slug":"Data-Science/General-Knowledge","permalink":"https://umiao.github.io/categories/Data-Science/General-Knowledge/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://umiao.github.io/tags/Machine-Learning/"},{"name":"Regularization","slug":"Regularization","permalink":"https://umiao.github.io/tags/Regularization/"}]},{"title":"DS-Study-Note-6 Naive Bayesian modeling","slug":"DS-Study-Note-6","date":"2022-05-02T16:39:44.000Z","updated":"2022-05-02T23:36:40.323Z","comments":true,"path":"2022/05/02/DS-Study-Note-6/","link":"","permalink":"https://umiao.github.io/2022/05/02/DS-Study-Note-6/","excerpt":"Naïve Bayesian Classifier: is a typical learning based method which make hypothesis on the distribution of prediction target.","text":"Naïve Bayesian Classifier: is a typical learning based method which make hypothesis on the distribution of prediction target. The discrete caseLet $X$ be the input feature vector and $Y$ be the labels, then our target is to find out the $Y$ which maximizes the conditional probability $P(Y|X)$, with given $X$.In the discrete case, we assume the conditional probabilities (of different channels of the input feature vector) are independent from each other, $$P(X^1, …, X^D|Y) &#x3D; \\prod_{d&#x3D;1}^DP(X^d|Y)$$Then, we can simply use the total probability formula to traverse the existing data to find out the desired $Y$ which corresponds to the maximal conditional probability.$$ P(Y&#x3D;k|x^1, …, x^D) &#x3D; \\frac{\\prod_{d&#x3D;1}^DP(x^d|Y&#x3D;k)P(Y&#x3D;k)}{\\sum_j\\prod_{d&#x3D;1}^DP(x^d|Y&#x3D;j)P(Y&#x3D;j)} $$ $$ Y&#x3D;{argmax}_k \\frac{\\prod_{d&#x3D;1}^DP(x^d|Y&#x3D;k)P(Y&#x3D;k)}{\\sum_j\\prod_{d&#x3D;1}^DP(x^d|Y&#x3D;j)P(Y&#x3D;j)} \\\\ &#x3D;{argmax}_k \\prod_{d&#x3D;1}^D P(x^d|Y&#x3D;k)P(Y&#x3D;k) $$ Apply to limited dataset$$ P(X^i&#x3D;v_j|Y&#x3D;k)&#x3D;\\frac{samples \\quad with \\quad X^i&#x3D;v_j \\quad and \\quad Y&#x3D;k}{samples \\quad with \\quad Y&#x3D;k} $$ For some outliers, i.e., for given input $X$, there does not exist such data (Y), you can use smoothing method for Interpolation (e.g., set a default value which equals the mean). Scaling factorYou can introduce a scaling factor $I$ to adjust the weight between the training data and the default mean value: $$ P(X^i&#x3D;v_j|Y&#x3D;k)&#x3D;\\frac{(samples \\quad with \\quad X^i&#x3D;v_j \\quad and \\quad Y&#x3D;k) + l}{(samples \\quad with \\quad Y&#x3D;k) + lM} $$$$ P(Y&#x3D;k)&#x3D;\\frac{(samples \\quad with \\quad label \\quad k) + l}{(data \\quad samples) + lK} $$Here $M$ stands for the number of unique values of $X$ (input) and $K$ stands for the number of unique values of $Y$ (output). The continuous caseWhen processing continuous functions, we need to assume the distribution of the target function, and Normal Distribution is most commonly used. Normal Distribution can be described with two parameters, the expectation (mean) $\\mu$ and the variance $\\sigma$. These two can be estimated with statistics, so we select Normal Distribution to describe the conditional probability.Of course, we use assumption not only about distribution but also about independence. We assume $P(Y|x_1, x_2)&#x3D;P(x_1|Y) \\times P(x_2|Y)$. $$ \\mu_j^i &#x3D; E[X^i|Y&#x3D;j] \\\\ \\sigma_j^{2^i} &#x3D; E[(X^i - \\mu_j^i)^2|Y&#x3D;j]$$ Generally, the essence is to determine a distribution relying on the statistics and then use the determined distribution to fit the real distribution. Pros and Cons Simple and straightforward. Provide the probabilistic distribution function Explainable &#x2F; interpretable Require domain knowledge. Require dataset for learning Performs well even the i.i.d assumption is not satisfied Using the normal distribution for modeling provides some good properties, but may be contrary to the facts Correction of distribution modeling with Gaussian distributionApplying the Gaussian &#x2F; Normal distribution can introduce good properties but may not be reasonable. E.g., the Gaussian distribution has two long tails at the left and right, and it has exactly one peak.In order to model the distribution which may have multiple peaks, you can use the sum of multiple Gaussian functions for modeling. You can still use the training data to estimate the parameters of these Gaussian distributions. EM (Expectation Maximization) Algorithm for parameter estimationIt includes two steps: the expection-step (E-step) and maximization-step (M-step). The prior estimate the parameters by observing the data and existing model, and then computes the expection of the likelihood function with the parameters. The latter find the parameters which maximizes the likelihood function. The algorithm assures that after each iteration, the value of likelihood function would increase, so that the function is bound to converge. Instance of EM algorithmGiven two coins with different distribution, we need to estimate their expection of head-up probability after flipping. However, the experiment records do not specify which coin a single record corresponds to. Thus, we need to estimate the coin an experiment record corresponds to, and the expection of getting a head after flipping at the same time. We first initialize the expection of the two coins to be different values. For each experiment record, find out the distribution of the mapping relationship $Z$, to complete a E-Step (e.g., 0.7 belongs to coin A, 0.3 belongs to coin B). Use the achieved $Z$ to assign weights to the experiment records. (View each record as a mixture of using coin A and B). Update the experiment results to: the estimate experiment result when flipping coin A &#x2F; B. (times the distribution of $Z$ with the experiment results) Base on the priciple of maximize the likelihood, use the updated experiment records to calculate the expectation of the two coins iteratively, as the M-Step Apply EM algorithm to solve Gaussian mixture modelEach observation (experimente record) corresponds to the overlap of multiple normal distributions and the parameters are unkown. Then, we can randomly initialize the parameters for the distributions, and find out how each data point can be decomposed into these distributions. Then, we can get the datapoints weighted so that it only includes one distribution. Then we can correct the parameters of normal distribution in an iterative manner. It should be noted that the guarantee of convergence does not mean that the EM algorithm can converge at the global optimal point, because the given initial parameters decide the upper-bound of performance to a large extent. AppendixThis is about the deduction of EM algorithm’s property of convergence guarantee. Jensen Inequityif $f$ is a Concave function, $X$ is a random variable, then $E[f(X)]\\le f(E[x])$.Similar conclusion holds, with the unequal sign in the opposite direction, when $f$ is a Convex function. Let $P(x,z)$ be the distribution with latent variable $z$ (the weight &#x2F; contribution of a certain Gaussian distribution made to a datapoint).$$ \\sum_{i&#x3D;1}^M\\sum_{z&#x3D;1}^NlnP(x,z) &#x3D; \\sum_{i&#x3D;1}^Mln\\sum_{z&#x3D;1}^NQ(z) \\frac{P(x,z)}{Q(z)}$$With Jensen inquity (log is a concave function):$$ \\sum_{i&#x3D;1}^Mln\\sum_{z&#x3D;1}^NQ(z) \\frac{P(x,z)}{Q(z)} \\ge \\sum_{i&#x3D;1}^M\\sum_{z&#x3D;1}^NQ(z) ln\\frac{P(x,z)}{Q(z)} $$ The key step is to adjust $Q(z)$ so that makes the right part equals the left part (reach the $&#x3D;$). The $&#x3D;$ can be reached when the input function is a constant function, i.e., $\\frac{P(x,z)}{Q(z)}&#x3D;c$.Then we have $\\sum_z P(x,z) &#x3D; c\\sum_z Q(z) \\Rightarrow Q(z) &#x3D; \\frac{P(x,z)}{\\sum_zP(x,z)} &#x3D; \\frac{P(x,z)}{P(x)} \\Rightarrow P(z|x)$ Then, we solved the issue of how to select $Q(z)$ – by selecting the posterior probability. This is the essence of E-step which is to build lower-bound for the likelihood function $L(\\theta) &#x3D; \\sum_{i&#x3D;1}^M\\sum_{z&#x3D;1}^NlnP(x,z)$, where $\\theta$ is the parameter set of these distributions.The following M-step aims at adjusting $\\theta$ to maximize the lower-bound of $L(\\theta)$.It should be noted that this process is guaranteed to converge, but it may get into local optimal rather than the real parameter values (reach the global optimal). This is determined by the initialization of parameters.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"General Knowledge","slug":"Data-Science/General-Knowledge","permalink":"https://umiao.github.io/categories/Data-Science/General-Knowledge/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://umiao.github.io/tags/Machine-Learning/"},{"name":"Naive Bayes","slug":"Naive-Bayes","permalink":"https://umiao.github.io/tags/Naive-Bayes/"}]},{"title":"Missing Values Patterns in Time Series Data","slug":"Missing-Values-Patterns-in-Time-Series-Data","date":"2022-05-02T03:36:26.000Z","updated":"2022-05-02T05:18:55.133Z","comments":true,"path":"2022/05/01/Missing-Values-Patterns-in-Time-Series-Data/","link":"","permalink":"https://umiao.github.io/2022/05/01/Missing-Values-Patterns-in-Time-Series-Data/","excerpt":"It is meaningful and believed to be possible to discover the pattern of the missing parts of the time series data. Such patterns may vary in different scenarios and sources and may be related with physical devices and configurations.","text":"It is meaningful and believed to be possible to discover the pattern of the missing parts of the time series data. Such patterns may vary in different scenarios and sources and may be related with physical devices and configurations. Algorithms for pattern detection In the above image, we can find out the distribution and comparison between the imputed values VS the known patterns. A pipeline of missing data pattern detecting is proposed in this paper.1 Create base matrices to represent data. An algorithm to quantify and categorize missing values slots. Evaluate the frequencies of time attributes to determinate the most crucial time scenarios to analyze. Use this time attributes to find patterns over classified slots applying Kernel Density Estimation (KDE) that is considered as a statistical model to understand the shape and features of data. Data missing Around 48% of studies process dataset with missing values.2 You can discard the records with missing values, or use data imputation methods to recover the missing values. (this can be especially common in time series data) It would be helpful if you can find out the mechanism of data missing so that you can select a suitable imputation method. Avoid the missing of values during collection would always be the best solution! Missing value mechanism Missing At Random (MAR): (may work well with statistical based methods) Missing Completely At Random (MCAR): (may work well with Hot-Deck) Missing Not At Random (MNAR): (may works well with learning algorithm, like Random Forest) Problem formulationRepresentationConsidering $n$ IOT devices (sensors), each report one attribute and they monitor in a period of $t$ time slots. The, we can denote the data with $x$ and specify a value with $x(t, n)$ (at the $t$th time and collected by the $n$th device). The $t$ is expected to be in the form of a timestamp and $x$ can be viewed as a 2D matrix. We can also introduce a binary matrix (as an indicator) to mark if an element of the above matrix is missing (equals null). We can define$$BM &#x3D; X(t, n) &#x3D; \\begin{cases} 0,\\quad (x(t, n) \\quad is \\quad null) \\\\ 1 \\quad (otherwise) \\end{cases} $$ Feature selectionA series of papers propose different feature selection strategies based on the matrix $x$ to formulate new feature sequences, including: finding the cumsum, finding the indexes of missing values, transpose of the missing value indexes, record the missing values’ count and span, etc. The missing value spans can also be pre-categorized into different levels (e.g., minute &#x2F; hour &#x2F; day level). KDE: Kernel Density EstimationSelect a bandwidth parameter $h$ (may be viewed as the window’s length) and a kernel function $K(x;h)$. The function $K$ can be selected from: gaussian, tophat, epanechnikov, exponential, linear or cosine. The 1-D time series case:$$ \\hat f_K(x)&#x3D;\\sum_{i&#x3D;1}^n K_{h_x}(x-x_i) $$The 1-D time series case: (the timestamp may have 2 or more channels)$$ \\hat f_K(x,y)&#x3D;\\sum_{i&#x3D;1}^n K_{h_x}(x-x_i) K_{h_y}(y-y_i)$$Use the above function to retrieves the points to make a bivariate scatter diagram to visualizeand understand the shape and features of missing data periods. Here $x$ and $y$ may represent the start and end time of a missing part. The mined patterns would be like the visualization results shown above. We can find the common parts shared by the periodical data and view the rest parts as random noise. References [1] [Lima, Juan-Fernando, Patricia Ortega-Chasi, and Marcos Orellana Cordero]”A novel approach to detect missing values patterns in time series data.” Conference on Information Technologies and Communication of Ecuador. Springer, Cham, 2019. [2] [Dong, Y., Peng, C.Y.J.]Principled missing data methods for researchers. Springer-Plus 2(1), 222 (2013).","categories":[{"name":"UCLA","slug":"UCLA","permalink":"https://umiao.github.io/categories/UCLA/"},{"name":"Course Study","slug":"UCLA/Course-Study","permalink":"https://umiao.github.io/categories/UCLA/Course-Study/"},{"name":"ECE209 in 2022 spring","slug":"UCLA/Course-Study/ECE209-in-2022-spring","permalink":"https://umiao.github.io/categories/UCLA/Course-Study/ECE209-in-2022-spring/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"UCLA","slug":"UCLA","permalink":"https://umiao.github.io/tags/UCLA/"}]},{"title":"DS-Study-Note-5 Support Vector Machine (SVM)","slug":"DS-Study-Note-5","date":"2022-04-27T06:51:47.000Z","updated":"2022-05-01T08:11:21.602Z","comments":true,"path":"2022/04/26/DS-Study-Note-5/","link":"","permalink":"https://umiao.github.io/2022/04/26/DS-Study-Note-5/","excerpt":"SVM is a machine learning model which aims at finding a Decision Boundary with a subset of the training set. The SVM is a non-probabilistic binary classifier.","text":"SVM is a machine learning model which aims at finding a Decision Boundary with a subset of the training set. The SVM is a non-probabilistic binary classifier. Linear SeparableLinear Separable stands for an attribute that two class of points can be completely divided by a hyperplane (more specifically, a line in the 2-D space).A hyperplance can be determined with normal vector $W$ and intercept $b$, i.e., $$ X^TW + b&#x3D;0 $$. For the two separable groups, they would satisfies $X^TW + b&gt;0$ and $X^TW + b&lt;0$, respectively. In order to enhance the robustness, we additionally require the best-fit hyperplane to separate these two classes with maximum margin &#x2F; interval, which is called Maximum Margin Hyperplane. SVM (Support Vector Machine)In the training set, the points which are nearest to the hyperplane are named Support Vector. After generalized to $n$ dimensional space, a point $x &#x3D; (x_1, …, x_n)$’s distance to the hyperplane $w^Tx+b&#x3D;0$ is $\\frac{|w^Tx+b|}{||w||}$ (the denominator corresponds to 2-norm). We are interested with these support vectors and optimize the hyperplane in order to maximize the margin between the support vectors which belong to different classes. If the maximized distance equals $d$, for all the support vectors, we further unfold the absolute value expression to have:$$ \\frac{w^Tx+b}{||w||} \\ge d, y&#x3D; 1; \\quad \\frac{w^Tx+b}{||w||} \\le -d, y&#x3D; -1$$while $y$ marks different classes.Ignore the constant factor, we can have $$ w^Tx+b \\ge 1 (y&#x3D;1); \\quad w^Tx+b \\le -1 (y&#x3D;-1)$$which equals $$ y(w^Tx+b) \\ge 1 $$, so that we use hyperplanes $w^Tx+b&#x3D; \\pm 1$ to seperate the two classes. Replace the numerator of the distance expression with the two hyperplanes to have our optimization target:$$ \\max_{w, b} margin \\Leftrightarrow \\max(\\frac{2}{||w||}) \\Leftrightarrow \\max(\\frac{2}{||w||})^2 \\Leftrightarrow \\min({||w||}^2), \\quad y(w^Tx+b) \\ge 1 $$ We can add a constant factor of $\\frac{1}{2}$ to absorb the constant factor after get derivative of $w^2$. Support VectorsNow we can tell that support vectors are all the vectors on the lines of $wx^T +b &#x3D; \\pm 1$. Only the support vectors would contribute to the classification. Primal-Dual TransformationIn order to solve the primal problem of $\\frac{1}{2} {||w||}^2$, we can use Method of Lagrange Multiplier to solve its Dual Problem. Solving the corresponding dual problem has the advantages of: Easier to solve with simpler constraints and only need to optimize one variable $\\alpha$ Able to introduce kernel function to generalize to non-linear cases Method of Lagrange Multiplier Problem formulation$$ \\min f(x_1, …, x_n) s.t. h_k(x_1, …, x_n)&#x3D; 0, k&#x3D;1,2,…,l $$That is to say, we decide to optimize function $f(x_1, …, x_n) $ with $l$ extra constraints. We can let $$ L(x, \\lambda &#x3D; f(x) + \\sum_{k&#x3D;1}^l \\lambda_k h_k(x) $$, where $L(x, \\lambda)$ is named Lagrange Function, and $\\lambda$ is NOT required to be non-negative. When solving the problem, we need to find $\\frac{\\partial L(x, \\lambda)}{\\partial x_i}$ for each $i \\in {1, …, n}$ and let them be $0$. This is called necessary condition of equality constraints (in order to get extremum). Strong DualityWe want to transform $$ \\min_w \\max_\\lambda L(w, \\lambda) \\Rightarrow \\max_\\lambda \\min_w L(w, \\lambda), \\quad (\\lambda_i \\ge 0)$$For function $f$, if we have $\\min \\max f \\ge \\max \\min f$, that is, the minimum of the possible maximums is still greate than the maximal possible minimums, we say there exists weak duality. If $f$ is convex optimization problem, we have strong duality.In order to handle the SVM problem, we require the Karush-Kuhn-Tucker (KTT) condition as the necessary and sufficient condition of strong duality. Karush-Kuhn-Tucker (KTT) conditionWe need to transform the optimization of inequality into optimization of equality:$$ \\min f(w)&#x3D;\\min \\frac{1}{2} {||w||}^2, g_i(w) &#x3D; 1 - y_i(w^Tx_i+b)\\le 0 $$into$$ L(w, \\lambda, a) &#x3D; f(w) + \\sum_{i&#x3D;1}^n \\lambda_i h_i(w) &#x3D; f(w) + \\sum_{i&#x3D;1}^n \\lambda_i [g_i(w) + \\alpha_i^2], \\lambda_i \\ge 0$$$g_i(w)$ is guaranteed to be $\\le 0$ for $\\forall i$, we can solve a series of non-zero values (a_i^2) to make every term of constraints $0$. Then we require $L$’s’ partial derivative to $w$, $\\lambda$ and $a$ equals $0$ to derive the KKT condtion:$$\\min L(w, \\lambda, a) \\Rightarrow \\min L(w, \\lambda) &#x3D; f(w) + \\sum_{i&#x3D;1}^n \\lambda_i g_i (w)$$The achieved minima must be greater than $0$, we have $\\sum_{i&#x3D;1}^n\\lambda_i g_i(w)\\le 0 $, $L(w, \\lambda) \\le p$ as a natural upper bound. So that our objective can be changed to $\\max_\\lambda L(w, \\lambda)$.The dual optimization problem after transformation: $$\\min_w \\max_\\lambda L(w, \\lambda) , \\lambda_i \\ge 0$$ Procedure of solving SVM problem Construct Lagrange Function $\\min_{w,b}\\max_{\\lambda}L(w,b,\\lambda)&#x3D; \\frac{1}{2} {||w||}^2 + \\sum_{i&#x3D;1}^n \\lambda_i [1 - y_i(w^Tx_i+b)] $ Transform to dual problem $\\max_\\lambda \\min_{w,b}L(w,b,\\lambda)$Find derivatives: $\\frac{\\partial L}{\\partial w}&#x3D;w - \\sum_{i&#x3D;1}^n \\lambda_i x_iy_i&#x3D;0$,$\\frac{\\partial L}{\\partial b}&#x3D;\\sum_{i&#x3D;1}^n \\lambda_i y_i&#x3D;0$ to get $\\sum_{i&#x3D;1}^n \\lambda_i x_iy_i&#x3D;w$, $\\sum_{i&#x3D;1}^n \\lambda_i y_i&#x3D;0$.Substitue into function:, that is, $$ \\min_{w,b}L(w,b,\\lambda)&#x3D;\\sum_{j&#x3D;1}^n\\lambda_i - \\frac{1}{2} \\sum_{i&#x3D;1}^n\\sum_{j&#x3D;1}^n \\lambda_i\\lambda_jy_iy_j(x_i \\cdot x_j) $$ We can tell that the above question is a quadratic programming problem and its scale is proportional to the number of training samples. It can be solved with Sequential Minial Optimization (SMO) algorithm. It optimizes one parameter a time and fixed the others. We just mentioned that SMO algorithm optimize only one parameter a time. However, we have a constraint $\\sum_{i&#x3D;1}^n \\lambda_i y_i&#x3D;0$ which must be satisfied. Thus, we update two parameters a time to solve this issue. With two selected parameters $\\lambda_i$ and $\\lambda_j$, we fix the other parameters and we have $$\\lambda_i y_i + \\lambda_j y_j &#x3D; c, \\lambda_i \\ge 0, \\lambda_j \\ge 0, c&#x3D; -\\sum_{k \\ne i,j}\\lambda_ky_k $$ Then we can have $\\lambda_j &#x3D; \\frac{c-\\lambda_iy_i}{y_j}$ so that we can replace $\\lambda_j$ with expression of $\\lambda_i$, find the partial derivative for $\\lambda_i$ and update the two paramteres, keep iterating until converge. By find partial derivative, we can have $w&#x3D;\\sum_{i&#x3D;1}^m \\lambda_i y_i x_i$. All the points corresponding to a $\\lambda_i &gt; 0$ are support vectors (non-support vectors correspond to $\\lambda_i&#x3D;0$), and we can find an arbitary support vector $x_s$ and substitute x to have $y_s(wx_s+b)&#x3D;1 \\Rightarrow y_s^2(wx_s+b)&#x3D;y_s$ to find $b$. $y_s^2&#x3D;1$, thus $b&#x3D;y_s - wx_s$. We can also find b via the mean value of the support vectors: $$b&#x3D;\\frac{1}{|S|}\\sum_{s\\in S}(y_s - wx_s) $$ With $w$ and $b$ found, we can construct the hyperplane $w^Tx+b&#x3D;0$ and use $f(x)&#x3D;sign(w^Tx+b)$ to decide the classification result. Parameters to be searched (tuned) in SVM C: decide the level of regularization. In fact, $C&#x3D;\\frac{1}{\\lambda}$ and $\\lambda$ is the regularization coefficient. (Is a squared l2-penalty) kernel: The kernel function adopted by SVM model. degree: The highest degree, when adapting Polynomial kernel function. gamma: Coefficent of the kernel function.Beside, you can decide whether to use heuristic shrinking, tolerance (the desired accuracy at convergence), size allocated to kernel cache, iteration times. You can also appoint random state for reproducing results. Soft MarginThe original SVM require the problem to be solved is linear separable. When this prerequisite is not satisfied, we can use soft margin to solve this problem. Originally, we require the two classes correspond to different signs, in a hyperplane expression. Now, we can allow SVM to misclassify on a handful of vectors.$$ y_i(w^Tx_i+b) - \\xi_i \\ge 1$$ which means that for such $i$, they do not meet the constraint and need the help of relax coefficient $\\xi_i$. In order to minimize the number of misclassified vectors, we would fix our objective (loss) (by appending a regularization term for the relax coefficients):$$ \\min_w \\frac{1}{2}{||w||}^2 + C \\sum_{i&#x3D;1}^m \\xi_i, \\quad g_i(w,b)&#x3D;1-y_i(w^Tx_i+b)-\\xi_i \\le 0, \\xi_i \\ge 0 $$Here, $C &gt; 0$ and can be understood as the penalty to the misclassified samples. If $C \\rightarrow \\infty$, then $\\xi_i \\rightarrow 0$ and the SVM (with soft margin) becomes linear separable SVM. Kernel FunctionThe essence of kernel function $K$ is to map vectors from low-dim Hilbert Space to high-dim. By this mean, we expect the samples which are not linear separable become separable, in the high-dim space.$$K(x,z)&#x3D;\\phi(x) \\cdot \\phi(z)$$Prerequisite: The Gram matrix formed by the set of all the points of the function is semi-positive definite. (a.k.a. complete inner product space) The introduction of kernel function is to solve the case of linear unseparable. It is costly to map samples to high-dimensional space before calculating dot product, thus kernel function can realize effectively calculating the dot product result of high-dimensional space, in low-dimensional space. Types of kernel functions Linear kernel: $x_i^Tx_j$ Polynomial Kernel: $(x_i^Tx_j+c)^d$ Radial basis function kernel: $exp(-\\frac{||x_i-x_j||_2^2}{2\\sigma ^2})$ Hyperbolic tangent kernel: $tanh(Kx_i^Tx_j+c), \\quad K&gt;0, c&lt;0$It is clear that only the latter three needs parameter tuning. Pros and ConsAdvantages: Supported by math theory, highly interpretable Do not rely on statistical method, simplify the regular classification and regression problem (solely rely on the support vectors, which are deterministic and crucial samples) Applying of kernel function can handle the unlinear tasks Computational complexity relies on the number of support vectors(rather than the dimensional of the sample space), avoid the curse of dimensionDisadvantages: Long training time (every time select a pair of parameters for optimization to maintain the sum unchanged). The complexity would be $\\mathcal{O}(n^2)$ where $N$ equals the number of training samples. If applied kernel function and need to store the matrix, extra space of $\\mathcal{O}(n^2)$ is required. The prediction rate is inversely proportional to the number of support vectors, leading to high complexity. Not suitable for scenarios which have millions, or even hundreds of millison of samples.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"General Knowledge","slug":"Data-Science/General-Knowledge","permalink":"https://umiao.github.io/categories/Data-Science/General-Knowledge/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://umiao.github.io/tags/Machine-Learning/"},{"name":"SVM","slug":"SVM","permalink":"https://umiao.github.io/tags/SVM/"}]},{"title":"DS-Study-Note-4 Metrics","slug":"DS-Study-Note-4","date":"2022-04-27T05:28:19.000Z","updated":"2022-04-27T06:47:18.035Z","comments":true,"path":"2022/04/26/DS-Study-Note-4/","link":"","permalink":"https://umiao.github.io/2022/04/26/DS-Study-Note-4/","excerpt":"Metrics are used for model training and evaluation. It reveals a model’s performance on a given dataset.","text":"Metrics are used for model training and evaluation. It reveals a model’s performance on a given dataset. PreliminaryWe use: $f$ to denote our model (function) $D$ to denote the dataset used, $m$ as the sample it contains. Error rateDefined as the number of incorrectly classified samples divide the number of total samples.$$ E(f;D) &#x3D; \\frac{1}{m}\\sum_{i&#x3D;1}^m \\mathbb{I}(f(x_i) \\ne y_i) $$Similarly, the continuous form:$$E(f;D) &#x3D;\\int_{x \\sim D} \\mathbb{I}(f(x_i) \\ne y_i)p(x)dx $$ AccuracyDefined as the number of correctly classified samples divide the number of total samples.$$ acc(f;D) &#x3D; 1 - E(f;D) &#x3D; \\frac{1}{m}\\sum_{i&#x3D;1}^m \\mathbb{I}(f(x_i) &#x3D; y_i) $$Similarly, the continuous form:$$acc(f;D) &#x3D; \\int_{x \\sim D} \\mathbb{I}(f(x_i) &#x3D; y_i)p(x)dx $$ Recall and Precision RateWhen the distribution of different classes are NOT balanced, we may be interested in how many samples of interest (positive samples) is found (Recall Rate) and how many of the filter samples of interest are correct (Precision Rate). We use TP &#x2F; FP &#x2F; TN &#x2F; FN to denote the frequency of True Positive &#x2F; False Positive &#x2F; True Negative &#x2F; False Negative. Precision Rate$$ P &#x3D; \\frac{TP}{TP + FP} $$ (TP + FP stands for all the samples classified as positive) Recall Rate$$ R &#x3D; \\frac{TP}{TP + FN} $$ (TP + FN stands for all the samples whose labels are positive) Tradeoff Pursuing Recall Rate and Precision Rate at the same time is often contradictory. This is because a high Precision means that the model would be cautious as possible so that many positive samples with lower confidence would be classified as negative. PR CurveIn order to unify the Precision and Recall rate and compare the performance of different model, PR Curve is proposed. The model is required to sort all the samples by the confidence (of being positive sample). Normally, for the first sample, the Precision would be 1 while the Recall would be the minima (close to 0). Predict the current sample to be positive, one-by-one, in the sorted order and keep calculating the corresponding Precison and Recall Rate. When all the samples are predicted to be positive, we have a Recall of 1 and the Precision reaches the minima. Break-Event Point of PR CurveIn order to compare the performance of two models, we can use the area under the PR curve as the metric. However, this value can be hard to calculate. Thus, we use the Break-Event Point to reach a balance between the Recall and Precision.It is calculated by the x-coordinate of the intersection point of the PR Curve and function $y&#x3D;x$.Ideally, we want both the Precision and Recall to be as high as possible. F1-ScoreF1-Score is a more commonly used metric to reach a balance between Precision and Recall, it is defined as the Harmonic Mean of these two:$$ \\frac{1}{F1} &#x3D; \\frac{1}{2} \\times (\\frac{1}{P} + \\frac{1}{R}) $$ Macro scope (Macro-F1) For N-class classification problem, first calculates $N$ F1-Scores on $N$ Confusion Matrix. Find the arithmetic mean of the $N$ F1-Scores. Micro scope (Micro-F1) Find $N$ groups of $TP$, $FP$, $TN$, $FN$ and find the arithmetic mean for each of the four. Calculate F1-Score with the mean $TP$, $FP$, $TN$, $FN$. GeneralizationWith the method describe above, you can similarly define micro-P, macro-P, micro-R, macro-R. Receiver Operating Characteristic (ROC) Curve Similar to PR Curve, sort all the samples by the confidence of predicting as positive. Find $$ TPR &#x3D; \\frac{TP}{TP + FN} $$, $$ FPR &#x3D; \\frac{FP}{TN + FP} $$.These two values stand for the ratio of correctly classified positive samples and incorrectly classified negative samples. For a 0-1 Classification task, a model based on random guess‘s ROC Curve should correspond to $y&#x3D;x$ and an accuracy of $0.5$. Area Under ROC Curve (AUC) With finite samples, you can draw the coordinate $(TPR, FPR)$ for each point and find the area under the ROC Curve. This is a common metric in evaluating models. Find a confidence threshold for ROC Curve Set the threshold to $\\inf$ which exceeds the confidence (score) of all the positive samples and they are all predicted as negative. In this case, both the TPR and FPR equal $0$. With the threshold goes down, these two would gradually raise to $1$. Define Sorting Loss based on AUC For each pair of positive-negative samples, if the positive sample achieves a score lower than the negative sample, add $1$ to the Loss. If equals, add $0.5$. This corresponds to the area above the ROC Curve and thus should be minimized. $AUC &#x3D; 1- L_{rank}$ Similarly, we can assign different weights to different types of mistakes a classifer made and draw the cost curve. We can find the weighted sum and for each point, draw the curve of expected cost (line determined by $(0, FPR)$ and $(1, FNR)$. Then we find the lower bound of all such lines, the area under this curve stands for the cost.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"General Knowledge","slug":"Data-Science/General-Knowledge","permalink":"https://umiao.github.io/categories/Data-Science/General-Knowledge/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://umiao.github.io/tags/Machine-Learning/"}]},{"title":"DS-Study-Note-3 Dimension Curse","slug":"DS-Study-Note-3","date":"2022-04-26T22:19:27.000Z","updated":"2022-04-27T05:26:41.457Z","comments":true,"path":"2022/04/26/DS-Study-Note-3/","link":"","permalink":"https://umiao.github.io/2022/04/26/DS-Study-Note-3/","excerpt":"DefinitionDimension curse stands for the troubles you would meet when processing high-dimensional data. E.g., computation of similarity, distance, neighbour or any metric based processing.","text":"DefinitionDimension curse stands for the troubles you would meet when processing high-dimensional data. E.g., computation of similarity, distance, neighbour or any metric based processing. The reason is that for high-dimensional space, the concept of distance will gradually fail, and even make any two points infinitely inseparable, even if they look different &#x2F; correspond to very different categories. As the dimension (of vector representation) grows, the data becomes more and more sparse and may correspond to higher variance and bias error. These phenomena are essentially caused by the same theory. However, the theoretical derivation is omitted here. Specifically, dimension curse can be categorized into the following more concrete problems: Distance concentration Combinational explosion Hubness Distance concentration As the number of dimension raises, for the queried point, the distance to its closest neighbour would converge to the distance to the furthest point. This also means that the difference of distance between arbitrary two points would be negligible. The difference of distance may be small enough, for a dimension of 20. The distance may remain effective, if there lies inherent clusters in the data and these clusters are distant from each other. Or, if many of the data’s dimensions are redundant (the data can be embedded into a space with much smaller dimension). In high-dimensional space, small change of the neighbourhood radius determines the difference of selecting only ONE point or selecting ALL the datapoints. This is because the volume ratio of a fixed radius hypersphere to a unit radius hypersphere will be close to 1. Increase of relevant features would be beneficial to the model. Increase of irrelevant features would impair the model’s performance. Distance under different dimension and space CANNOT be compared with each other. Combinational explosion As the dimensionality increases, a larger percentage of the training data resides in the corners of the feature space. It is also much more difficult to traverse the increasing search space as the size of the search space grows exponentially, as shown in the above image. A complex search space may correspond to the same configuration (object &#x2F; training &#x2F; testing sample) in the low-dimensional case, resulting in overfitting. Training samples with larger scale are required to suppress overfitting (on data with high dimensions). Algorithms like Random Forest can restrict the number of features used (in one tree). Hubness With the increase of dimension, a handful of points are significantly more frequent to become the nearest neighbour of other points. This is also called Hubness. If we record the frequency of each point becoming the nearest neighbour of another point, we can find that is frequency follows Zipf’s Law and is right-skewed heavily. Generally, points close to the mean value of the entire dataset become hubs more easily. When cluster exists, data points close to the mean value of the cluster are more likely to become hubs as well. Outlier &#x2F; Anti-hubs These points are most distant from the majority of other points. Anti-intuitive things may happen, i.e., hubs exist in the low-density region of the high-dimensional space, but the hubs are close to the other points. At the same time, anti-hubs may exist in the high-density region of the high-dimensional space, but the anti-hubs are distant to the other points (which makes them outliers). This can be viewed as a mismatch between the probabilistic density and distance distribution. Solution (to dimension curse) For the cases of Multicollinearity and Duplication of Components(redundancy) happen, we can simply remove the redundant variables after evaluation. In the case when each dimension (feature) contributes equally to the model’s result, methods like Dimensional Reduction (including CNN, Convolutional Neural Network) can be applied. Dimensional Reduction’s Disadvantages: Converted data points DO NOT represent original features. Less interpretable, hard to visualize, weaker theoretical support.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"General Knowledge","slug":"Data-Science/General-Knowledge","permalink":"https://umiao.github.io/categories/Data-Science/General-Knowledge/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://umiao.github.io/tags/Machine-Learning/"}]},{"title":"DS-Study-Note-2 Bias VS Variance","slug":"DS-Study-Note-2","date":"2022-04-23T07:41:01.000Z","updated":"2022-04-26T22:18:22.843Z","comments":true,"path":"2022/04/23/DS-Study-Note-2/","link":"","permalink":"https://umiao.github.io/2022/04/23/DS-Study-Note-2/","excerpt":"The target of Machine Learning is to fit an (unknown) distribution. There lies three possible error: bias, variance and irreducible error.","text":"The target of Machine Learning is to fit an (unknown) distribution. There lies three possible error: bias, variance and irreducible error. The irreducible error CANNOT be avoided with any algorithm as it can be viewed as the result of unknown factor, noise, accidents, etc. Thus, we would focus on the bias and variance error. Definition Bias can be understood as the accuracy of the model, i.e., the ability to estimate the output value accurately. Variance can be understood as the stability of the model, i.e., the ability of resisting the noise and disturbance contained by the input. I also understood this ability as being able to recognize similar inputs and generate similar results for them. Example Applying K-fold cross validation can reduce the influence casted by the outliers and enhance the generalization ability, which reduces the variance error. At the same time, part of the data is not used for training, which impairs the model’s fitting ability and increase the bias error. An intuition is that, a more complex model is more sensitive to the noise contained by the input, which makes the output less stable (higher variance error). At the same time, a simpler model more easily ignores the random noise and difference of distribution between the training set and the testing set. Tradeoff and AnalysisThe variance of the parameters you are estimating can be reduced, at the cost of increasing bias. If you would like to generalize a model trained on a certain training set, then you CANNOT minimize the bias and variance error at the same time. Bias: Bias error comes from the erroneous assumptions of the learning algorithm. High bias error corresponds to underfitting. Bias error measures the closeness between the distribution you modeled and the expectation of the real distribution. Introduction of bias term in linear models aims at simplying the learning process without introducing way more complicated distribution which makes the model hard to generalize. At the same time, such models would fail to solve the more complicated models which do not meet the assumption (that this problem can be approximated by linear model). Variance: Variance error comes from the noise &#x2F; fluctuation &#x2F; disturbance of the training set. High variance error probably means that you are modeling on the random noise of the training set, which cannot be generalized, and this means overfitting. Variance error reveals the level of concentration of your model.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"General Knowledge","slug":"Data-Science/General-Knowledge","permalink":"https://umiao.github.io/categories/Data-Science/General-Knowledge/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://umiao.github.io/tags/Machine-Learning/"}]},{"title":"DS-Study-Note-1 Overfitting and Brief Introduction on Decomposition and Regularization","slug":"DS-Study-Note-1","date":"2022-04-22T16:23:14.000Z","updated":"2022-04-23T07:37:49.936Z","comments":true,"path":"2022/04/22/DS-Study-Note-1/","link":"","permalink":"https://umiao.github.io/2022/04/22/DS-Study-Note-1/","excerpt":"Overfitting is a modeling error in statistics that occurs when a function is too closely aligned to a limited set of data points. —- Definition ranked 1st in Google","text":"Overfitting is a modeling error in statistics that occurs when a function is too closely aligned to a limited set of data points. —- Definition ranked 1st in Google Definition Overfitting stands for making excessive learning steps on the training set, which results in the model extracting noise of the training set as valid pattern to fit the training set’s distribution. In this case, the trained model would show low performance on the testing set and new data (real-world data). Possible Causes: The volume of the training data is too small. The data distribution of the training set data does NOT subject to the testing and real business data. This also means that the iid (identically and independent distributed) assumption is not satisfied. There exists noise in the training set. Too many iteration times. Fail to learn correct features with ability of generalization and representative. The overfitting MAY be phenomenon of information leakage, i.e., too complicated model remembers the training which makes the inference equivalent to the table look-up. Solutions: Discard some features. This can be implemented by Feature Selection or simply randomly discard a subset. This process can be: Conducted manually. Randomly. (Random Forest) Decided by Model Selection Algorithm. E.g., PCA(Principal component analysis). PCA: Solve the eigen-vector of the covariance matrix. It is obvious that the larger the covariance is, the more useful the corresponding eigen-value is. Find the largest k eigen-values and use their corresponding eigen-vectors to form a matrix as the PCA output. (You can also use SVD for such decomposition.) You can also use dimensional reduction tools like LR(lower–upper) decomposition, SVD(Singular Value Decomposition). Introduce regularization. Introduce drop-out layer when training network. Use Early-Stop to achieve the tradeoff the generalization ability and convergence on the training set. (In this case, evaluation set is required for observation.) A combination of methods above. Adopt models like Random Forest. Any method which is believed to be able to control the model’s complexity. E.g., control the depth, number of trees. Selection Bewteen the L1 and L2 Regularization Term L1 - LASSO (Least Absolute Shrinkage and Selection Operator): cast penalty on the sum of the absolute value of the model parameters.Regularize all the parameters equally. Able to transform some parameters into 0. (make the model sparse)$${L1}_{reg} &#x3D; \\lambda \\sum_{j&#x3D;1}^p |\\beta _j|$$ L2 - Ridge Regression: cast penalty on the sum of the square value of the model parameters.$${L2}_{reg} &#x3D; \\lambda \\sum_{j&#x3D;1}^p \\beta _j^2$$ In essence, these two regularization method is to conduct L1 &#x2F; L2 Normalization on the model parameters and add the normalized term to the Loss Function for optimization.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"General Knowledge","slug":"Data-Science/General-Knowledge","permalink":"https://umiao.github.io/categories/Data-Science/General-Knowledge/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://umiao.github.io/tags/Machine-Learning/"}]},{"title":"SQL-Study-Note-11 User and Privilege Management","slug":"SQL-Study-Note-11","date":"2022-04-21T19:39:39.000Z","updated":"2022-04-21T22:06:33.950Z","comments":true,"path":"2022/04/21/SQL-Study-Note-11/","link":"","permalink":"https://umiao.github.io/2022/04/21/SQL-Study-Note-11/","excerpt":"Most data science practitioners would not be granted the privilege of managing the database system (not even the privilege to update &#x2F; delete), so…","text":"Most data science practitioners would not be granted the privilege of managing the database system (not even the privilege to update &#x2F; delete), so… Create and Manage User1234567891011121314CREATE USER join@&#x27;%.google.com&#x27; IDENTIFIED BY &#x27;1234&#x27;;-- You can restrict the domain of user with this method.-- You can also specify an ip after &#x27;@&#x27;.-- &#x27;1234&#x27; Stands for the password.SELECT * FROM mysql.USER;-- Retrieve the information of all the users.-- It is also supported to use GUI interface to manage the user,\\-- their host to log in with, etcDROP USER bob@gmail.com;-- Drop a user.SET PASSWORD (john) = &#x27;1234&#x27;;-- Reset a user&#x27;s password.-- It is also supported to EXPIRE PASSWORD for one user.-- So that he would be required to change the password by next log in. Privilege Management12345678GRANT SELECT, INSERT, UPDATE, DELETE, EXECUTE ON sql_store.* TO user_a;-- An example of granting the privileges.SHOW GRANTS;-- Show all the grantsREVOKE privilege .. ;-- Revoke specified granted privilege. Refer documentation for more related instructions.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"SQL-Study-Note-10 Index","slug":"SQL-Study-Note-10","date":"2022-04-21T17:53:59.000Z","updated":"2022-04-21T19:55:39.737Z","comments":true,"path":"2022/04/21/SQL-Study-Note-10/","link":"","permalink":"https://umiao.github.io/2022/04/21/SQL-Study-Note-10/","excerpt":"Index can be used to find the row (line) numbers corresponding to the value being queried. Index is added to certain columns and is stored in memory (RAM) for most times.","text":"Index can be used to find the row (line) numbers corresponding to the value being queried. Index is added to certain columns and is stored in memory (RAM) for most times. Create IndexIndex can speed up the query, however, it would also increase the size (memory comsuption) of database as well as the cost of maintenance. It is usually implemented by binary tree in database systems. 12CREATE INDEX idx_state ON customers (state);-- Specify a column of a table to create an index. Explain and ANALYZE1EXPLAIN SELECT * FROM ... Add EXPLAIN before a sql query would get explanatory information instead of the query result. E.g., which information is used and how many records are went through, for performance evaluation. 123456SHOW INDEXES IN customers;-- Reveal all the indexes in the customers table.-- You can find out the indexes added, the cardinality and name.ANALYZE TABLE customers;-- Indexes includes primary index, secondary index, etc.-- You can use the ANALYZE command to view the statistics and values of a table Different Index Types prefix indexYou should create prefix index, instead of index on the entire column, for acceleration.12345CREATE INDEX idx_n ON customers (last_name(20));-- In this case, the last_name column would be grouped and indexed according to the first 20 characters only.COUNT (DISTINCT LEFT(last_name, 10));-- You can use this way to analyze the performance of creating prefix index on the first 10 characters. -- If the number of distinct values is large enough, then this prefix is able to differentiate the possible values. Full-Text IndexThe idea of such index is similar to the implementation principles of Search Engines. For all the non-stop words, record the corresponding passages (rows) and the position where they appear.1234567CREATE FULLTEXT INDEX idex_t_body ON posts (title, body);-- FULLTEXT INDEX can monitor multiple columns.SELECT * FROM posts WHERE MATCH(title, body) AGAINST (&#x27;react redux&#x27;);-- Search &#x27;react redux&#x27; in the two columns: title and body. They are viewed as TWO words.MATCH(title, body) AGAINST (&#x27;react -redux +form&#x27; IN BOOLEAN MODE);-- It is possible to exclude some words by adding a &#x27;-&#x27;.-- So the above query matches contents including react/form and without redux. Composite indexesEnable indexing on multiple columns in order to solve the issue that too many results are returned after filtering with the primary index. You can use the appearing order of the columns to decide the filtering priority of the Composite indexes.1CREATE INDEX idx_n ON customers (last_name, state, points); MySQL supports Composite index to include at most 16 columns. However, including 4-6 columns is fairly enough in practice. At the same time, Composite index supports sorting on the columns so that the more frequently used columns appear ahead. In fact, you can decide the priority of the indexes being used, by changing the appearing order of the conditions of the WHERE clause. You can use the command of: 1USE INDEX idx_name; to force MySQL to use certain index, even it is not optimal. You can NOT make full use of the index, if column is included in the sql expression (e.g. in WHERE condtion). So extract the required column first before conducting transformation. Sorting and PerformanceSorting should be avoided as possibleIt is because it is costy. You should utilize Indexes for the purpose of sorting, as possible. 1234SHOW STATUS;-- Show the variables being used in MySQL server.last_query_cost;-- You can measure the cost of last query by this mean. It should be noted that, for column a and b, ORDER BY clause can use the Index for sorting if the order is among one of these: ORDER BY a ORDER BY b ORDER BY a, b ORDER BY a DESC, b DESCAny other order would introduce external sorting operation (which means Scaning the Entire Table!).There is an exception that if WHERE clause can locate to a Single Column, then external sorting would not happen. If you can achieve the result of query solely rely on the index, then it is a Overlay Index. Duplicate index and Redundant Index Duplicate Indexes: Repeatly create the same index, e.g. on columns (a, b, c). Redundant Indexes: One index’s functionality is completely covered by the other. E.g., creating index for column (a) and (a, b) at the same time. (The latter can cover the prior)","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"SQL-Study-Note-9 Data Modeling, Constraint and Normalization Form","slug":"SQL-Study-Note-9","date":"2022-04-20T22:22:22.000Z","updated":"2022-04-21T17:53:30.704Z","comments":true,"path":"2022/04/20/SQL-Study-Note-9/","link":"","permalink":"https://umiao.github.io/2022/04/20/SQL-Study-Note-9/","excerpt":"Data Modelling Pipeline Understand the requirements; Build a conceptual Model; Build a logical model; Build a physical model.","text":"Data Modelling Pipeline Understand the requirements; Build a conceptual Model; Build a logical model; Build a physical model. Foreign Key ConstraintAlthough modify the primary key IS NOT recommended, we would consider the update to the foreign key caused by the primary key anyway. Option (strategy of updating): restrict: restrict modification cascade: update the foreign keys according to the primary key set null: set the corresponding foreign key in the foreign table into NULL no action: reject the update It is highly not recommended to use set null, as it would result in organ record in the corresponding tables (no idea which id it belongs to). Dataset NormalizationNF-1 (First Normal Form):Each record unit (element specified by row and column index) should contain a single value only and contain NO duplicate column.E.g., if you want to add tags to the Course table, you should extract tags into an independent table (and use id mapping to retrieve the tags) for it to be extendable. NF-2 (Second Normal Form):Frist is to satisfy NF-1. Also, every non candidate-key attribute depends on the whole candidate keys (that is to say, they must not depend on a true subset of the candidate keys). That is to say, each table should contain exactly one entity category only. E.g., a table stores course information should NOT contain information like the enroll time of each student. If there is an attribute which does not belong to the entity represented by this table, create a new table to store it. NF-3 (Third Normal Form):Frist is to satisfy NF-2.Also, all the attributes of the table should be determined by candidate key (for example, id) and should NOT be determined by the other non-primary attributes.That is to say, all the columns of the table should NOT be generated &#x2F; derived by the other columns, to avoid errors caused by duplicate storage and erroneous update. Data Model $ \\leftrightarrow$ TableIn MySQL, you can use forward engineer to convert data model into actual tables. Its essence is to generate sql script for database &#x2F; table generation, with the specified data model &#x2F; structual graph.The script would be like: 123CREATE schema IF NOT EXISTS … USE schema … CREATE TABLE … It is also supported to regenerate and fix the table via modifying the data model. In this case, you should select synchronize model instead of forward engineer. For those table with foreign keys, if you want to update the table, foreign keys would prevent you from doing so (as a constraint). You should first drop all the foreign keys and then reconstruct it to link to correlated tables. Reverse EngineerIt is also supported to generate data model (graphs) from existing tables. That is reverse engineering.It is highly recommended that you only put ONE database into a single data model, unless these databases are really highly correlated. Data Management Opertion (Via SQL Script)Create of Database: 12CREATE DATABASE IF NOT EXISTS name;-- Make well use of EXISTS clause to avoid error. Create Table:1234567CREATE TABLE cus ( c_id, INT PRIMARY KEY AUTO_INCREMENT, first_name VARCHAR(50) NOT NULL, points INT NOT NULL DEFAULT 0, email VARCHAR(250) NOT NULL UNIQUE ) Use Alter Table to update Table:1234567ALTER TABLE customers ADD last_name VARCHAR(50) NOT NULL AFTER first_name-- You can also use MODIFY / DROP instead of ADD to edit and delete the existing and known columns. Add Constraints (e.g., Foreign Key):12345FOREIGN KEY fk_col_name (c_id 表中列名) REFERENCES customers(c_id) ON UPDATE NO ACTION ON DELETE NO ACTION -- You cannot drop a table without droping its foreign key constraints in advance. Charset1234567SHOW CHARSET; -- You can use this command to show the charset.CREATE / ALTER DATABASE db_name CHARACTER SET lain1;-- About modifying / altering the charset (for a dataset).CREATE / ALTER TABLE () CHARACTER SET latin1;-- You can set the charset in the table level, too.-- Also, charset can be set in column level, just like adding constriants like &#x27;NOT NULL&#x27; Database Engine &#x2F; Storage Engine1234SHOW ENGINES; -- Show all the engines.ALTER TABLE customers ENGINE = InnoDB;-- Specify the engine for a table.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"Missing Value Imputation in Traffic Data","slug":"Missing-Value-Imputation-in-Traffic-Data","date":"2022-04-19T21:52:23.000Z","updated":"2022-05-05T23:30:44.310Z","comments":true,"path":"2022/04/19/Missing-Value-Imputation-in-Traffic-Data/","link":"","permalink":"https://umiao.github.io/2022/04/19/Missing-Value-Imputation-in-Traffic-Data/","excerpt":"Lost of sensor-generated data can be very common. The methods of imputation can be coarsely categorized into: 1. Prediction methods; 2. Interpolation methods; 3. Statistical Learning methods.","text":"Lost of sensor-generated data can be very common. The methods of imputation can be coarsely categorized into: 1. Prediction methods; 2. Interpolation methods; 3. Statistical Learning methods. Imputation problem &amp; Model Formulation1Let $Y_c$ be the traffic dataset persists for $N$ consecutive days,$$ Y_c &#x3D; [Y(1), …, Y(N)] $$in which the ith $Y(i)$ be noted as 1-D vector$$ Y(i) &#x3D; [y_i(1), …, y_i(D)]^T, i \\in [1, N] $$.Concatenate all the vectors we have together, we would have:$$ Y_{series} &#x3D; [y(1), …, y(D \\times N)]^T $$. Typical traffic data includes the speed and number of vehicles on a certain lane at a time. These data can form a numerical time sequence. Such data can be collected by sensor installed on the roads or along the roadsides. ARIMA-based method2ARIMA stands for Autoregressive Integrated Moving Average. In ARIMA(p, d, q), p denotes the order of the autoregressive part, d is the degree of differencing and q is the order of moving average part: $$ (1 - \\sum_{i&#x3D;1}^p \\alpha_i L^i) (1 - L)^d y(t) &#x3D; (1 + \\sum_{i&#x3D;1}^q \\beta_i L^i) y(t) \\xi(t) $$ and L is the backshift operator, $L_y(t) &#x3D; y(t- 1)$. $\\xi(t)$ is white Gaussian noise. First train this model with the known series and impute missing data one by one. The imputed data would be used as known data for next prediction. We use Akaike information criterion to determine $p$ and $q$. $d$ is suggested to be set to $1$. BNs-based imputation method3BN stands for Bayesian Netowrk. Based on the known dataset, learn the distribution model of multivariable variants $Y_{mv}(t) &#x3D; [y(t-m), …, y(t)]^T$.Assume this learning target as Gaussian mixture model (GMM).Use split and merge expectation maximisation algorithm to determine the model parameters. With a learnt GMM, missing data of $y(t)$ can be estimated as the expectation foregoing value from the latest $m$ items, as: $$ \\hat y(t) &#x3D; E[y(t) | y(t-m), …, y(t-1)] $$ k-NN based imputation method4Weighted k-NN is a non-parametric estimation method. Selection S-step (Selection Step):Use a metric to find $k$ nearest traffic daily flow vectors to the corrupted vector $Y(i)$ in pattern-similar from the dataset $Y_c$.Metrics can be Euclidean distance and Pearson correlation, for examples. Then, the lost entry &#x2F; dimension of $Y(i)$ can be imputed with the mean value of the (entries of the) $k$ vectors. Imputation I-step: (Imputation Step)Because our k-NN algorithm is weighted, then we need to find the weighted average of the k entries (averaged by the correlation coefficent given by the selected metric). Grid search and other optimization methods may be applied to determine best $k$. LLS-based imputation method5Selection S-step (Selection Step):Exactly the same as the above k-NN method. Imputation I-step: (Imputation Step)Decompose k selected vectors dataset into matrix $A$ and $B$. The dimension should be corresponding to the missing part of $Y_{mis}(i)$ and observed part $Y_{obs}(i)$ of $Y(i)$. We would have $$ \\hat Y_{mis}(i) &#x3D; B((A^TA)^{-1}A^T Y_{obs}(i)) $$. This is just a pseudo-inverse, $A$ should be full-ranked&#x2F; MCMC-based imputation method 6, 7First assume the entire data sequence $Y$ follows a certain distribution, e.g., Gaussian distribution.The conditional expectation $E[Y_{mis}|Y_{obs}, \\Phi]$ would be approximated by MCMC(Markov chain Monte Carlo) with DA(Data Augmentation), since the expectation is hard to be solved precisely due to its high dimension. Here $\\Phi$ stands for the parameter of the selected distribution. The MCMC with DA is a special case of Gibbs sampler described as follows: Imputation I-step: (Imputation Step)Given a current estimated model parameter $\\Phi ^k$, this step uses the conditional probability $Y_{mis}^{k+1}&#x3D;p(Y_{mis}|Y_{obs}, \\Phi)$ to simulate missing values for each observation independently. Posterior P-step:Use $p(\\Phi | Y_{obs}, Y_{mis}^{k+1})$ to update model parameter $\\Phi$. In this manner, a Markov chain of $(Y_{mis}^{1},\\Phi ^{1} )$, …, $(Y_{mis}^{N},\\Phi ^{N} )$ should be constructed. The missing data is estimated as$$ \\hat Y_{mis} &#x3D; \\frac{1}{N_{sample} - N_{burn-in}} \\sum_{t &#x3D; N_{burn-in + 1}}^{N_{sample}} Y_{mis}^t $$. The first $N_{burn-in}$ samples would be discarded. One feasible parameter setting is $N_{sample}&#x3D;1500$ and $N_{burn-in}&#x3D;500$. I believe the introduction of burn-in is to allow the model sometime to converge. PPCA-based imputation method8PPCA stands for Probabilistic Principal Component Analysis. It assumes that the observed data depends on latent varaiables $$ Y &#x3D; Wx + \\mu + \\epsilon $$ where $Y$ is a D-dimensional vector of observed data, $x$ is a q-dimensional latent varaible defined Gaussian distribution and $\\epsilon$ is isotropic noise. $x \\sim N(0,1)$, $\\epsilon \\sim N(0, \\sigma ^2 I)$ and $\\mu$ stands for a base mean value. Use Expectation Maximisation (EM) method to find a set of imputed data which best fit the above distribution. Concrete steps of EM method: Expectation E-step:Find out the expectation of completed log-likelihood function with previous estimated parameters $\\Phi ^k$ and observed data part $Y_{obs}$:$$ Q(\\Phi | \\Phi ^k) &#x3D; E_{X, Y_{mis} | Y_{obs}, \\Phi ^k}[log p_c (Y_c, X|\\Phi ^k)] $$We can use this to update our guess of the missing data part $Y_{mis}^k$ and latent data $X^k$. Maximisation M-step:Computiong parameter space $\\Phi$ by maximising the expectation of log-likelihood in E-step:$$ \\Phi ^{k+1} &#x3D; \\arg \\max_{\\Phi} Q(\\Phi | \\Phi ^k) $$ Conditional expectation $E[Y_{mis} | Y_{obs}, \\Phi]$ can be difficult to calculate and can approximate with MCMC with DA. Dataset Intended to Use: ComparisonPrediction and interpolation methods mentioned cannot capture stochastic variations in daily traffic flow. On the contrary, statistical learning methods could achieve traffic flow information by emphasising the statistical characteristics of traffic flow. Shorting coming of such methods: Cannot handle the situations in which neighbour (data points) DO NOT even exist. References [1] [Li, Yuebiao, Zhiheng Li, and Li Li]”Missing traffic data: comparison of imputation methods.” IET Intelligent Transport Systems 8.1 (2014): 51-57. [2] [Ahmed, Mohammed S., and Allen R. Cook]Analysis of freeway traffic time-series data by using Box-Jenkins techniques. No. 722. 1979. [3] [Ueda, Naonori, et al.] “Split and merge EM algorithm for improving Gaussian mixture density estimates.” Journal of VLSI signal processing systems for signal, image and video technology 26.1 (2000): 133-140. [4] [Troyanskaya, Olga, et al.] “Missing value estimation methods for DNA microarrays.” Bioinformatics 17.6 (2001): 520-525. [5] [Kim, Hyunsoo, Gene H. Golub, and Haesun Park.] “Missing value estimation for DNA microarray gene expression data: local least squares imputation.” Bioinformatics 21.2 (2005): 187-198. [6] [Ni, D., Leonard II, J.D.] “Markov chain Monte Carlo multiple imputation using Bayesian networks for incomplete intelligent transportation systems data”, Transp. Res. Rec., 2005, 1935, (1), pp. 57–67 [7] [Gilks, W.R., Richardson, S., Spiegelhalter, D.J.]”Markov chain Monte Carlo in practice” (Chapman &amp; Hall, London, 1996) [8] [Tipping, M.E., Bishop, C.M.]”Mixtures of probabilistic principalcomponent analyzers”, Neural Comput., 1999, 11, (2), pp. 443–482","categories":[{"name":"UCLA","slug":"UCLA","permalink":"https://umiao.github.io/categories/UCLA/"},{"name":"Course Study","slug":"UCLA/Course-Study","permalink":"https://umiao.github.io/categories/UCLA/Course-Study/"},{"name":"ECE209 in 2022 spring","slug":"UCLA/Course-Study/ECE209-in-2022-spring","permalink":"https://umiao.github.io/categories/UCLA/Course-Study/ECE209-in-2022-spring/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"UCLA","slug":"UCLA","permalink":"https://umiao.github.io/tags/UCLA/"}]},{"title":"SQL-Study-Note-8 - Data Type of MySQL","slug":"SQL-Study-Note-8","date":"2022-04-17T05:47:13.000Z","updated":"2022-04-20T22:19:31.193Z","comments":true,"path":"2022/04/16/SQL-Study-Note-8/","link":"","permalink":"https://umiao.github.io/2022/04/16/SQL-Study-Note-8/","excerpt":"Data Type of MySQL Suggestion for Data Type Selection","text":"Data Type of MySQL Suggestion for Data Type Selection VARCHAR: For short string set to 50, for long string set to 255. Maximum length is 65535, 64KB. (Note that Char Type is fix-lengthed). MEDIUMTEXT: 16M document &#x2F; LONGTEXT: 4GB document &#x2F; TINYTEXT: 255 Bytes &#x2F; TEXT: equal to VARCHAR, 64KB Note that Chinese charcter takes 3 Byte each, so we allocate $3n$ Bytes to string with length of $n$ (pick the upper bound). Integer Type: TINYINT: 1 Byte, UNSIGNED TINYINT and SMALLINT : 2 Byte, MEDIUMINT: 3 Bytes, INT: 4 Bytes, BIGINT: 8 Bytes. Leading zero filling is supported: INT(4) -&gt; ‘0003’ Float Type: DECIMAL(p,s) defines the int length and decimal length, it can be viewed as a fixed-point decimal (int). DECIMAL &#x3D; DEC &#x3D; NUMERIC &#x3D; FIXED FLOAT: 4 Bytes, DOUBLE: 8 Bytes (Expressed in exponential form) Boolean: BOOL &#x2F; BOOLEAN, 1 bit Enumerate Type: ENUM(‘a’, ‘b’, ‘c’): value must be selected from the given set. This is not a good design as it is complex to change the domain of legal values, you may even need to rebuild the entire table. It is not reusable itself. Creating a table to store the mapping relationship is recommended. Time: Timestamp can only store date up to 2038 AD as it takes 4 Bytes. To store later time, use Datastamp. BLOB for Binary Large Object: TINYBLOB &#x2F; BLOB &#x2F; MEDIUMBLOB &#x2F; LONGBLOB takes 255 Bytes &#x2F; 65KB &#x2F; 16 MB &#x2F; 4 GB, respectively. Store files in the FileSystem as possible rather than store them in the database. Otherwise, you may come into problems like high memory usage, slow copying, low performance, indirect and complicated IO, etc. JSON: In order to set JSON object, you can use string form like: ‘{ “k”:v}’. You can also create the object with function:.1JSON_OBJECT(&#x27;weight&#x27;, 10, &#x27;dimensions&#x27;, JSON_ARRAY(1, 2, 3)); In order to extract the attributes included in JSON, you can use 12345678JSON_EXTRACT(properties, ‘$.weight’);-- while the properties stand for the desired column name / key of the JSON object. JSON_EXTRACT(properties, ‘$.weight.data.sub.time’);-- You can use multiple dots to get the nested attributes.properties -&gt; ‘$.weight’; -- Semi-CPP syntax is also supportedproperties -&gt; ‘$.weight[idx]’;-- Can specify a certain element of the JSON list use &#x27;[]&#x27; At the same time, it should be noted that the returned results are still in JSON format. 1234properties -&gt; ‘$.weight’;-- would return something like &quot;sony&quot;, which leads to problem when comparing with other resultsproperties -&gt;&gt; ‘$.weight’;-- is able to return sony, without &quot;&quot; In terms of updating partial attributes, you can use JSON_SET. Here SET stands for the motion of setting. 123SET properties = JSON_SET / JSON_REMOVE(properties, ‘$.weight’, 30, ‘$.age’, 10) WHERE id=1-- JSON_REMOVE is used to remove attributes The above example can be used to set part of the attributes in JSON object properties.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"SQL-Study-Note-7 - Transactions","slug":"SQL-Study-Note-7","date":"2022-04-17T05:07:50.000Z","updated":"2022-04-17T05:47:05.601Z","comments":true,"path":"2022/04/16/SQL-Study-Note-7/","link":"","permalink":"https://umiao.github.io/2022/04/16/SQL-Study-Note-7/","excerpt":"Transactions Principles of Transaction (ACID) Atomicity Consistency Isolation Durability","text":"Transactions Principles of Transaction (ACID) Atomicity Consistency Isolation Durability 12345START TRANSACTION;// Instruction block to be executed-- If only part of the transaction is done and the connection to server is lost, the finished part would be rolled backCOMMIT; A transaction would lock the lines and tables to be updated so that they are untouchable to other transactions.If one transaction comes into locked resources, it would wait the owner of the lock to finish, or until it expires the time limit itself.At the same time, ROLLBACK is also a SQL instruction and keyword. Different level of transaction isolation: 12SET TRANSACTION ISOLATION LEVEL SERIALIZABLE;-- Note that this setting is session-leveled. It comes with the tradeoff that the higher the isolation level is, the lower performance it reaches. (In extreme situation, serializable level would not benefit from distributed architecture.) Read Uncommitted: May read uncommited data (dirty read). Rarely used in actual application as the performance improvement is very limited. Read Committed: Default level for most DBMS (Oracle, SQL server). However, it may counter Nonrepeatable Read: same select may have different result within one transaction. This is due to the UPDATE operation made by other transactions and can be solved by Line Level Lock. Repeatable Read: Applied with Line Level Lock. However, it can still encounter Phantom Reads (caused by the delete &#x2F; insert operations made by other transactions). Require Table Level Lock to solve. Serializable: Ban the parallel processing and sort all the transactions.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"SQL-Study-Note-6 - Trigger and Events","slug":"SQL-Study-Note-6","date":"2022-04-17T04:46:07.000Z","updated":"2022-04-17T05:05:21.886Z","comments":true,"path":"2022/04/16/SQL-Study-Note-6/","link":"","permalink":"https://umiao.github.io/2022/04/16/SQL-Study-Note-6/","excerpt":"TriggerTriggers are the code blocks executed automatically before insertion &#x2F; update &#x2F; delete take effect.","text":"TriggerTriggers are the code blocks executed automatically before insertion &#x2F; update &#x2F; delete take effect. 123456789DELIMITER $$CREATE TRIGGER payments_after_insert AFTER/BEFORE INSERT/UPDATE/DELETE ON payments FOR EACH ROWBEGIN -- You can either write SQL codes here or call the existing procedure...END $$DELIMITER ; Extensive syntax123NEW -- Return the just inserted line.OLD -- Return the just deleted line.NEW.amount -- Can use . to specify a column These two (NEW &#x2F; OLD) are keywords of MySQL.Triggers can be used to modify data of any table EXCEPT the table which is being listened by the trigger. This is because trigger can trigger itself and results in infinite loop. 12SHOW TRIGGERS -- Show all the created triggers.ShOW TRIGGERS LIKE &#x27;c%&#x27; -- Filter the triggers. Triggers can be also used for auditing purpose, i.e., record the executor, attribute and timestamp when an operation is done. EVENTEvents are periodically triggered codes for multiple tasks. 12SHOW VARIABLES -- Show all system variables.SET GLOBAL event_scheduler=ON / OFF -- Switch the event_scheduler Create Event: 12345678DELIMITER $$CREATE EVENT yearly_delete_state_audit_rows ON SCHEDULEEVERY 1 YEAR STARTS ‘2020-01-01’ ENDS ’2029-01-01’DO BEGIN -- Note that DO is required here....END $$DELIMITER ; Two ways to calculate the diff of time: 12NOW() – INTERVAL 1 YEAR;DATESUB(NOW(), INTERVAL 1 YEAR); Activate &#x2F; Deactivate Events: 1ALTER EVENT e_name DISABLE;","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"SQL-Study-Note-5 - Stored Procedure and User Defined Functions","slug":"SQL-Study-Note-5","date":"2022-04-17T02:01:32.000Z","updated":"2022-04-17T04:48:52.424Z","comments":true,"path":"2022/04/16/SQL-Study-Note-5/","link":"","permalink":"https://umiao.github.io/2022/04/16/SQL-Study-Note-5/","excerpt":"Stored ProcedureMotivationGenerally, developers prefer not to interpret string as SQL codes &#x2F; instructions due to security concerns.","text":"Stored ProcedureMotivationGenerally, developers prefer not to interpret string as SQL codes &#x2F; instructions due to security concerns. You can wrap the query and update functionality with Stored Procedure. DBMS is able to further optimize the stored procedure and enhance the security. The stored procedure itself is similar to a function implementation. Syntax Implementation123456DELIMITER $$CREATE PROCEDURE get_clients()BEGIN SELECT * FROM clients;END $$DELIMITER ; The reason of changing DELIMITER is that, we need to use ; to seperate SQL statements within the stored procedure. (We are forced to do so!)This will damage the integrity of our BEGIN-END statement block. Thus, we should temporarily change the DELIMITER (into $$) and change back to ; after the definition of our stored procedure. In order to avoid conflict of naming, MySQL would add a pair of &#96; to the names of databases, tables and columns. 1Student -&gt; `Student` -- Auto-Renamed to avoid conflicts. You can use 1CALL procedure_name() to call the defined stored procedure. If you are using the workbench of MySQL, you can simply right click Store Procedures to create one. In this case you do not need to worry about the delimiter and MySQL would help you with the transformation (Some sort of Syntactic sugar). Delete Stored Procedure1DROP PROCEDURE (IF EXISTS) get_clients; Parameter Setting of Stored Procedure12CREATE PROCEDURE get_clinets ( state CHAR(2) );-- You must specify the size and type of the passed parameter. After setting the parameter declaration, the parameters can be then used in the procedure for program logic building.All the parameters are REQUIRED. However, they can have default values.Even if you want to use the default value, you should pass a NULL to the procedure as a placeholder. 123456IF state IS NULL THEN SET state = ‘CA’; END IF;-- By using such statement, you can realize default value de facto.table.col_name = IFNULL(para, table.col_name);-- This one is more concise and thus recommended.-- If para is NULL, then use the default value. Parameter Verification and Constraints12345IF payment_amount &lt;= 0 THEN SIGNAL SQLSTATE &#x27;22003&#x27; SET MESSAGE_TEXT = ‘Invalid payment amount’;END IF-- payment_amount is required to &gt; 0. If not satisfied, a error code of &#x27;22003&#x27; is raised and the prompting MESSAGE_TEXT is written. SIGNAL is similar to throw exception in other languages. SQLSTATE is the predetermined error code (corresponding to ‘out of range’ here, refer the documentation). Provide Return Value for Stored Procedure1234CREATE PROCEDURE `name` (client_id INT, OUT invoices_count INT)BEGIN SELECT COUNT(*) INTO invoices_count FROM invoices;END This is a syntactic sugar anyway. What MySQL actually does is define two parameters and pass them to the PROCEDURE for updating. Then SELECT the updated parameter.At the same time, MySQL uses a ‘@’ prefix to identify variables. For an example, 1SET @a = 0; User Variable and Local VariableSET is used to assign the User Variable while DECLARE is used to assign the Local Variable used within a stored procedure. 1DECLARE risk_factor DECIMAL(9, 2) DEFAULT 0; You can assign the declared variable with the following code: 1SELECT COUNT(*) INTO risk_factor FROM table; User-Defined FunctionsThe only difference between the procedure and function is that function can only return one single value. 12345678CREATE FUNCTION get_risk_f ( client_id INT) -- Must specify the type for the input parameterRETURNS INTEGER -- The return parameter must have a type too. Function always returns a single value rather than a query result.DETERMINISTIC -- Optional attribute. Always return the same result for the same id.READS SQL DATA -- Optional. This function can read SQLMODIFIES SQL DATA -- Optional. This function can modify table.BEGIN...RETURN 1; Functions can be deleted with DROP keyword as well.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"SQL-Study-Note-4 - View","slug":"SQL-Study-Note-4","date":"2022-04-17T01:39:43.000Z","updated":"2022-04-17T02:00:31.084Z","comments":true,"path":"2022/04/16/SQL-Study-Note-4/","link":"","permalink":"https://umiao.github.io/2022/04/16/SQL-Study-Note-4/","excerpt":"ViewIntroduction to ViewWith the introduction of View, middle &#x2F; query results can be stored for further query and use, just like a real table.","text":"ViewIntroduction to ViewWith the introduction of View, middle &#x2F; query results can be stored for further query and use, just like a real table. Creation and Manipulation on View Creation of View: 1CREATE VIEW view_name AS (SELECT …) Created Views would NOT be stored with the tables. It would be stored in ‘Views’ instead. Alter &#x2F; Drop of View: 1234DROP VIEW sales_by_client;-- Drop / delete Operation.CREATE / REPLACE VIEW AS ...;-- REPLACE has the advantage against CREATE that it does not require the view to be dropped in advance. Created Views would NOT be stored with the tables. It would be stored in ‘Views’ instead. In fact, the Views should be viewed as stored query code. When you need to use them, you can simply rerun the script to retrieve the result. So you can use version control tools to store and share them. Updatable Views: Updatable Views stand for those Views who DO NOT contain the keywords of DISTINCT &#x2F; aggregate functions &#x2F; GROUP BY &#x2F; HAVING &#x2F; UNION. In this case, these Views can be updated via CREATE &#x2F; REPLACE. Update Opertion: 1UPDATE view_name SET due_date = DATE_ADD(due_date, INTERVAL 2 DAY) WHERE invoice_id = 1l Point of updatable views: If you DO NOT have the authorization to modify a table, you can still create a View based on that table and update the View, as long as it is Updatable. WITH CHECK OPTION: If the UPDATE operation may cause some rows to be deleted, you can add WITH CHECK OPTION to the end of the UPDATE code to prevent this from happening.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"SQL Study Note - 3 - Function and the Aggregate Function","slug":"SQL-Study-Note-3","date":"2022-04-17T01:34:31.000Z","updated":"2022-04-17T01:48:08.598Z","comments":true,"path":"2022/04/16/SQL-Study-Note-3/","link":"","permalink":"https://umiao.github.io/2022/04/16/SQL-Study-Note-3/","excerpt":"Function and the Aggregate Function","text":"Function and the Aggregate Function Aggregate Function: 1COUNT(), MAX(), MIN(), AVG(), SUM() It should be noted that COUNT(row_name) only returns the number of the non-empty records. If you need to find out the total number of rows, you shoud use COUNT(*). You can use COUNT (DISTINCT client_id) to find out the number of unique client_id, too. Non-Aggregate Function: 1234RAND() -- Generate a random number within (0, 1)RAND(seed) -- Specify the rand seedSQRT() -- Find the square root for each valueCONCAT(a, b) -- Concat two strings into one Non-aggregate function would return a same-lengthed sequence for the input values. These functions are element-wised. GROUP BY clause: Group the rows according to a given column name. Rows with the same value in the very column would be aggregated together. The order of query clause: IMPORTANT: SELECT -&gt; FROM -&gt; WHERE -&gt; ORDER BY Grouping based on multiple attributes: 1GROUP BY state, city In this case, the grouping would based on the tuple: (state, city) Having: 1SELECT SUM(res) AS aggregated_res FROM t GROUP BY state HAVING aggregated_res &gt; 100; You cannot use WHERE clause to filter the result of the aggreate function, because at that time, the query is not yet completed and the aggregate result is not yet calculated. It should be noted that HAVING clause supports the filtering on Composite Condition, e.g. HAVING aggre_res1 &gt; 10 AND aggre_res2 &lt; 10. The HAVING clause is execute after the query is finished. So that it can only filter the selected columns. The columns not selected cannot be used in the filtering constraint. HAVING does NOT support alias. WITH ROLLUP: 1SELECT SUM(res) AS aggregated_res FROM t GROUP BY state HAVING aggregated_res &gt; 100; Conduct an extra aggregate operation for all the aggregated results. E.g., the SUM &#x2F; MEAN of the aggregated results. This is only supported by MySQL. If composite grouping is applied, e.g., GROUP BY col_a, col_b, each group identified by a unique (col_a, col_b) would conduct an extra aggregation. Nesting Sub-query: 12SELECT * FROM (SELECT ...);SELECT * FROM table WHERE col_name IN / NOT IN (SELECT ...); I.E., select from the result of another select operation. ALL VS ANY &#x2F; SOME 1234SELECT * FROM invoices WHERE invoice_total &gt; ALL(SELECT invoice_total FROM invoices WHERE client_id = 3);-- Require the records to be greater than ANY of the sub-query results in order to be selectedSELECT * FROM invoices WHERE invoice_total &gt; ANY / SOME(SELECT invoice_total FROM invoices WHERE client_id = 3);-- Require the records to be greater than ONE of the sub-query results in order to be selected ANY &#x2F; SOME are completely equivalent. If one value of the sub-query satisfies the logical expression, the ‘&gt; ANY’ expression is satisfied. For ALL, only if all the values of the sub-query satisfies the logical expression, the ‘&gt; ALL’ expression can be satisfied. ‘&#x3D; ANY’ also equals to ‘IN’. Correlated sub-query Code-writing Order: SELECT, FROM, WHERE, GROUP BY, HAVING, ORDER BY Executing Order: FROM, WHERE, GROUP BY, HAVING, SELECT, ORDER BY The marked line is crucial, as the logic of correlated subquery would retrive one value from the main query at a time, working in an iterative manner, input the value into the sub-query to ge the result and pass the result back to the main query. The main query would check the constraint of WHERE clause and return the result; This means that different alias are required even they points to the same table. Sub-query can touch the alias of the main &#x2F; outer query. EXISTS 1SELECT * FROM c WHERE EXISTS (SELECT id FROM I WHERE c_id = c.c_id) EXISTS keyword has advantage against the ‘IN (sub-query)’ manner. This is because the ‘IN (sub-query)’ needs to finish the sub-query first before the sub-query can return any result to the outer query. However, EXISTS can work as a short-circuiting operator which stops immediately when the first results is found. Write Sub-query in SELECT &#x2F; FROM clause Use SELECT to duplicate the result of aggregate function1SELECT (SELECT AVG() FROM t) AS average FROM t; This query makes sense because, the aggregate function would only return one single value. If you want to conduct row-wise calculation on the aggregate value, you have to duplicate it. You can also SELECT FROM a sub-query like it is a real-table. However, an alias is required. This may make the query too complicated and should be used with care. Numerical Function123456789ROUND(num, mantissas_n); -- Round the given number, and mantissas_n decimal places are preserved.TRUNCATE(num, mantissas_n); -- Truncate the given number with the decimal setting.--Simiarly, we have:CEILING()FLOOR()ABS()RAND() Other available functions can be found in the MySQL documentation. String Function1234567891011121314151617181920LENGTH(&#x27;sky&#x27;); -- Return the length of the input string.UPPER(&#x27;sky&#x27;);--transform to upper caseLOWER(&#x27;sky&#x27;);--transform to lower caseLTRIM() / RTRIM() / TRIM();-- Remove the left / right / both side of spaces.LEFT(string, n);-- Return the left n chars;RIGHT(string, n);-- Return the right n chars;SUBSTRING(string, start, n);-- Return n chars starts from start.LOCATE(&#x27;n&#x27;, &#x27;kinter&#x27;);-- Find the smallest index where the pattern (&#x27;n&#x27;) occurs in the searched string (&#x27;kinter&#x27;). Index starts from 1.REPLACE(string, source, target);-- Replace all the pattern of source to target, in string.CONCAT(a, b);-- Concat strings into 1. Refer the documentation for more string functions. Time Function1234567891011121314NOW();-- Return the current time and date.CURDATE();-- Return the current date.CURTIME():-- Return the current time.YEAR(time) / MONTH() / DAY() / HOUR() / MINUTE() / SECOND() ...-- Extract the year... of a time / date. DAYNAME(time);-- Return the name of day, like FridayMONTHNAME(time);-- Return sth like DECEMBER...EXTRACT( YEAR FROM NOW() )-- Personalize a date / time.. Year can be substitute with other keywords E.g., extract order of this year: YEAR(date) &#x3D; YEAR(NOW()); Fomat Time &#x2F; Date123456789101112DATE_FORMAT(NOW(), &#x27;%y&#x27;);-- %y for 2-digit year, and %Y for 4-digit year;-- Similar for D/d/M/m, ...-- Can be use to format time as well.SELECT DATE_ADD( NOW(), INTERVAL 1 DAY );-- Return the date of 1 day later. Accept negative value like -1.-- Can Use DATE_SUB instead and the behavior is very similar.DATEDIFF(d_1, d_2);-- Find the diff of two dates. (by days, the input accepts DATE only)-- Result can be negative, calculated by d_1 - d_2.TIME_TO_SEC(time);-- Transform time to second, starts from 12 am. IFNULL &amp; COALESCE123SELECT order_id IFNULL(shipper_id, &#x27;Not Assigned&#x27;) AS s;-- If a shipper_id is NULL, return &#x27;Not Assigned&#x27; instead.COALESCE(shipper_id, comments, ..., &#x27;Not assigned&#x27;); The COALESCE is the generalization of IFNULL. The principle is, by offering a bunch of column_name &#x2F; values, return the first one which IS NOT NULL. IF &#x2F; CASE (Conditional Statement)1234567891011IF (expression, first, second);-- If the expression is TRUE, then return first; else, return second.SELECT order_id CASE WHEN YEAR(order_date) = YEAR(NOW()) THEN ‘Active’ WHEN YEAR(order_date) = YEAR(NOW()) - 1 THEN ‘Last Year’ ELSE ‘Other cases’END AS categoryFROM orders-- The case statement is quite similar with IF statement.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"SQL Study Note - 2 - The Update / Delete / Insert Syntax","slug":"SQL-Study-Note-2","date":"2022-04-17T01:29:50.000Z","updated":"2022-04-17T01:48:03.088Z","comments":true,"path":"2022/04/16/SQL-Study-Note-2/","link":"","permalink":"https://umiao.github.io/2022/04/16/SQL-Study-Note-2/","excerpt":"The update &#x2F; delete &#x2F; insert syntax","text":"The update &#x2F; delete &#x2F; insert syntax The column attributes of table: PK： primary key NN： Not Null UQ：Unique Index B： binary UN： unsigned data type ZF： zero filled AI： Auto incremental G： Generated column Data Type in MySQL: INT (11): integer with a length of 11 VARCHAR(50): array of char with a size $\\le 50$. CHAR(50): array of char with a size $&#x3D; 50$. DEFAULT: Use the given default value to fill this column. Insert data to table: 1INSERT INTO customers VALUES (DEFAULT, ‘John’, ‘Smith’, ‘1990-01-01’, NULL, ‘address’, ‘city’, ‘CA’, DEFAULT); At the same time, MySQL allows to specify the column names to be assigned values. 1INSERT INTO customers (first_name, …, state, points) VALUES (…); In this case, you do not need to assign the values to the order (of columns) defined by the table. The number of affected rows would be returned after successful insertion. Insert Multi-rows: 1INSERT INTO shippers (name) VALUES (&#x27;S1&#x27;), (&#x27;S2&#x27;), (&#x27;S3&#x27;); LAST_INSERT_ID: Returns the most recently generated Auto Incremental ID. Enable hierarchical data insertion. I.E., find the ID of the latest inserted data, and then use the id to associate &#x2F; update other tables. This syntax feature can eliminate ambiguity, and it is also convenient to correspond a main table record to multiple sub table records. Duplicate Table: 1CREATE TABLE orders_archived AS SELECT * FROM orders; Use the selected partial &#x2F; entire data of other table to create a duplicate. However, column attributes (constraints) like PK, AI would be ignored. Batch Insertion with Select: 1INSERT INTO orders_archived SELECT * FROM orders WHERE order_date &lt; ’2019-01-01’; Column attributes (constraints) like PK, AI would be ignored as well. Truncate Table in workbench: Right click a table and select ‘truncate table’ would remove all the data (records) but would not remove the table itself. Update a single row: 1UPDATE invoices SET payment_total = 10, payment_date=’2019-03-01’ WHERE invoice_id = 1; Filter the results (single line) that meet the conditions and update the filtered results. It is also allowed to use default, arithmetic expression, etc. as the new value of the selected row. Note that even if multiple statements are selected for your filter criteria, MySQL workbench runs in the security update mode by default, allowing you to update only one row at a time. However, in other environments, there is no such problem. You can drag it to the bottom of SQL editor and choose to uncheck the safe updates option. After changing the settings, you need to reconnect for the settings to take effect. WHERE attribute IN (1,2,3) can be used to filter multiple records. Use SELECT to UPDATE (apply sub-query in UPDATE): 1UPDATE invoices SET payment_total = 10, payment_date=’2019-03-01’ WHERE client_id = ( SELECT client_id FROM clients WHERE name=’Myworks’ ); In this manner, a single logical judgement is replaced with a sub-query to update multiple values at a time. If multiple values would be returned in your sub-query, the WHERE clause should be changed to WHERE client_id IN (sub-query). You should test the sub-query before update the table. You CANNOT update the same table where you conduct your sub-query —- If you have to do so, create a duplicate and give it an alias. DELETE:1DELETE FROM invoices WHERE (invoice_id=1); Obviously, the condition within the parenthesis can be a sub-query, too. Rebuild the Database:12DROP DB If EXISTS DB;-- Conduct the script of DB building then.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"SQL Study Note - 1 - Syntax Basics","slug":"SQL-Study-Note-1","date":"2022-04-16T21:05:53.000Z","updated":"2022-04-17T01:31:33.088Z","comments":true,"path":"2022/04/16/SQL-Study-Note-1/","link":"","permalink":"https://umiao.github.io/2022/04/16/SQL-Study-Note-1/","excerpt":"Overview The core of database system is to interact with DB (DataBase) with DBMS (Database Management System).","text":"Overview The core of database system is to interact with DB (DataBase) with DBMS (Database Management System). The DB can be generally divided into: Relational DB NoSQL DB (e.g., KV (key-value) based DB) Recommended code style of SQL (Structed Query Language): Capitalize all keywords and reserved words, and lowercase all other contents. Each statement should end with ;. Keywords and Syntax RulesThe query syntax SHOW DATABASES 1SHOW DATABASES; List all the names of exisitng databases (under the current schema). USE 1234USE database_name;SELECT * from table;SELECT * from database_name.table; -- Must specify the database_name for the DBs which are not in use Select one DB as the default one. SELECT 1SELECT (column_name) FROM (table_name) WHERE (condition) ORDER BY (col_name). Select the desired data from a table. It should be noted that you should use &#x3D; in SQL to determine equivalence. (It DOES NOT mean assignment.) You can conduct calculation on the selected result. AS 1SELECT (column_name) FROM (table_name) WHERE (condition) ORDER BY (col_name). For each col_name, table_name to be queried and the queried results, you can always to set alias (surname) for them with AS. – &#x2F; comment 12-- This a comment.// This is also acceptable. DISTINCT 1SELECT DISTINCT column_name FROM t Add DISTINCT ahead of the queried target (column) to receive all the unique values. AND &#x2F; OR &#x2F; NOT 1SELECT * FROM t WHERE col_a &gt; 10 and col_b = &#x27;CA&#x27; Logical operator to be used with the WHERE clause. IN 1SELECT * FROM t WHERE col_a IN (&#x27;A&#x27;, &#x27;B&#x27;) Determine if the queried value belongs to a set. BETWEEN 123SELECT * FROM t WHERE point BETWEEN 100 AND 300;SELECT * FROM t WHERE point &gt;= 100 AND point &lt;= 300;-- These two are equivalent Determine if the queried value within a given interval. Both ends of the interval are closed ([beg, end]). LIKE12SELECT * FROM t WHERE name like &#x27;b%&#x27;-- Able to match &#x27;Bob&#x27; and &#x27;bike&#x27; Provide functionality similar to Regular Expression. % can match arbitrary string, _ can match arbitrary char. Not sensitive to the case. REGEXP1SELECT * FROM t WHERE name REGEX &#x27;^f[a-z]+d&#x27; Match a given Regular Expression pattern. IS NULL12SELECT * FROM t WHERE name IS NULL;SELECT * FROM t WHERE name IS NOT NULL; Query all the records which are (not) null in a given column. ORDER BY1SELECT * FROM t ORDER BY col_name DESC / AESC Decide the sorting order of the returned records. LIMIT12SELECT * FROM t LIMIT offset, tot_numSELECT * FROM t LIMIT tot_num Restrict the number of the returned records. Skip the first $n&#x3D;$offset records and then return tot_num lines of records. Return all the records if number of matched records fewer than tot_num. INNER JOIN123SELECT * FROM t_a JOIN t_b on t_a.col_1 = t_b.col_2;SELECT * FROM t_a JOIN t_b on t_a.col_1 = DB_2.t_b.col_2;-- You can conduct crossed-DB join by specifying the name of the DB not in use. Can be simply writtene as JOIN. Concat two tables based on the given condition. Conduct Cartesian product implicitly. SELF JOIN1SELECT * FROM t_a AS a JOIN t_a AS b on a.col_1 = b.col_2; A table can join with itself, but different alias are required. Multi-table JOIN1SELECT * FROM t_a JOIN t_b on a.col_1 = b.col_2 JOIN t_c on b.col_2 = c.col_3; Multiple (n) tables can be joined but this is not recommended when $ n &gt; 3 $ due to performance concern. Composite JOIN &#x2F; Implicit JOIN1SELECT * FROM t_a JOIN t_b on a.col_1 = b.col_2 JOIN t_c on b.col_2 = c.col_3; Sometimes only a tuple of multiple attributes can uniquely identify a row of the table. In this case, these attributes become Composite Primary Key. 1SELECT * FROM t_a, t_b; Implicit JOIN is conducted in the above example but this is not recommended, too. OUTER JOIN1SELECT * FROM t_a AS a JOIN t_b AS b on a.col_1 = b.col_2 JOIN t_c AS c on b.col_2 = c.col_3; When using INNER JOIN, some records of the left table cannot match with the right table because the condition of the ON clause is not satisfied. However, if we want to return all the records of the left (right) table regardless of the boolean value of the ON clause, we can use LEFT (RIGHT) OUTER JOIN. You should use RIGHT JOIN instead of LEFT JOIN as possible. SELF OUTER JOIN is similar, and alias is still required. USING123SELECT * FROM t_a JOIN t_b on t_a.col_1 = t_b.col_1;SELECT * FROM t_a JOIN t_b USING (col_1);-- These two are equivalent. Can be used to simplify the code, if the column names of the to-be-joined tables are exactly the same. You can Join on a tuple like ‘USING (id1, id2, id3)’ and these column names should be exactly the same as well (in the two tables). NATURAL JOIN1SELECT * FROM t_a NATURAL JOIN t_b; Let the compiler (DBMS) to decide the way of join. Not recommended to use! CROSS JOIN1SELECT * FROM t_a CROSS JOIN t_b; Conduct Cartesian Product. UNION1SELECT * FROM a UNION SELECT * FROM b; Concatenate multiple queried results together (on the direction of row). The column names should be exactly the same. IT SHOULD BE NOTED that ORDER BY can be set only once, so union all the results before setting the ORDER clause.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]}],"categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"},{"name":"General Knowledge","slug":"Data-Science/General-Knowledge","permalink":"https://umiao.github.io/categories/Data-Science/General-Knowledge/"},{"name":"UCLA","slug":"UCLA","permalink":"https://umiao.github.io/categories/UCLA/"},{"name":"Course Study","slug":"UCLA/Course-Study","permalink":"https://umiao.github.io/categories/UCLA/Course-Study/"},{"name":"ECE209 in 2022 spring","slug":"UCLA/Course-Study/ECE209-in-2022-spring","permalink":"https://umiao.github.io/categories/UCLA/Course-Study/ECE209-in-2022-spring/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://umiao.github.io/tags/Machine-Learning/"},{"name":"Random Forest","slug":"Random-Forest","permalink":"https://umiao.github.io/tags/Random-Forest/"},{"name":"Regularization","slug":"Regularization","permalink":"https://umiao.github.io/tags/Regularization/"},{"name":"Naive Bayes","slug":"Naive-Bayes","permalink":"https://umiao.github.io/tags/Naive-Bayes/"},{"name":"UCLA","slug":"UCLA","permalink":"https://umiao.github.io/tags/UCLA/"},{"name":"SVM","slug":"SVM","permalink":"https://umiao.github.io/tags/SVM/"}]}