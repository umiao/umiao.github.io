{"meta":{"title":"The blog of Blur","subtitle":"Hearing the fall of snow.","description":"Personal blog of blur.","author":"Blur - Shenghui Xu","url":"https://umiao.github.io","root":"/"},"pages":[{"title":"About Me","date":"2022-04-16T17:45:16.000Z","updated":"2022-06-17T03:08:39.592Z","comments":true,"path":"about/index.html","permalink":"https://umiao.github.io/about/index.html","excerpt":"","text":"Who am i This is Shenghui Xu and welcome to my blog. I am currently a year-one MS student with the Electrical Computer Engineering department of University of California, Los Angeles. I am now focusing on the track of Signals &amp; Systems and have a GPA of 4.0. I am also an incoming applied researcher intern at ebay. Skills and Tools Languages Python &#x2F; Matlab &#x2F; R &#x2F; Java &#x2F; JavaScript &#x2F; C++ Bash &#x2F; CMD &#x2F; Code Climate Frameworks TensorFlow &#x2F; Torch &#x2F; Keras &#x2F; Theano &#x2F; CUDA &#x2F; CUDNN Scikit-learn &#x2F; nltk &#x2F; Numpy &#x2F; Spark (PySpark) Qt Web HTML&#x2F;HTML5 &#x2F; CSS &#x2F; Node.js jQuery &#x2F; Vue.js &#x2F; quasar Tornado &#x2F; Chrome Dev Tools Editor &amp; IDE VIM &#x2F; Sublime Text &#x2F; Notepad++ &#x2F; Lime Text Visual Studio &#x2F; Qtcreator &#x2F; Pycharm &#x2F; Eclipse Version Control &amp; Deployment Git &#x2F; SVN &#x2F; Github &#x2F; GitLab &#x2F; Gitee &#x2F; Anaconda &#x2F; npm Testing Jenkins &#x2F; Lint &#x2F; Pytest &#x2F; Docker &#x2F; Unit Testing Data Management MySQL &#x2F; MongoDB &#x2F; Redis &#x2F; Memcached Projects Experiences"}],"posts":[{"title":"Designing Data-Intensive-Applications-Note-6","slug":"Designing-Data-Intensive-Applications-Note-6","date":"2024-02-24T17:34:35.000Z","updated":"2024-02-24T17:39:45.363Z","comments":true,"path":"2024/02/24/Designing-Data-Intensive-Applications-Note-6/","link":"","permalink":"https://umiao.github.io/2024/02/24/Designing-Data-Intensive-Applications-Note-6/","excerpt":"Discussion on Indexes and R-Tree.","text":"Discussion on Indexes and R-Tree. Secondary Index The mapping may not be unique, i.e., we may have multiple rows mapped to the same key in secondary index. This can be solved by either[1] making each value in that key to be list of matching row identifiers, or by[2] making each key unique (adding a row identifer to the key). May help with Join. Storing values within the indexIn index, the key can just be the search keyword, but the value can be:[1] Actual row (or document &#x2F; vertex) in question[2] Reference to the row stored elsewhere(That file is called heap file, in no particular order). Heap file helps: eliminating duplication, because if there lies multiple secondary index, only the reference of data is stored; efficient at overwriting values, if new value is no larger than the old value. Otherwise, the new value needs to be moved to somewhere with enough space. In that case, we can either update all secondary indexes to be updated to point to the new location, or leave a forwarding pointer in place. Clustered index: if extra hop from index to heap file is too much of performance penalty for reads, we can directly store indexed row within index.E.g., In MySQL’s InnoDB engine, the primary key of a table is always a clustered index and secondary indexes refer to the primary key. In SQL server, you can specify one clustered index per table. Compromise between clustered &#x2F; non-clustered index: covering index or index with included columns, only stores some of the columns within the index. Both cluster and covering index can speed up the reads but require more storage &#x2F; write overhead.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Data System","slug":"Data-Science/Data-System","permalink":"https://umiao.github.io/categories/Data-Science/Data-System/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Data System","slug":"Data-System","permalink":"https://umiao.github.io/tags/Data-System/"}]},{"title":"Designing Data-Intensive-Applications-Note-5","slug":"Designing-Data-Intensive-Applications-Note-5","date":"2024-02-24T08:38:40.000Z","updated":"2024-02-24T17:35:50.668Z","comments":true,"path":"2024/02/24/Designing-Data-Intensive-Applications-Note-5/","link":"","permalink":"https://umiao.github.io/2024/02/24/Designing-Data-Intensive-Applications-Note-5/","excerpt":"Discussion on Storage structures.","text":"Discussion on Storage structures. Storage and Retrieval:Real life DB may need concurrency control, reclaiming disk space so that log does not grow forever, handling errors and partially written records. Log: an append-only sequence of records. Index: additional structure that is derived from the primary data (faster read, slower write) Hash Indexes: basic implementation is to use in memory hash map to store byte offset (then you can generalize memory hash map into disk hash). Log run out of space:To cope with such risk, we can: Break log into segments of a certain size, Closing a segment file when it reaches certain size Make subsequent writes to new segment file.We can do compaction on segments to keep only the most recent data (this may even allow us to compress multiple segments into one). Good Practice in storage: Store file as binary format, first encode length of a string in bytes, followed by the raw string Consider tombstone (special delete record) to avoid really deleting records (“delete” execute on read time). Crash recovery: in memory cache will lost, we can consider storing a snapshot of each segment’s hash map on disk, which can be loaded into memory more quickly. Partially written result: we can keep checksums to detect corrupted parts. Concurrency control: we need the log to be strictly ordered, so better have one writer thread only. Reason of Append Only Design: Appending and segment merging are sequential write operations, which are generally much faster than random writes, especially on magnetic spinning-disk hard drivesor even flash-based solid state drives (SSDs).Crash recovery will also be easier as values will not be partially overwritten.Also we can avoid fragmented data file. Shortcomings of hash table index: It relies on memory, and performs poor on range queries. You have to go through all keys. SSTables and LSM-Trees: If we further require writes of segment files to be sequential (sorted by key), then it is a Sorted String Table (SSTable). Advantages over hash indexes: Easier to merge segments as they are ordered (applicable to merge sort). Tiebreak: if multiple keys exist, we only keep the latest record. (similar as the idea of using timestamp &#x2F; ItemID as additional rank keyword) Sparse Index: Now that we do not need to keep all keys’ indexes, as we can use similar idea of interpolation. As the storage is now ordered, we only need to find out the interval where a key might exist. Maybe we can keep an index every few KBs (like paging). Also it is possible to compress multiple keys into a compressed block. To save disk space &#x2F; IO cost. Maintenance: it is possible to maintain SSTables in disk (using B-Trees) but it is easier using memory (called memtable) with red-black trees &#x2F; AVL trees. Memtable When memtable gets bigger than threshold (a few MBs), write it to disk as an SSTable file, and the new SSTable file becomes the most recent segment of database. While writing to disk, writes can continue to a new memtable instance. When serving reads, try to find key in the memtable, then in reversed order to check on-disk segments (check the latest ones first) Run merging and compaction process from time to time to merge segment files, and remove out-dated values. Recovery of memtable: memory is not persistent, so keep a separate log on disk to record every write to restore crashed memtable (discard records after such restoration). LSM-Tree: details and optimization: MemTable resides in memory, is NOT persistent, and ordered by key. Can enhance reliability by Write-Ahead Logging (WAL). Memtable turn into Immutable Memtable after reaching certain size, and no longer writable. New memtable will handle write request, and Immutable Memtable is pending transit to SSTable. SSTable is LSM-Tree’s data structure in disk. Can be optimized using bloom-filter and indexes on key. Note that the updating of disk is journal &#x2F; log like, and the write is also in order. The write performance is improved, however there lies redundancy due to out-dated data (that is why we need compact). Also query needs to be in reverse order as that is the latest. Compact Strategy:Because we have amplified read &#x2F; write &#x2F; space, that is to say, we are reading (two level read of MemTable &#x2F; SSTable) &#x2F; writing (Compact may be triggered so we are writing more) &#x2F; storing (due to redundancy) more data than actually needed. Size-tiered: Each tier of SSTable has similar size, and restrict number (N) of SSTable in each tier. If the number of SSTables reach N, then compact them and pass to next tier.(This will result in huge SSTable in deeper level)(Also for same tier SSTables, each key may has multiple records) Leveled: Each level restricts the gross size of SSTables, and in the same level, slice data into SSTables with similar sizes. However, the SSTables are guranteed to be globally ordered so only stored once.Strategy: if a level exceeds the limit on size, then take out 1 SSTable, merge it with next level SSTables which have intersection. (This process can be recursive if still exceeds the size limit) Make LSM-tree out of SSTables: This is used by key-value storage engine like LevelDB and RocksDB. Known as Log-Structured Merge-Tree (LSM-tree). Instance: Lucene (mapping from term to postings kept in SStable-like sorted files), is an indexing engine for full-text search used by Elasticsearch and Solr. It uses a similar method for storing its term dictionary.The mentioned full-textr search engine actually stores all links related to a keyword as key, with list of resources as the value. B-Tree Standard index implementation in almost all relational databases Instead of break database into variable-size segments and always write sequentially, B-Tree break into fixed-size blocks &#x2F; pages (traditionally 4KB) and read or write one page at a time. It is more close to hardware, and each page can be identified using an address, stored on disk.Implementation Starting from the root, keep going down to narrow the range of indexes and eventually check existence. The number of references to child pages in one page of the B-tree is called the branching factor, usually a several hundred. To update a key, search for leaf page containing that key and write back. To add new key, find the page whose range encompasses the new key and add it to the page. If there is not enough space, split the page into 2 half-full pages. This ensures that the tree is balanced, has a depth of $O(\\log n)$. Most database can fit in a depth of 3 – 4. (A four-level tree of 4 KB pages with a branching factor of 500 can store up to 256 TB) Comparing with LSM-Tree: B-Tree’s rewrite is in-place and can be viewed as happening at hardware level, rather than on log level.Robustness If you split a page because an insertion caused it to be overfull, you need to write the two pages that were split, and also overwrite their parent page to update the references to the two child pages. This is a dangerous operation, because if the database crashes after only some of the pages have been written, you end up with a corrupted index (e.g., there may be an orphan page that is not a child of any parent). Solution: Include an additional data structure of Write-Ahead Log (WAL) (redo log), which is append-only file which records every B-tree modification before it applied to the pages. This can restore B-Tree to a consistent state. Also, if multiple threads want to access B-Tree, we will need lightweight locks for consistency, which is more complex than log-based structure. Optimizations Can replace WAL with a copy-on-write scheme. That is a modified page is written to a different location, and a new version of the parent pages in the tree is created to point at the new location. This is also useful for concurency control. We can save space in pages by abbreviating the keys, as we only need enough information to act as boundaries between key ranges (e.g., omit common prefixs). Thus we can have higher branching factor and fewer levels. We can try to layout the tree that leaf pages appear in sequential order in disk to save disk seeking time. However, it is difficult. For LSM-trees, such locality is easier as it rewrites large segments during merging. Comparison with LSM-Tree: LSM-tree is faster in write, and B-Tree is faster in read. B-tree must do write twice: once to WAL (Write-Ahead Log) and once to the tree page itself (need more when split is needed). Need to change a page at minimum. Some storage engines even overwrite the same page twice in order to avoid ending up with a partially updated page in the event of a power failure. Log-structured indexes also write data multi-times due to repeated compaction &amp; merging (write amplification), impacting SSD; disk bandwidth is also limited and may cause fragmentation. LSM-tree usually has smaller write amplification as it do not have to overwrite several pages in the tree. Also, magnetic hard drive’s sequential writes is much faster than random writes, giving LSM-tree more advantage.However, the compaction can impact read-write performance (due to limited disk bandwidth), even storage engines try to perform compaction incrementally and without affecting concurrent access.Initial write and compaction will share the same bandwidth. As database become larger, the compaction is less effective and may be the bottleneck of performance &#x2F; use up the disk space (write is usually not restricted in SSTable-based storage).For high-percentile, LSM-tree can have high response time and less predictable. Many SSD firmware will use log-structured algo to turn random writes into sequential write.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Data System","slug":"Data-Science/Data-System","permalink":"https://umiao.github.io/categories/Data-Science/Data-System/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Data System","slug":"Data-System","permalink":"https://umiao.github.io/tags/Data-System/"}]},{"title":"Designing Data-Intensive-Applications-Note-4","slug":"Designing-Data-Intensive-Applications-Note-4","date":"2024-02-24T00:38:51.000Z","updated":"2024-02-24T08:24:36.660Z","comments":true,"path":"2024/02/23/Designing-Data-Intensive-Applications-Note-4/","link":"","permalink":"https://umiao.github.io/2024/02/23/Designing-Data-Intensive-Applications-Note-4/","excerpt":"Discussion on data model and query language.","text":"Discussion on data model and query language. Concept of Data Model &#x2F; Query Language:We have the questions to answer:- We want to build abstraction, one data model layer on top of another, but we need to know how one layer is represented by below layer.- Data modeling &#x2F; API assignment. How to model the data, JSON &#x2F; XML? Table in DB? Graph Model? Relational Model v.s. Document Model: Relational: Data organized into relations (called tables in SQL) and each relation is an unordered collection of tuples (rows in SQL). Hide details behind cleaner interface. Document: NoSQL, retrieve document by name, each stores information of one object and any of its related metadata.Examples: JSON, BSON, XML.A collection is a group of documents. Driving Forces behind NoSQL (Maybe not only SQL is more accurate):- Greater scalability, like very large dataset or very high write throughput- Preference for free and open source software over commercial database products- Specialized query operations not well supported by relational DB- Frustration with the restrictiveness of relational schemas, and a desire for a more dynamic and expressive data model Shortcomings of document model: In document model, you cannot directly refer to some nested items (because it is not stored independently). Thus we should not use it when document structure is too complex &#x2F; nesting is too deep. Also it has poor support for Join. Object-Relational Mismatch:Usually, an awkward translation layer is required between the objects in the Relational Model (tables) Versus Document Model (sometimes named as impedance mismatch) Object-Relational Mapping (ORM) like ActiveRecord and Hibernate can reduce the amount of boilerplate code, but the difference still exists (such mismatch) ID can out-perform pure string in Many-to-One &#x2F; Many-to-Many mappings.Store fields as IDs and retrieve their values can help:1. Consistent styling2. Avoid ambiguity3. Ease of updating4. Localization support5. Better search matching Both stroing by ID &#x2F; storing by string has some sort of redundancy. The previous is for human, meaningful info is stored in just one place and everything refers to it use an meaningless ID. For string storage, you are copying the same values (might consume more storage).However, the benefit using ID is that you do not have to update ID, and can better enforce consistency when update is needed. In document model DB, such unifying can be hard as you may need to query DB for multiple times (Good at one-many, not good at many-one, many-many). Conference on Data Systems Languages (CODASYL) and the network model: Disclaimer: This may be something old and just material for fun Allowing an item to have multiple parents to model many-one &#x2F; one-many The link is more like pointers (static pointers stored on the disk), follow a path to traverse (start from root record) [This may be problematic in many-many cases, you need to maintain complex paths of data to update] Relational Model: Relation (table) is simply a collection of tuples (rows), you can read any of the rows and select those matching your condition. You can insert new row into table without thinking of foreign key issues (this is done in query time not in insert time) Key insight: For relational DB, you only need to build a query optimizer once, and then all applications that use the database can benefit from it.If you don’t have a query optimizer, it’s easier to hand-code the access paths (a.k.a the network model) for a particular query than to write a general-purpose optimizer—but the general-purpose solution wins in the long run. Document model: Different from network model, as it uses unique identifier (foreign key in relational, document reference in documental) to link. It is possible to use denormalization (add redundant copies, grouping data, etc) to reduce need of join (in relational DB), but it takes extra work to enforce consistency. Schemaless &#x2F; Schema flexibility in Document Model In JSON, usually we do not support enforcing of any schema (there may be some schema, but not enforced by the DB). We may see any value &#x2F; any key. schema-on-read: the structure of the data is implicit, and only interpreted when the data is read. Just like dynamic type checking rather than static type chekcing. This can be tricky for MySQL, as it copies the entire table when doing alter table. Data Locality of queries May grant performance advantage if you want to access entire document frequently.(If in multiple tables, then may need more disk seek time) Unless the modification does not change the encoded size, we usually need to rewrite entire document when making modifications. (thus should try to keep documents small) This idea is beyond document database, also applicable to relational DB. Google’s Spanner database: offers the same locality properties in a relational data model, by allowing the schema to declare that a table’s rows should be interleaved (nested) within a parent table. Oracle: Offers the same using a feature called multi-table index cluster tables. The column-family concept in the Bigtable data model (used in Cassandra and HBase) has a similar purpose of managing locality. More and more relational DB is adding support to JSON (allowing values to be nested obj, rather than only trivial values). Query Languages for Data:Declarative SQL is regarded as declarative. Just specify the pattern of result data you want, what conditions to meet, but not about how to receive this result. Usually order is not guaranteed as well, but it is good for parallelization Details are hidden, and optimization can be done without modifying the query Imperative IMS &#x2F; CODASYL query are using imperative code. Guide device to do certain operations in certain order. This can be harder to optimize &#x2F; rewrite on multiple machines HTML is declarative as well. Document Object Model (DOM) API is imperative instead (a bad rewrite using DOM API can be less interactive as it detect conditions &amp; runs only one time). Map-Reduce Map-Reduce can be in between of declarative and imperative, consisting of important operations of map (collect) and reduce (fold &#x2F; inject). Usually map func provides the partial result, and reduce func works on aggregation. Restrictions: they have to be “pure” functions, no additional queries can be done, no side effect should be made (so that they can be executed in any time &#x2F; any order). Graph-Like Data Model: Describe data with Vertices and edges. It is good for, e.g., describing social network and hierarchical relationships. Property Graphs: Each vertex consists a unique identifier, a set of outgoing edges, a set of incoming edges, a collection of properties (K-V pairs). Each edge consists of a unique identifier, the vertex which the edge starts, a label to describe the kind of relationship between the two vertices, a collection of properties (K-V pairs). Any two vertices can be linked; You can find the incoming &#x2F; outgoing edges for a vertex; You can use labels for different kinds of relationships, store different kind of info in a single graph. This actually allow us to store data in different granular. Cypher Query Language:Demos an example of creation of a property graph: Example of query: Such query effectively specify an endpoint &#x2F; destination vertex, and we want to find a path to get there. We first declare a person should have a “born_in” edge, then we want to keep tracking “within” edge to reach certain country &#x2F; address vertex. Application in SQL: This can be difficult as the number of “join”s needed is not clear. You can use RECURSIVE syntax (used to enlarge the same table &#x2F; set) for such purpose (but very complex and ineffective). Triple-Stores: Basically the same as property graph model, just using different words of: (subject, predicate, object)This just like (in_vertex, directional_edge, out_vertex) SPARQLA query language for triple-stores using the RDF (Resource Description Framework) data model.It looks like: DatalogSimilar triple-stores languages, write as predicate(subject, object).See example of decalration as well as query below: It can also recursively find a person who born in USA and lives in Europe.Note that in above example, rule “within_recursive” can derive itself. (You can view the first line of row as “tail recursive”)","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Data System","slug":"Data-Science/Data-System","permalink":"https://umiao.github.io/categories/Data-Science/Data-System/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Data System","slug":"Data-System","permalink":"https://umiao.github.io/tags/Data-System/"}]},{"title":"Designing Data-Intensive-Applications-Note-3","slug":"Designing-Data-Intensive-Applications-Note-3","date":"2024-02-24T00:28:51.000Z","updated":"2024-02-24T00:40:36.164Z","comments":true,"path":"2024/02/23/Designing-Data-Intensive-Applications-Note-3/","link":"","permalink":"https://umiao.github.io/2024/02/23/Designing-Data-Intensive-Applications-Note-3/","excerpt":"Discussion on maintainability, evolvability and operability.","text":"Discussion on maintainability, evolvability and operability. Maintainability Fixing bugs Keeping its systems operational Investigating failures Adapting it to new platforms Modifying it for new use cases Repaying technical debt Adding new features Try to focus on the following 3 design principles: OperabilityMake it easy for operations teams to keep the system running smoothly. Duties of operations teams include: Monitor health Track issue &amp; cause Update software and platform Track how systems interfere each other Manage deployment and configuration Anticipate and resolve future problems Perform complex maintenance System security Define process Preserve org-level knowledge about sys Examples of improving maintainability: Provide visibility into runtime behavior and internal of system Automation and integration Avoid dependency on individual machines Docs and operational models Good default behavior Self-healing of sys Exhibit predictable behavior. SimplicityMake it easy for new engineers to understand the system, by removing as muchcomplexity as possible from the system. Examples of complexity: Explosion of the state space, Tight coupling of modules, Tangled dependencies, Inconsistent naming and terminology, Hacks aimed at solving performance problems, Special-casing to work around issues Best Resolving practice: Introduce abstraction to hide details. Extract data system into well-defined, reusable components. Note this is not the same as simplicity of the user interface. EvolvabilityMake it easy for engineers to make changes to the system in the future, adapting it for unanticipated use cases as requirements change.Also known as extensibility, modifiability, or plasticity. Examples of Evolvability needs:- You learn new facts,- Previously unanticipated use cases emerge,- Business priorities change,- Users request new features,- New platforms replace old platforms,- Legal or regulatory requirements- Change, growth of the system forces architectural changes","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Data System","slug":"Data-Science/Data-System","permalink":"https://umiao.github.io/categories/Data-Science/Data-System/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Data System","slug":"Data-System","permalink":"https://umiao.github.io/tags/Data-System/"}]},{"title":"Designing Data-Intensive-Applications-Note-2","slug":"Designing-Data-Intensive-Applications-Note-2","date":"2024-02-24T00:01:35.000Z","updated":"2024-02-24T00:19:28.466Z","comments":true,"path":"2024/02/23/Designing-Data-Intensive-Applications-Note-2/","link":"","permalink":"https://umiao.github.io/2024/02/23/Designing-Data-Intensive-Applications-Note-2/","excerpt":"Topics about scalability, load &amp; press. Definition, metrics and mitigation.","text":"Topics about scalability, load &amp; press. Definition, metrics and mitigation. Scalability Systems can go wrong, with larger load and press. Examples of loads: requests per second to a web server ratio of reads to writes in a database number of simultaneously active users in a chat room Hit rate on a cache. Example: Twitter is more frequently read than write, so rather than the first version of: (when a user checking timeline, look up all the people they follow, merge their tweets), we can update everyone’s timeline when a post is created (version 2). Concern: number of followers is highly biased, and some head users’s posting can cause a lot of writes. Mixed solution: for celebrities, read their posts when creating timeline, and then merge with the preprocessed family tweets. Describe PerformanceWays to describe a system’s performance:1. Increase load param, so that system resources unchanged.2. Increase load param, measure extra sys resources needed to keep performance unchanged.Scenarios: In Hadoop, we care more about throughput. In online sys, we care more about response time. Latency: duration of a request waiting to be handled. Response time: client’s view, end to end time including network delays and queueing delays.Simply averaging these metrics is NOT good due to existence of outliers. Better checking percentiles.High percentiles (aka tail latencies) are important to user’s experience. Service Level Objectives v.s. Service Level AgreementsService Level Objectives (SLO) and Service Level Agreements (SLA) are contracts defining the expected performance and availability of a service. E.g., require median response time &lt; 200ms, P99 latency &lt; 1s (SLO), available for 99.9% of time (SLA). Random FactorsRandom factors which may cause same response taking different time:Random additional latency could be introduced by: Context switch to a background process. The loss of a network packet and TCP retransmission. Garbage collection pause. Page fault forcing a read from disk. Mechanical vibrations in the server rack. Queueing delayA few slow requests can hold up the processing of subsequent requests, known as head-of-line blocking. In real life, user would send requests no matter of whether the previous one is finished (this habit will result in a longer line in queue) We can have a solution of keeping all request within the time window, sort their response times every minute. We can have approximation algos like: Forward Decay (sounds like remove an item from a list with a probability related to its size, retire smaller ones). T-digest (some kind of sketching, use clustering to simplify. Use centroids (mean. weight) to replace a group of numbers). HdrHistogram (check if it is just histogram with low accuracy fixed-point num) if we cannot count percentiles accurately. *User can hardly tell a 200ms latency, and can usually bear a &lt;1s latency. Long latency will impair user experience. Solutions to cope with load: Scaling Up &#x3D; Vertical Scaling: Move to more powerful machine. Scaling Out &#x3D; Horizontal Scaling: Use more machines and distribute load. (Usually inevitable) Some sys can be elastic (adding resources automatically or manually). Tips &#x2F; Methodology Distributing stateful data sys to multiple machines can be complex, thus try to keep database on a single node (scale up) if possible Scalable architecture depends on the volume of reads, writes, data to store, data complexity, requirement of response time, access pattern or mixture of above. Usually, iterating quickly on product features is more important than scaling to some hypothetical future load.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Data System","slug":"Data-Science/Data-System","permalink":"https://umiao.github.io/categories/Data-Science/Data-System/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Data System","slug":"Data-System","permalink":"https://umiao.github.io/tags/Data-System/"}]},{"title":"Designing Data-Intensive-Applications-Note-1","slug":"Designing-Data-Intensive-Applications-Note-1","date":"2024-02-23T23:44:41.000Z","updated":"2024-02-24T00:02:41.031Z","comments":true,"path":"2024/02/23/Designing-Data-Intensive-Applications-Note-1/","link":"","permalink":"https://umiao.github.io/2024/02/23/Designing-Data-Intensive-Applications-Note-1/","excerpt":"Introduction to designing data intensive applications, a.k.a data systems, providing high-level ideas about what it is and why it is needed.","text":"Introduction to designing data intensive applications, a.k.a data systems, providing high-level ideas about what it is and why it is needed. Key Concepts Reliability: tolerating hardware &amp; software faults; as well as human error Scalability: Measuring load &amp; performance; latency, percentiles and throughput Maintainability: operability, simplicity &amp; evolvability Motivation: More applications are data intensive than compute-intensive. The amount of data, complexity and how fast it changes is the issue. Commonly used components: Store data so that they, or another application, can find it again later (databases) Remember the result of an expensive operation, to speed up reads (caches) Allow users to search data by keyword or filter it in various ways (search indexes) Send a message to another process, to be handled asynchronously (stream processing) Periodically crunch a large amount of accumulated data (batch processing) Meaningfulness of having the data system concept: The boundaries between categories got blurred: datastores that are also used as message queues (Redis), and there are message queues with database-like durability guarantees (Apache Kafka) Single tool is harder to handle multiple use cases and needs. So we can break down into tasks which can efficiently run on single tool. Selection of components and getting them to work make you also a data system designer Fault and FailureFault tolerant &#x2F; resilient: System anticipate faults (certain types of faults) and can cope with them.Fault $\\ne$ failure, as fault is one component of the system deviating from its spec, whereas a failure is when the system as a whole stops providing the required service to the user. We should try stop fault turn into failure. Intentionally triggering faults may help prevent failure (like release resources).This book focuses more on curable faults. Hardwares We assume hardware related errors to be random and independent. Hard disks are reported as having a mean time to failure (MTTF) of about 10 to 50years. Thus, on a storage cluster with 10,000 disks, we should expect on average one disk to die per day. Solution:Add Disk redundancy, like RAID configuration.Servers: dual power supplies and hot-swappable CPUsPower: batteries and diesel generators SoftwareFor AWS, it is fairly common for virtual machine instances to become unavailable without warning, as the platforms are designed to prioritize flexibility and elasticity over single-machine reliability. Software &#x2F; systematic error can be harder to anticipate, and can occur a lot more than hardware failures as they are correlated. Like software bug, or deplete of resources (CPU time, memory, disk, etc). The failures can also be cascading, one trigger another. Mitigation MethodsSome methods to help mitigate system failures: Carefully thinking about assumptions and interactions in the system. Thorough testing. Process isolation. Allowing processes to crash and restart. Measuring (do some validation &#x2F; verification), monitoring, and analyzing system behavior in production Human can be very unreliable, causing ~75% failures by false operation rather than 10-25% of hardware outages. Methods:1. Well-designed abstractions, APIs, and admin interfaces make it easy to do “theright thing” and discourage “the wrong thing.” (should reach a balance, not become too complex)2. Decoupling testing sandbox from prod.3. Thorough, full-grained test.4. Fast recovery &#x2F; rollback, gradual roll out, logs and data recomputation.5. Monitoring on metrics (telemetry).6. Good managing and training.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Data System","slug":"Data-Science/Data-System","permalink":"https://umiao.github.io/categories/Data-Science/Data-System/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Data System","slug":"Data-System","permalink":"https://umiao.github.io/tags/Data-System/"}]},{"title":"Name of Every Keyboard's Key","slug":"Name-of-Every-Keyboard-s-Key","date":"2023-09-18T05:52:17.000Z","updated":"2023-09-18T06:13:45.446Z","comments":true,"path":"2023/09/17/Name-of-Every-Keyboard-s-Key/","link":"","permalink":"https://umiao.github.io/2023/09/17/Name-of-Every-Keyboard-s-Key/","excerpt":"For better communication during teamwork &#x2F; paircoding &#x2F; code review… It is vital to keep every keyboard’s key’s name in mind.","text":"For better communication during teamwork &#x2F; paircoding &#x2F; code review… It is vital to keep every keyboard’s key’s name in mind. kbd { background-color: #eee; border-radius: 3px; border: 1px solid #b4b4b4; box-shadow: 0 1px 1px rgba(0, 0, 0, 0.2), 0 2px 0 0 rgba(255, 255, 255, 0.7) inset; color: #333; display: inline-block; font-size: 0.85em; font-weight: 700; line-height: 1; padding: 2px 4px; white-space: nowrap; } .tg {border-collapse:collapse;border-spacing:0;} .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; overflow:hidden;padding:10px 5px;word-break:normal;} .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;} .tg .tg-depl{background-color:#F7F8FA;color:#1D2129;text-align:left;vertical-align:top} .tg .tg-lq71{background-color:#F7F8FA;border-color:inherit;color:#1D2129;font-weight:bold;text-align:left;vertical-align:top} .tg .tg-d5mg{background-color:#F7F8FA;color:#1D2129;font-weight:bold;text-align:left;vertical-align:middle} .tg .tg-6zo8{background-color:#F7F8FA;color:#1D2129;font-weight:bold;text-align:left;vertical-align:top} .tg .tg-cw55{background-color:#F7F8FA;color:#1D2129;text-align:left;vertical-align:middle} Char English Name Chinese Name ~ tilde 波浪符 ` back quote 反引号 ! exclamation mark or bang 叹号 @ at 艾特 # hash or&nbsp;&nbsp;&nbsp;number or sharp 井号 $ dollar 美元符 % percent 百分号 ^ caret 脱字符、插入符 &amp; and or&nbsp;&nbsp;&nbsp;ampersand 与和符 * asterisk 星号 ( ) parentheses 圆括号、小括号 [ ] brackets 方括号、中括号 { } curly brackets 花括号、大括号 - hyphen or dash or minus 连字符、减号 _ underscore 下划线 + plus 加号 = equal 等号 / slash 斜线 \\ back slask 反斜线 | pipe or&nbsp;&nbsp;&nbsp;bar 竖线 : colon 冒号 ; semicolon 分号 ' single quote 单引号 \"&nbsp;&nbsp;&nbsp;\" quote 双引号 &lt; less than or angle brackets 小于 &gt; greater than or angle brackets 大于 , comma 逗号 . period or dot 句号 ? question mark 问号 ESC escape key 跳离键 Backspace backspace key 退格键 Insert insert key 插入建 Home home key 原位键 Delete delete key 删除键 End end key 结尾键 Page Up page up key 向上翻页键 Page&nbsp;&nbsp;&nbsp;Down page down key 向下翻页键 Enter enter key 回车键 Tab tab key 制表键 Caps&nbsp;&nbsp;&nbsp;Lock caps lock key 大写锁定键 ALT alternate key 可选键 CTRL control key 控制键 SHIFT shift key 上档键 Space space bar 空格键 Num num lock key 数字键盘锁定键","categories":[{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"Software Engineering","slug":"Job-Search/Software-Engineering","permalink":"https://umiao.github.io/categories/Job-Search/Software-Engineering/"}],"tags":[{"name":"Keyboard","slug":"Keyboard","permalink":"https://umiao.github.io/tags/Keyboard/"},{"name":"Term","slug":"Term","permalink":"https://umiao.github.io/tags/Term/"}]},{"title":"Basics About Linux & Vim Usage","slug":"Basics-About-Linux-Vim-Usage","date":"2023-09-18T05:16:13.000Z","updated":"2023-09-18T05:51:30.177Z","comments":true,"path":"2023/09/17/Basics-About-Linux-Vim-Usage/","link":"","permalink":"https://umiao.github.io/2023/09/17/Basics-About-Linux-Vim-Usage/","excerpt":"For a Software Engineer &#x2F; Researcher, Linux and Vim should be the basic of basics (as important parts of CLI, Command Line Interface). In this post, we have discussed some basic knowledge as well as useful tips.","text":"For a Software Engineer &#x2F; Researcher, Linux and Vim should be the basic of basics (as important parts of CLI, Command Line Interface). In this post, we have discussed some basic knowledge as well as useful tips. Notes about Linux Usage:File System Structure of LinuxA tree like directory, the root would be “&#x2F;”.Some of the root folders: “bin” storing binary executives “etc” for system configurations “home” for user files “lib” for libraries and modules “opt” for optionalpackages “root” for super user folder “tmp” for temporary files, etc. Format of commands:cmd [options] [arguments]The options and arguments all serve as part of input, use whitespace to separate.For single character option, use -option. For a word, use –-option (two -s). Wildcard Matching Operator “”* for any character of any length “?” for single character [ab..] for any character among a, b, …, [!ab] for any single character except a, b. File Type Normal file “-” Directory “d” Symbolic link “l” (hard link: point to a drive’s block of the file, just like a file; soft link: save an absolute path of a file) Character device file “c” Block device file “b” Socket “s” Named pipe file “p” Frequently Used Commands: “pwd”: show the current directory of current user “cd”: change dir “.”: current directory “..”: parent directory “-”: the directory before “cd” command got executed “~”: Absolute path name of the user’s main directory Absolute directory: start with “&#x2F;”, describe the complete path to the location of file. Relative directory: not start with “&#x2F;”, appoint a location in relative to your currentworking dir. Auto-Complete: can press “Tab” to auto fill the command name &#x2F; file name &#x2F; etc. Ifthere are too many possibilities, need multiple press. “ls”: show the information of file &#x2F; dir “mkdir”: create an empty dir in current dir “rmdir”: remove empty dir only “touch”: generate an empty file, or update the time of a file (-a, -m, -d) “cp”: copy file or directory “mv”: move &#x2F; rename file or dir “rm”: remove file or dir “ln”: create a symbolic link “find”: look up for a file “stat”: look for file’s type &#x2F; property “cat”: create file &#x2F; check the content of text file “more”: used to view the text files, displaying one screen at a time. “less”: similar to more, support search &#x2F; roll back “tail”: check the last n lines of file, with tail -n filename “head”: check the first n lines instead “echo”: redirect content into given files “|”: pipe command. Pass the result of prior command to latter command, like: ls -la | wc. This is for listing info and word count. “&gt;, &gt;&gt;”: &gt; would override content, &gt;&gt; would append content. “zip &#x2F; unzip &#x2F; gzip &#x2F; gunzip &#x2F;rar &#x2F; tar”: used to zip &#x2F; unzip &#x2F; pack. Regular Expression related: “grep”: stands for “global search regular expression”. Format: grep [options] PATTERN [FILE…]Examples: grep &#39;^[a-zA-Z]&#39; myfile grep -v &#39;^#&#39; myfile (reverse, only keep not matched result) grep -lr root &#x2F;etc&#x2F;* (recursively list all filenames under &#x2F;etc&#x2F; with“root”) Shell Variables:It can be classified as: Internal Variable: provided by the system, use only to the user (cannot be modified) Environment Variable: define the work env, can be used in shell; user can modify some of them User Variable: defined by user (often used in script). (Definition: varName&#x3D;Value, Reference: $varName). “export” can switch variables between “Global” and “Local” Some common shell env variables: HOME, LOGNAME, USER, PWD, MAIL, HOSTNAME,INPUTRC, SHELL, LANG, HISTSIZE, PATH, PS1, PS2. VI &#x2F; VIM (Visual Interface iMproved)Entering: vim +n filename (open file, place cursor to the beginning of nth line) vim + filename (open file, place cursor to the beginning of last line) vim +/pattern filename (open file, place cursor to first matched position) Modes: Normal: default mode when run vim Insert: press i, o, a to enter insert mode Cmdline: type commands starting with :, &#x2F;, ?, ! Under Normal Mode: G : jump to the end of file ZZ : save and exit ZQ : exit without save &#x2F; or ? : for string look up. (can be used with “n” for next match) n : Next. Search for next match. yy : copy one line p : paste to next line; P: paste to previous line dd : delete one line dw : delete a word; can use “dnw” to delete n words. d$ : delete to the end of current line. “dG” will delete to end of document. y : similarly, we can use “y” just as d to duplicate words. p : use p for pasting. Can use “10p” to paste for 10 times. x : delete the character at the cursor. u : Undo. Undo the last operation. “Ctrl + r” : to revoke the undo. “h,j,k,l” : move the cursor to left&#x2F;down&#x2F;up&#x2F;right “0” : move to head of line “^” : move to the first non-blank character “$” : move to end of line “g_” : move to the last non-blank character of current line “w” : move to the beginning of next word “e” : move to the end of next word “fa” : move to the position of next “a”. If use “Fa”, would move to previous a. “nfa” : would move to the nth “a”’s position in current line. Can use “;” to jump to next appointed character, “,” can jump to previous appointed character. “nG” : move cursor to the beginning of line n. “gg” : move cursor to beginning of line 1. “G” : move to the beginning of last line. “H &#x2F; M &#x2F; L”: move cursor to the beginning &#x2F; middle &#x2F; end of current screen “% &#x2F; * &#x2F; #” : match parenthesis &#x2F; match next word where cursor locates &#x2F; match previous word where cursor locates Under Insertion Mode:After entering this mode, any typed character would be recognized as content of file and then added. “Esc”: press to exit insertion mode (back to normal mode) Under Command Mode:Under Normal Mode, press “:” to enter Command mode.Vim would get into “Pending command” state. “:w” : save file “:w newfile” : save as a new file named “newfile” “:wq” : save and exit “:q!” : exit without save “:q” : save without changes made “:!command” : execute linux command, e.g., “date”, “ls”, … “:r !command” : e.g., “:r !date”: add the result to the position of cursor Environment Setup: “:set autoindent &#x2F; noautoindent” “:set number &#x2F; nonumber” : row number setting “:set ic &#x2F; noic” : require being insensitive &#x2F; sensitive to capitalized characters “:set tabstop&#x3D;value” : map tab into “value” spaces “:set all” : show all the configurable options “:n” : locate at line n “2, 5d” : delete lines with row numbers of 2 – 5. “:s&#x2F;aa&#x2F;bb&#x2F;g” : replace all “aa” with “bb” at current line. Replace “s” with “%s” would do the replacement across the entire document. Replace “s” with “n1, n2” would do the replacements for lines (with line number n1~n2). If omit “&#x2F;g”, would only replace the first appearance. If replace “&#x2F;g” with “&#x2F;gc”, confirmation would be required.","categories":[{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"Software Engineering","slug":"Job-Search/Software-Engineering","permalink":"https://umiao.github.io/categories/Job-Search/Software-Engineering/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://umiao.github.io/tags/Linux/"},{"name":"Vim","slug":"Vim","permalink":"https://umiao.github.io/tags/Vim/"},{"name":"CLI","slug":"CLI","permalink":"https://umiao.github.io/tags/CLI/"}]},{"title":"Learn English Vocabulary by Frequency","slug":"Learn-English-Vocabulary-by-Frequency","date":"2023-09-16T06:01:32.000Z","updated":"2023-09-16T17:31:55.354Z","comments":true,"path":"2023/09/15/Learn-English-Vocabulary-by-Frequency/","link":"","permalink":"https://umiao.github.io/2023/09/15/Learn-English-Vocabulary-by-Frequency/","excerpt":"As an engineer working in the industry, English is a very important and basic tool…Also, if we think like a machine, it would be then very natural for us to study English Vocab in its frequency order.. A weird idea, right?","text":"As an engineer working in the industry, English is a very important and basic tool…Also, if we think like a machine, it would be then very natural for us to study English Vocab in its frequency order.. A weird idea, right? English Words (not familiar to me) (in descending order of frequency) This post is based on this dataset [link](https://www.kaggle.com/datasets/rtatman/english-word-frequency) shared by Kaggle. > In fact, I already found this source is highly skewed, since it is likely crawled from Internet. ‘eBay’ has been a very frequent word, which might because of hundreds millions of listings going into this dataset. kbd { background-color: #eee; border-radius: 3px; border: 1px solid #b4b4b4; box-shadow: 0 1px 1px rgba(0, 0, 0, 0.2), 0 2px 0 0 rgba(255, 255, 255, 0.7) inset; color: #333; display: inline-block; font-size: 0.85em; font-weight: 700; line-height: 1; padding: 2px 4px; white-space: nowrap; } A very useful hotkey: you can use Ctrl + Shift + Home / End to quickly select all the content above / beneath your cursor. This would be super useful if you want to skip a bunch of high freq words! #customers { font-family: Arial, Helvetica, sans-serif; border-collapse: collapse; width: 100%; } #customers td, #customers th { border: 1px solid #ddd; padding: 5px; } #customers tr:nth-child(even){background-color: #f2f2f2;} #customers tr:hover {background-color: #ddd;} #customers th { padding-top: 12px; padding-bottom: 12px; text-align: left; background-color: #04AA6D; color: white; font-size: 10px;} Word Meaning Frequency lyrics words of song; poem expresses writer's emotions 92579731 catalog (make) a list of items 57567413 Dave Name; Hebrew Dawid meaning \"beloved\" or \"favourite\" 32281596 vintage year or place wine is produced; Adj: denoting high quality wine 31830831 provisions Verb: set aside, supply; Noun: action / thing of supplying 31126350 voyeur people enjoy seeing pain/sex/distress 27928748 census official count of a population 25385152 glossary alphabetical list of terms 23506998 fiscal relating to government revenue (taxes) 23506040 &lt;/tbody&gt; &lt;/table&gt;","categories":[{"name":"Productivity","slug":"Productivity","permalink":"https://umiao.github.io/categories/Productivity/"}],"tags":[{"name":"English","slug":"English","permalink":"https://umiao.github.io/tags/English/"},{"name":"Vocabulary","slug":"Vocabulary","permalink":"https://umiao.github.io/tags/Vocabulary/"},{"name":"Word Frequency","slug":"Word-Frequency","permalink":"https://umiao.github.io/tags/Word-Frequency/"}]},{"title":"Options Futures and Other Derivatives - Note - 2","slug":"Options-Futures-and-Other-Derivatives-Note-2","date":"2023-09-11T06:59:30.000Z","updated":"2023-09-11T07:02:52.718Z","comments":true,"path":"2023/09/10/Options-Futures-and-Other-Derivatives-Note-2/","link":"","permalink":"https://umiao.github.io/2023/09/10/Options-Futures-and-Other-Derivatives-Note-2/","excerpt":"Discussing different types of traders.","text":"Discussing different types of traders. Types of tradersThree broad categories of traders can be identified: Hedgers: Use derivatives to reduce the risk that they face from potential future movements in a market variable. Speculators: Bet on the future direction of a market variable. Arbitrageurs: Take offsetting positions in two or more instruments to lock in a profit. Hedgers","categories":[{"name":"Investment","slug":"Investment","permalink":"https://umiao.github.io/categories/Investment/"}],"tags":[{"name":"options","slug":"options","permalink":"https://umiao.github.io/tags/options/"},{"name":"futures","slug":"futures","permalink":"https://umiao.github.io/tags/futures/"},{"name":"trading","slug":"trading","permalink":"https://umiao.github.io/tags/trading/"},{"name":"investment","slug":"investment","permalink":"https://umiao.github.io/tags/investment/"}]},{"title":"Options Futures and Other Derivatives - Note - 1","slug":"Options-Futures-and-Other-Derivatives-Note-1","date":"2023-09-09T19:42:55.000Z","updated":"2023-09-11T06:58:48.832Z","comments":true,"path":"2023/09/09/Options-Futures-and-Other-Derivatives-Note-1/","link":"","permalink":"https://umiao.github.io/2023/09/09/Options-Futures-and-Other-Derivatives-Note-1/","excerpt":"Learning note of “Options Futures and Other Derivatives”, 11th Edition.","text":"Learning note of “Options Futures and Other Derivatives”, 11th Edition. Introduction Derivatives exchange is a market where individuals trade standardized contracts that have been defined by the exchange. Chicago Mercantile Exchange (CME) Open outcry system is being replaced by electronic trading -&gt; growth in algorithmic trading. Over-the-counter Market Alternative to exchanges; larger than the exchange-traded market (in total volume of trading) Usually between financial institutions; prepared to quote both a bid price (a price at which they are prepared to buy) and an offer price (a price at which they are prepared to sell). The terms of a contract do not have to be those specified by an exchange. Market size rises to $600+ trillion in 2022. Forward Contracts Agreement to buy or sell an asset at a certain future time for a certain price. A spot contract is an agreement to buy or sell an asset today. One of the parties to a forward contract assumes a long position and agrees to buy the underlying asset on a certain specified future date for a certain specified price. The other party assumes a short position and agrees to sell the asset on the same date for the same price. Payoffs from Foward ContractsAssuming we have forward quotes for the USD&#x2F;GBP exchange rate, for 6-month forward, the bid price: 1.4416 and offer price: 1.4422, then we can buy 1M GBP for 1.4422M USD.If the spot exchange rate rose to 1.5000, at the end of the 6 months, the forward contract would be worth $57,800 (&#x3D; 1,500,000 - 1,442,200). Similarly, if the spot exchange rate fell to 1.3500 at the end of the 6 months, the forward contract would have a negative value to the corporation of $ 92,200. We assume the Delivery price&#x3D;$k$, and price of asset at contract maturity &#x3D; $S_T$.The payoff from a long position in a forward contract on one unit of an asset is $S_T - K$, and for a short position, the payoff is $K - S_T$. Forward Prices and Spot PricesForward Prices and Spot Prices are tightly related. If a a stock that pays no dividend and is worth 60, and you can borrow or lend money for 1 year at 5%, then the 1-year forward price should be 63. If not, then you can hold long &#x2F; short forward contracts for profit. Future ContractsLike a forward contract, a futures contract is an agreement between two parties to buy or sell an asset at a certain time in the future for a certain price.Unlike forward contracts, futures contracts are normally traded on an exchange. Options Options are traded both on exchanges and in the over-the-counter market. A call option gives the holder the right to buy the underlying asset by a certain date for a certain price. A put option gives the holder the right to sell the underlying asset by a certain date for a certain price. The price in the contract is known as the exercise price or strike price; The date in the contract is known as the expiration date or maturity. American options (most of the options) can be exercised at any time up to the expiration date. European options can be exercised only on the expiration date itself. One contract is usually an agreement to buy or sell 100 shares. It should be emphasized that an option gives the holder the right to do something.The holder DOES NOT have to exercise this right. This is what distinguishes options from forwards and futures, where the holder is obligated to buy or sell the underlying asset. The price of a call option decreases as the strike price increases (negatively correlated), while the price of a put option increases as the strike price increases (positively correlated).Both types of option tend to become more valuable as their time to maturity increases (increased uncertainty, converged to the delivery price). ExampleSuppose an investor instructs a broker to buy one December call option contract on Google with a strike price of $520. The broker will relay these instructions to a trader at the CBOE and the deal will be done. The (offer) price is $32.00, this is the price for an option to buy one share.In the United States, an option contract is a contract to buy or sell 100 shares. Therefore, the investor must arrange for $3,200 to be remitted to the exchange through the broker. The exchange will then arrange for this amount to be passed on to the party on the other side of the transaction. The investor got the right to by 100 Google shares for $ 520 each at the cost of 3200 USD (cost on options). If Google reached 600 USD and the option is exercised, the profit would be $(600-520)*100-3200&#x3D;4800$. If Google stayed off 520 USD, the maximal loss is made (the premium) since the holder of the option won’t exercise it. If a call &#x2F; put option is sold, the sellar would immidiately receive the premium. For now, we discussed 4 types of participants in options markets: Buyer of calls Seller of calls Buyer of puts Seller of puts Buyers hold long position, and sellers hold short position. Selling an option also known as writing the option.","categories":[{"name":"Investment","slug":"Investment","permalink":"https://umiao.github.io/categories/Investment/"}],"tags":[{"name":"options","slug":"options","permalink":"https://umiao.github.io/tags/options/"},{"name":"futures","slug":"futures","permalink":"https://umiao.github.io/tags/futures/"},{"name":"trading","slug":"trading","permalink":"https://umiao.github.io/tags/trading/"},{"name":"investment","slug":"investment","permalink":"https://umiao.github.io/tags/investment/"}]},{"title":"Make Full Use of Apple Watch","slug":"Make-Full-Use-of-Apple-Watch","date":"2023-09-04T19:48:04.000Z","updated":"2023-09-05T02:22:29.955Z","comments":true,"path":"2023/09/04/Make-Full-Use-of-Apple-Watch/","link":"","permalink":"https://umiao.github.io/2023/09/04/Make-Full-Use-of-Apple-Watch/","excerpt":"My pup sent me an Apple Watch as a gift, and for long time I have now idea how to make full use of it.","text":"My pup sent me an Apple Watch as a gift, and for long time I have now idea how to make full use of it. Control Center &#x2F; 控制中心 打开控制中心：从表盘上向上轻扫。从其他屏幕，按住屏幕底部，然后向上轻扫。 【注】你不能从 Apple&nbsp;Watch 的主屏幕打开“控制中心”。而需要按一下数码表冠前往表盘或打开 App，然后打开“控制中心”。 关闭控制中心：从屏幕顶部向下轻扫，或者按一下数码表冠。 图标 描述 有关更多信息 断开与无线局域网的连接。 轻触以连接/断开网络。 呼叫你的 iPhone。 轻触就会叮叮叮！ 检查电池百分比。 静音 Apple&nbsp;Watch。 如果 Apple Watch 正在充电，闹钟和计时器即使在静音模式中仍会响起。 打开剧院模式。 剧院模式可防止 Apple Watch 屏幕在抬腕时亮起，以始终保持屏幕熄灭状态。还会打开静音模式，但你仍会收到触感通知。 选取专注模式/勿扰模式。 可于设置中打开并设置专注时间。 关闭“睡眠”专注模式。 可使用睡眠 / health app 管理睡眠计划、质量等。长按表冠以从睡眠模式解锁。 打开手电筒。 向左轻扫以选取一种模式：稳定的白光、闪烁的白光或者稳定的红光。 打开飞行模式。 打开入水锁定。 开启防水功能以防误触。 选取音频输出。 切换音频输出，如连接至蓝牙设备。 检查耳机音量。 更改文字大小。 打开或关闭“对讲机”。 要求足够高的watchOS与iOS版本，可以使用FaceTime，并使用Walkie-Talkie程序（从联系人中选取添加好友、接受邀请、进行对话）。 APP &#x2F; 应用小技巧 “健康”等相关应用用于检测运动情况、心率、睡眠等。 “计算器”速算小费（TIP）等。 “相机”可远程唤起手机相机，用于自拍等。 “指南针”中包含了Pin（记录位置、停放车辆等）、足迹追踪（Back Tracking， 防止迷路）等功能。 “微信”可强制发起弹窗（长按联系人的一条信息，即可发出信息 &#x2F; Signal)。","categories":[{"name":"Productivity","slug":"Productivity","permalink":"https://umiao.github.io/categories/Productivity/"}],"tags":[{"name":"Productivity","slug":"Productivity","permalink":"https://umiao.github.io/tags/Productivity/"},{"name":"Apple Watch","slug":"Apple-Watch","permalink":"https://umiao.github.io/tags/Apple-Watch/"},{"name":"Tips","slug":"Tips","permalink":"https://umiao.github.io/tags/Tips/"}]},{"title":"Need-To-Knows For Software Security Engineer","slug":"Need-To-Knows-For-Software-Security-Engineer","date":"2023-09-01T02:35:51.000Z","updated":"2023-09-01T03:45:50.741Z","comments":true,"path":"2023/08/31/Need-To-Knows-For-Software-Security-Engineer/","link":"","permalink":"https://umiao.github.io/2023/08/31/Need-To-Knows-For-Software-Security-Engineer/","excerpt":"This post covers common software &#x2F; cyber-security threats and preventions. Including SQL Injection, Cross-Site Scripting (XSS), OS Command Injection, Weak Session Token Generation and Missing Function Level Access Control.","text":"This post covers common software &#x2F; cyber-security threats and preventions. Including SQL Injection, Cross-Site Scripting (XSS), OS Command Injection, Weak Session Token Generation and Missing Function Level Access Control. SQL Injection Example of attack:SELECT * FROM Users WHERE Username = ‘admin’ AND Password = abc’ OR 1=1;— Here, “abc’ OR 1&#x3D;1;—” is the constructed malicious input. ‘—’ (double dashes) states end of line. Solution:Use parameterized query: Wrong Code:Straight forward concatenation (trust user’s input as part of the SQL command).VALUES (request_user_name “,” + request_user_age + “”) Correct Code:Parametrized query:insert_user = db.prepare “INSERT INTO users (name, age) VALUES (?, ?)” insert_user.execute (request_user_name, request_user_age). Other ways of injection attack: Union attack: use UNION to extend the results returned by the original query. It requires both queries to contain the same amount of columns. Conclusion: All popular dev frameworks have secure construction of db queries. Use allowlist validation on all user input. (e.g., ban the ‘) Apply least privilege principle on all backend db users. Consider GET and POST parameters, Cookies and other HTTP headers. Cross-Site Scripting (XSS)Example: (https://bank.com/search.html?keyword=&#60;script&gt;document.Location=“https://phising.com”&#60;/script&gt;)The user recognized a valid and known host name is the URL, clicks on the link. Surfs to the URL. The website (http://bank.com) does not validate or encode the params before rendering &#x2F; reflecting it back to the user.Then, the browser may execute the injected JavaScript and redirects the user to a phishing site, tricking them into submitting their passed. Prevention: you should never trust the user input. All user input that is rendered on the application output should be validated or encoded.The essential is that, if you do not validate the user input, the unescaped input would be displayed in an immediate response to the user. This would allow attacker to perform actions within the application on behalf of other users. Frameworks offer specific API calls to protect the application from XSS by HTML encoding untrusted user input. Cookies should be securely configured. Set the HttpOnly flag to prevent scripts from accessing them. Note that you should not mark strings as “safe”, e.g., &#123;&#123;recipe_searched|safe&#125;&#125;. Otherwise, the engine would escape the user input so that they would not got executed &#x2F; interpreted as html &#x2F; js. Eliminating such dangerous use cases can help prevent Cross-Site Scripting, which should be already well prevented by the dev framework. OS Command InjectionSuch vulnerability can happen when user controlled input, through parameters, cookies, http headers, etc are passed to the system shell without any prior validation. Example: url&#x2F;delete?fileToDelete&#x3D;aFile.txt;&#183;rm -rf &#x2F;var&#x2F;www&#183;If the application simply appends the GET params to the command string, the malicious command would got executed.Note that in this case, the command would run with the privilege of the running application.Note that many of the characters needs to be “url encoded” in order to be transferred and parsed correctly. Solution: Use framework specific API calls instead of OS commands, If not possible, validate all user controlled output against a white-list before passing to the shell. Note that you should apply least privilege to the application.Also,!!! Never set shell=True. This would bypass the validation and have it executed in the shell directly.You can make use of “check_output” to validate if the user provided part is a shell parameter. If not, then you should fail it. Remote File InclusionOpen any file &#x2F; Include in the page makes the application vulnerable to remote file inclusion injection. An adversary could upload a malicious script to be executed on the client-side server.You can specify it as “text&#x2F;plain” also set security policy to solve the issue. File TraverseUsing the URL GET parameter to open local files makes the application vulnerable to Path Traversal attacks.An adversary could modify the image parameter path to include any server file in the application page (e.g. ?image=/etc/paswd). Solution: Remove file response. Also not open server files from URL Get Params. Weak Session Token GenerationIf the generation is too weak, an attacker may easily associate to another active user’s session with constructed ID. Solution: Use built-in session management functionalities, instead of inventing your own. Store the session ID in a cookie and then protect session cookies. This can be done by setting an expiry timestamp, path,“secure” and “HttpOnly” flag and invalidate on logout. Session ID properties must be secure. Make them unpredictable, time limited and single session.Use a secure communication channel. Missing Function Level Access ControlDefinition: user can perform functions that they are not authorized for, or when resources can be accessed by unauthorized users.When access checks have not been implemented, or when a protection mechanism exists but is not properly configured. Solution: Protect all business functions using a role based authorization mechanism, implemented on the server side. Authorization should be applied using centralized routines, provided by the framework or external modules. Deny access by default. Use least privilege principle. Implement function access control on the server, never the client.","categories":[{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"Software Engineering","slug":"Job-Search/Software-Engineering","permalink":"https://umiao.github.io/categories/Job-Search/Software-Engineering/"}],"tags":[{"name":"Software Engineering","slug":"Software-Engineering","permalink":"https://umiao.github.io/tags/Software-Engineering/"},{"name":"Cyber Security","slug":"Cyber-Security","permalink":"https://umiao.github.io/tags/Cyber-Security/"},{"name":"Certificate","slug":"Certificate","permalink":"https://umiao.github.io/tags/Certificate/"}]},{"title":"Object Oriented Design -- Principles and Practices","slug":"Object-Oriented-Design","date":"2022-09-05T07:09:14.000Z","updated":"2022-09-17T15:52:20.062Z","comments":true,"path":"2022/09/05/Object-Oriented-Design/","link":"","permalink":"https://umiao.github.io/2022/09/05/Object-Oriented-Design/","excerpt":"Important principles as well as solution to concrete interview questions.","text":"Important principles as well as solution to concrete interview questions. SOLID PrinciplesSOLID stands for: S (SRP): The Single Responsibility PrincipleThere should NEVER be more than one reason for a class to change.(A class should be changed and seperated, if it needs to undertake multiple type of responsibilities) O (OCP): The Open Closed PrincipleThe software should be expandable instead of immutable. That is to say, being open to extension and close to mutation. L (LSP): The Liskov Substitution PrincipleOnly when an instance of a child class is able to replace any instance of its super class, we can say there is an is-A relationship between the two classes. I (ISP): The Interface Segregation PrincipleWe should not force people to be dependent on the interfaces which they do not use.Using multiple specialized interfaces is better than a single universal interface. D (DIP): The Dependency Inversion PrincipleHigh level modules should not be dependent on low level modules. Both should be dependent on abstraction. Abstraction should not be dependent on details, details shoulde be dependent on abstraction. Design Patterns Observer Design Pattern: A subject corresponds to multiple observers, and notifies them of any state changes. Usually used for implementing event handling systems in “event driven” software. The observers are physically separated and have no control over the emitted events and their sources. Most implementations use background threads listening. The sole responsibility of a subject is to maintain a list of observers and to notify them of state changes by calling their update() operation. The responsibility of observers is to register (and unregister) themselves on a subject (to get notified of state changes) and to update their state (synchronize their state with the subject’s state) when they are notified. Composite Design Pattern: A part-whole hierarchy should be represented so that clients can treat part and whole objects uniformly. (Define a unified Component interface for different parts) A part-whole hierarchy should be represented as tree structure. (Requests can be forwarded to their child components) Try not to differentiate the leaf nodes and branch, have all the objects showing similar behavior. State Design Pattern: Allows an object to alter its behavior when its internal state changes. This can be a cleaner way for an object to change its behavior at runtime without resorting to conditional statements and thus improve maintainability. Aim to solve the tasks of: An object should change its behavior when its internal state changes. State-specific behavior should be defined independently. That is, adding new states should not affect the behavior of existing states. Essence: State should be defined as an object. A class delegate state-specific behavior to the state object, rather than implement it itself. Singleton Design Pattern: Only allow one “single” instance of a class to be initialized. The only instance can be stored as class static variable and use hidden &amp; private constructor. OOD Interview TopicsDesign an elevatorRequirement: an elevator would finally stopped at the required floor. It would head towards the same direction, until all the requests (to that direction) have been solved. We can try to abstract the following objects &#x2F; classes from the real-life elevator: elevator (define its state, properties and restrictions, etc) panel (serve as container of buttons) button request (signal triggered by button) controller (schedule &amp; control the elevator). Also, the following facts should be noted: Normally, the status of an elevator can only be: up, down and idle. There may be more states like under repair &amp; inspection. You should not abstract passengers into a class, because the elevator can be agnostic of certain passenger &#x2F; items. It only check the total weight of them to decide if they can fit into one single ride. Design a parking lotWe can try to abstract the following objects &#x2F; classes from the real-life parking lot: Parking spot Entrance Cashier &#x2F; Management System Timing System Ticket Printer Design a playlistThis is mainly about system design I believe. Comprehensively consider the data structure and pipeline to be applied to optimize the performance. Work on the requirement to decide which part would need random access ability and find suitable data structure.","categories":[{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"}],"tags":[{"name":"OOD","slug":"OOD","permalink":"https://umiao.github.io/tags/OOD/"},{"name":"Object Oriented Design","slug":"Object-Oriented-Design","permalink":"https://umiao.github.io/tags/Object-Oriented-Design/"},{"name":"Software Engineering","slug":"Software-Engineering","permalink":"https://umiao.github.io/tags/Software-Engineering/"}]},{"title":"Collection and Solution of Brainteasers - 1","slug":"brainteaser_1","date":"2022-07-29T07:35:43.000Z","updated":"2022-07-29T23:33:41.462Z","comments":true,"path":"2022/07/29/brainteaser_1/","link":"","permalink":"https://umiao.github.io/2022/07/29/brainteaser_1/","excerpt":"Strange questions you would expected to meet only in interviews of financial &#x2F; trading firms LOL!","text":"Strange questions you would expected to meet only in interviews of financial &#x2F; trading firms LOL! 1 - Screwy piratesQuestion: Five pirates looted a chest full of 100 gold coins. Being a bunch of democratic pirates, they agree on the following method to divide the loot:The most senior pirate will propose a distribution of the coins. All pirates, including themost senior pirate, will then vote. If at least 50% of the pirates (3 pirates in this case)accept the proposal, the gold is divided as proposed. If not, the most senior pirate will befed to shark and the process starts over with the next most senior pirate … The process isrepeated until a plan is approved. You can assume that all pirates are perfectly rational:they want to stay alive first and to get as much gold as possible second. Finally, beingblood-thirsty pirates, they want to have fewer pirates on the boat (kill as many as possible) if given a choice between otherwise equal outcomes.How will the gold coins be divided in the end? Solution: Infer from the most trivial case. One pirate: get all $100$ gold. (1 v 0) Two pirate: The 2nd pirate would deny the 1st’s plan anyway to get all $100$ golds. Thus, 1st pirate’s expectation is $0$. (1 v 1) Three pirate: Now, the pirate who’s the first to move only need to give $1$ gold to the one who is second to move. (2 v 1) Four pirate: In the three pirate case, the last pirate’s equity is denied, so the first to move give him $1$ gold. (2 v 2) Thus, for any $2n+1$ pirate case ($n &lt; 99$), the first-to-move pirate ($2n+1$) offer the pirate $1$, $3$, …, $2n-1$ one coin and keep the rest. 2 - Tiger and sheepQuestion: One hundred tigers and one sheep are put on a magic island that only has grass. Tigers can eat grass, but they would rather eat sheep. Assume: A. Each time only one tiger can eat one sheep, and that tiger itself will become a sheep after it eats the sheep. B. Alltigers are smart and perfectly rational and they want to survive. So will the sheep beeaten? Solution: Trace back from the trivial case, like $1$ tiger ($n&#x3D;1$) and $1$ sheep. The the tiger would eat the sheep. When $n&#x3D;2$, no tiger would eat the sheep, otherwise, the case would be transformed to $n&#x3D;1$ and the tiger who eat the sheep would be eaten. Similarly, we can have that when $n&#x3D;3$, or $n&#x3D;2k-1, \\forall k \\ge 1$, a tiger can eat the sheep. 3 - River crossingQuestion: Four people, A, B, C and D need to get across a river. The only way to cross the river is by an old bridge, which holds at most 2 people at a time. Being dark, they can’t cross the bridge without a torch, of which they only have one. So each pair can only walk at the speed of the slower person. They need to get all of them across to the other side asquickly as possible. A is the slowest and takes $10$ minutes to cross; B takes $5$ minutes; Ctakes $2$ minutes; and D takes $1$ minute.What is the minimum time to get all of them across to the other side?Solution:The key is to hide the latency lol. That is to say, A and B should cross in the same round and never get back. Thus, the plan would be like: ($\\rightarrow$, CD), ($\\leftarrow$, D), ($\\rightarrow$, AB), ($\\leftarrow$, C), ($\\rightarrow$, CD). The total time consumption would be $17$ mins. 4 - Birthday problemQuestion: You and your colleagues know that your boss A’s birthday is one of the following $10$ dates:Mar 4, Mar 5, Mar 8Jun 4, Jun 7Sep 1, Sep 5Dec 1, Dec 2, Dec 8A told you only the month of his birthday, and told your colleague Conly the day. Afterthat, you first said: “I don’t know A’s birthday; C doesn’t know it either.” After hearing what you said, C replied: “I didn’t know A’s birthday, but now I know it.” You smiled and said: “Now I know it, too.” After looking at the 10 dates and hearing your comments,your administrative assistant wrote down A’s birthday without asking any questions. So what did the assistant write? Solution: The key is like looking for a unique identifier. Since the days $2$ and $7$ are unique, then $C$ would be able to know the whole date if the day is $2$ or $7$. However, I believe $C$ does not have a chance to know it, thus, the month cannot be Jun or Dec. Then, $C$ learns the month is within Mar and Sep and know the whole date. Then, we can know that $C$’s day is within $\\{1, 4, 8\\}$ because if the day is $5$, $C$ still cannot tell. However, if the day is $4$ or $8$, I cannot figure out the date. Then, the month must be a unique identifier and can only be 1st Sep. 5 - Card gameQuestion: A casino offers a card game using a normal deck of $52$ cards. The rule is that you turn over two cards each time. For each pair, if both are black, they go to the dealer’s pile; if both are red, they go to your pile; if one black and one red, they are discarded. The process is repeated until you two go through all $52$ cards. If you have more cards in your pile, you win $100$ $; otherwise (including ties) you get nothing. The casino allows you to negotiate the price you want to pay for the game. How much would you be willing to pay to play this game? Solution: $0$ because you will never win. Whenever you take away two red cards, there are two black cards for the dealer. In case of a tie (one black and one red), the difference of black and red cards number would not change (both in the deck and in your piles). Consider the idea of symmetry. 6 - Burning ropesQuestion: You have two ropes, each of which takes $1$ hour to burn. But either rope has different densities at different points, so there’s no guarantee of consistency in the time it takes different sections within the rope to bum. How do you use these two ropes to measure $45$ minutes? Solution: For a rope that takes $x$ minutes to burn, if you light both ends of the rope simultaneously, it takes $\\frac{x}{2}$ minutes to burn. So we should light both ends of the first rope and light one end of the second rope. $30$ minutes later, the first rope will get completely burned, while that second rope now becomes a $30$-min rope. At that moment, we can light the second rope at the other end (with the first end still burning), and when it is burned out, the total time is exactly $45$ minutes. 7 - Defective ballQuestion: You have $12$ identical balls. One of the balls is heavier OR lighter than the rest (you don’t know which). Using just a balance that can only show you which side of the tray is heavier, how can you determine which ball is the defective one with $3$ measurements? Solution: Split the $12$ balls into groups of $4$, and measure the first $2$ groups. If it balances, the defective ball is in the rest group. If not, that ball is within the prior $2$ groups…The detailed strategy is shown in the following image: 8 - Trailing zerosQuestion: How many trailing zeros are there in $100!$ (factorial of $100$)? Solution: This is an easy problem. We know that each pair of $2$ and $5$ will give a trailingzero. If we perform prime number decomposition on all the numbers in $100!$, it is obvious that the frequency of $2$ will far outnumber of the frequency of $5$. So the frequency of $5$ determines the number of trailing zeros. Among numbers $1, 2, …, 99$, and$100$, $20$ numbers are divisible by $5$ ( $5, 10, …, 100$ ). Among these $20$ numbers, $4$ aredivisible by $5^2$ ( $25, 50, 75, 100$ ). So the total frequency of $5$ is $24$ and there are $24$trailing zeros. 9 - Horse raceQuestion: There are $25$ horses, each of which runs at a constant speed that is different from the other horses’. Since the track only has $5$ lanes, each race can have at most $5$ horses. If you need to find the $3$ fastest horses, what is the minimum number of races needed to identify them? Solution: It is natural for us to index the horses from $1$ to $25$ and split them into $5$ groups of $5$ to take the first race. Then, with the first round $5$ races, you can eliminate the last $2$ of each group. We can assume $1, 6, 11, 16, 21$ are the fastest within each group (and already sorted by their speed, though we do not know the speed now). We can find that for $16, 21$ cannot be in top $3$; $11$ may be within top $3$ but may not (because other members of top $3$ may be beat by $1$ and $6$). Then, we can race the $6$th time among $1, 6, 11, 16, 21$ to find out the local top $3$. Then, the pool of top $3$ can only be: $1, (2, 3), 6, (7), 11$ and $1$ is the absolute champion. Then we only need to give $7$th ride to $2, 3, 6, 7, 11$. 10 - Infinite sequenceQuestion: If x ^ x ^ x … ^ x&#x3D;2 and $x^y&#x3D;x^y$, find $x$.Solution: Since we can have $x &gt; 1$ and $\\lim_{n \\rightarrow \\inf}$ x ^ x ^ x … ^ x (a total of n) &#x3D;2 converges to a constant $2$, then another ^ x would not change the result. Then we can have x ^ (x ^ x ^ x … ^ x) &#x3D; $x^2 &#x3D; 2$, $x&#x3D;\\sqrt 2$.","categories":[{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"Financial Firm","slug":"Job-Search/Financial-Firm","permalink":"https://umiao.github.io/categories/Job-Search/Financial-Firm/"}],"tags":[{"name":"IQ","slug":"IQ","permalink":"https://umiao.github.io/tags/IQ/"},{"name":"Brainteasers","slug":"Brainteasers","permalink":"https://umiao.github.io/tags/Brainteasers/"},{"name":"Math","slug":"Math","permalink":"https://umiao.github.io/tags/Math/"}]},{"title":"NLP-2 Reading Note: A Survey on Text Classification","slug":"NLP-2","date":"2022-06-20T18:38:01.000Z","updated":"2022-07-29T07:09:48.412Z","comments":true,"path":"2022/06/20/NLP-2/","link":"","permalink":"https://umiao.github.io/2022/06/20/NLP-2/","excerpt":"Review the major NLP tasks as well as methods (in terms of text classification).","text":"Review the major NLP tasks as well as methods (in terms of text classification). AbstractThe methods discussed in this survey are proposed between 1961-2021. Traditional model: Preprocess and mannually extract features from the documents. Deep learning model: Learns and implements the feature extraction (as non-linear transformation). Better preserve the sequential structure and contextual info. Traditional models1 - Preprocessing:graph LR; A[word segmentation] --> B[data cleaning]; B --> C[statistics]; 2 - Word Representation:graph LR; A[Representation Methods] --> 1[\"Bag-Of-Words (BOW)\"]; A --> 2[N-gram]; A --> 3[\"Term Frequency-Inverse Document Frequency (TF-IDF)\"]; A --> 4[word2vec]; A --> 5[\"Global Vectors for word representation (GloVe)\"]; Bag-Of-Words (BOW): Representing each text with a dictionary-sized vector. The i-th element in the vector represents the frequency of the i-th word in themapping array of the sentence. N-gram: Considers the information of adjacent words to predict a word’s probability. Adopts the Markov hypothesis: word appears only concerning the words that preceded it. Counting and recording the occurrence frequency of all fragments (in a sliding window with size N), and predict the sentence (next word) based on the above frequency vector. TF-IDF: TF (Term Frequency) is the word frequency of a word in a specific article, and IDF (Inverse Document Frequency) is the reciprocal of the proportion of the articles containing this word to the total number of articles in the corpus. Use the multiplication of the two to represent a word. The importance of a word increases proportionally with the number of times it appears in a document. However, it decreases inversely with its frequency in the corpus as a whole. word2vec: Use local context information to obtain (fixed-length, real value) word vectors. Two essential models: CBOW and Skip-gram. The former is to predict the current word on the premise that the context of the current word is known. The latter is to predict the context when the current word is known. The two manners: GloVe: Global Vectors for Word Representation: Construct the co-occurrence matrix of words based on the corpus. Learn the word vector based on the co-occurrence matrix and GloVe model (fit the co-occurrence frequency by dot-product of vectors to measure the relevance). 3 - ClassifiersFinally, the represented text (vector) is fed into the classifier according to selected features. Probabilistic Graphical Models (PGMs) basedExpress the conditional dependencies among features in graphs, such as the Bayesian network. It is a combination of probability theory and graph theory. Naïve Bayes (NB) : Assumption: when the target value has been given, the conditions between text $T &#x3D; [T_1,T_2, . . . ,T_n]$ are independent. It uses the prior probability to calculate the posterior probability: $P(y|T_1, …, T_n)&#x3D;\\frac{p(y)\\prod_{j&#x3D;1}^np(T_j|y)}{\\prod_{j&#x3D;1}^n p(T_j)}$. The sturcture would be quite simple (though the assumption is not actual) and would be shown below: Naive Bayes Transfer Classification (NBTC): Use EM Algorithm to settle the different distribution between the training set and the target set (by obtaining a locally optimal posterior hypothesis on the target set). By assuming the distribution of variables, Bernoulli NB, Gaussian NB and Multinomial NB can be implemented. K-Nearest Neighbors (KNN) basedFinding the category with the most samples on the k nearest samples. Possible improvements: (metric of) feature similarity, K value, index optimization (accelerate the search of k nearest negihbours). However, this algorithm can be very SLOW on large datasets. The Neighbor-Weighted K-Nearest Neighbor (NWKNN) is proposed to improve classification on Unbalanced corpora (casting big weight for neighbors in a small category, a small weight for neighbors in a broad class). Support Vector Machine (SVM) basedChange the classification task into multiple binary classification tasks in essence (construct a optimal hyperplane to divide two classes with max distance of category boundary). Decision Trees (DT) basedSupervised tree structure learning including: Tree constrction and tree pruning. The key is to divide the dataset into diverse subsets, and every leaf node stands for a category. The Iterative Dichotomiser 3 (ID3) algorithm uses information gain as the attribute selection criterion to select node &#x2F; discriminant attribute. DT based models usually need to be train on each dataset (lack of generalization ability). Integration-based Methods.Aims at aggregating the results of multiple algorithms for better performance. Bootstrap aggregation: Random forest, Adaptive Boosting (AdaBoost), XGBoost. These classifiers are expected to be independent (combine different learners would be ideal). Summary:NB assume the features are independent. It is less sensitive to missing data and simple. The performance would drop with large number of features and high correlation between features. SVM can solve high-dimensional &#x2F; non-linear problems. It has a high generalization but sensitive to missing data. KNN depends on the finite, surrounding samples thus vulnearable in cases of crossover &#x2F; overlap of the class domain. DT is easy to explain, however, the feature engineering can be difficult. However, it works well on small datasets. Deep learning methodsMainstream methods include Multi-Layer Perceptron (MLP), Recursive Nerual Network (RNN), Convolutional Neural Network (CNN). Bert is able generate the contextualized word vectors and is a significant turning point. RNN: Usually used to learn a (latent) semantic vector representative automatically (each input word would be viewed as a leaf node of the entire tree like model structure. Finally, all nodes are combined into a parent node to represent the entire input text for prediction). It can also deal with input with variable length. It should be noted that it is a biased model as the following inputs profit over the former and decreasing the semantic efficiency (LSTM is proposed to partially alleivated this issue). It can also be realized in a bi-directional manner. CNN: It can be implemented both in character level and word level. It can also combined with pyramid structure, residual network, etc. Self-attention &#x2F; Transformer &#x2F; Bert: There are alread a lot of materials on this topic. Pre-trained Model: Including models like Embedding from Language Model (ELMo), OpenAI GPT, BERT, etc.We can clearly tell that ELMo is LSTM based while GPT and BERT are Transformer based. These methods comeup with larget training time, training data and resources. A typical setting is like over 200K training sets and 1.5B training data, batch size over 8K. We also have other models like BART (Seq2Seq based denoising autoencoder, introducing noise to the document and use Seq2Seq model to reconstruct) and SpanBERT (improved implementation of BERT, like mask a continuous paragraph, rather than word leveled. Span Boundary Objective (SBO) is added to predict span by the token next to the span boundary, and the NSP pre-training task is removed). GNN (Graph Neural Networks) based methodJust put a image here:","categories":[{"name":"AI","slug":"AI","permalink":"https://umiao.github.io/categories/AI/"},{"name":"NLP","slug":"AI/NLP","permalink":"https://umiao.github.io/categories/AI/NLP/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://umiao.github.io/tags/NLP/"}]},{"title":"NLP-1 Roadmap","slug":"NLP-1","date":"2022-06-20T17:23:43.000Z","updated":"2022-06-20T18:49:12.139Z","comments":true,"path":"2022/06/20/NLP-1/","link":"","permalink":"https://umiao.github.io/2022/06/20/NLP-1/","excerpt":"Have a glance on the NLP tasks and techniques. Will be discussed in a more detailed manner.","text":"Have a glance on the NLP tasks and techniques. Will be discussed in a more detailed manner. General Roadmap for NLP Studygraph LR; 1[Preliminary]--> 1.1[Linear Algebra]; 1 --> 1.2[Statistics and Probability]; 1--> 1.3[Metrics]; 2[Traditional model]; 2 --> 2.1[Statistical ML]; 2.1 --- 2.1.1(Linear Classification); 2.1 --- 2.1.2(SVM); 2.1 --- 2.1.3(Tree Model); 2.1 --- 2.1.4(\"HMM / CRF\"); 2 --> 2.2[Neural Network]; 2.2 --- 2.2.1(Word embedding); 2.2 --- 2.2.2(CNN); 2.2 --- 2.2.3(RNN/LSTM/GRU/Bidirectional); 2.2 --- 2.2.4(ELMo); 2.2 --- 2.2.5(BERT); 3[Paradigm and trick]; 3 --> 3.1[Document Classification]; 3.1 --- 3.1.1(ReNN); 3.1 --- 3.1.2(MLP); 3.1 --- 3.1.3(RNN); 3.1 --- 3.1.4(CNN); 3.1 --- 3.1.5(Attention); 3.1 --- 3.1.6(Transformer); 3.1 --- 3.1.7(GNN); 3 --> 3.2[Sentence Matching]; 3.2 --- 3.2.1(Representation based); 3.2 --- 3.2.2(Interaction based); 3 --> 3.3[Annotation]; 3.3 --- 3.3.1(Embedding Module); 3.3 --- 3.3.2(Context Encoder Module); 3.3 --- 3.3.3(Inference Module); 3 --> 3.4[Generation]; 3 --> 3.5[Language Model]; 3.5 --- 3.5.1(Word Level); 3.5 --- 3.5.2(Sentence Level); 4[Application Scenarios]; 4 --> 4.1[Single Task]; 4.1 --- 4.1.1(Lexical Analysis); 4.1 --- 4.1.2(Syntax Analysis); 4.1 --- 4.1.3(Semantic Analysis); 4.1 --- 4.1.4(Document Generation); 4 --> 4.2[Complex Task]; 4.2 --- 4.2.1(Search / Recommendation); 4.2 --- 4.2.2(Conversation); 4.2 --- 4.2.3(Knowledge Graph); By the wayOther roadmaps of the ML &#x2F; Statistical topics would be placed here (from reddit):","categories":[{"name":"AI","slug":"AI","permalink":"https://umiao.github.io/categories/AI/"},{"name":"NLP","slug":"AI/NLP","permalink":"https://umiao.github.io/categories/AI/NLP/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://umiao.github.io/tags/NLP/"}]},{"title":"DS-Study-Note-9 Gradient Boosting Machine Tree Model(s)","slug":"DS-Study-Note-9","date":"2022-05-26T02:54:23.000Z","updated":"2022-05-28T22:43:50.842Z","comments":true,"path":"2022/05/25/DS-Study-Note-9/","link":"","permalink":"https://umiao.github.io/2022/05/25/DS-Study-Note-9/","excerpt":"Gradient Boosting Machine TreeGBMTree stands for Gradient Boosting Machine Tree.","text":"Gradient Boosting Machine TreeGBMTree stands for Gradient Boosting Machine Tree. The idea is to train multiple serial weak learner, while the objective of each learner is to fit the negative gradient of the loss function of the previous cumulative model.Thus, after this weak learner is attached, the loss of the new cumulative model shall be maximally reduced. Also, each base (weak) learner can be linearly combined with different weights (so that those learners with higher performance would contribute more to the result). A common implementation of the base learner is Tree Model (e.g., Decision Tree). Primary FeatureThe primary feature of GBM (Gradient Bossting Machine) is that, it conduct Gradient Descent in the space of function, instead of the space of model parameters (e.g., Neural Network calculates the gradient of current loss to the model parameters for update). In Gradient Boosting, in each iteration, a weak learner is generated via fitting the negative gradient of loss function to the cumulative model formed by the learners generated before (this means that the previous model is left unchanged). Then, the current weak learner is added to the cumulative model to reduce the loss. Differences: Gradient descent of the parameter space would: use the gradient to update the parameters Gradient descent of the function space would: fit a new function with the gradient Math TheoryConsidering we have $n$ training samples $\\{x_i, y_i\\}$, and the cumulative model in the $k$th round is $F_{k-1}(x)$. Then, the model in the kth round should be:$$ F_k(x) &#x3D; F_{k-1}(x) + arg \\min_{h \\subset H} Loss(y_i, F_{k-1}(x_i) + h(x_i)) $$where $h(x)$ is the desired weak learner. In fact, after the $k-1$th round, we can have $\\hat y &#x3D; F_{k-1}(x)$ and the loss $Loss(y, \\hat y)$. Thus, in order to minimize the model’s loss after introducing the $k$th weak learner, the gradient of this learner $h(x)$ should be negative to the gradient of $F_{k-1}(x)$, so that:$$ Gradient \\ h(x) &#x3D; - \\frac{\\partial Loss(y, \\hat y)}{\\partial F_{k-1}(x)} $$ In the actual implementation of this algorithm, a concrete loss function and a learning rate $\\alpha$ should be appointed. The learning rate would be used when updating the model: $F_k(x) &#x3D; F_{k-1}(x) + \\alpha h(x)$.We also need to set a boundary condition (number of iteration, minimal improvement &#x2F; difference, etc) to decide when to terminate this algorithm. GBDT (Gradient Boosting Decision Tree) AlgorithmGBDT uses CART (Classifier And Regression Tree) as the weak learner.The benefit of such design is that, the Decision Tree itself is unstable, as minor fluctuation of training data can greatly influence the (inference) result (single Decision Tree has high variance). In Ensemble Learning, we expect weak classifiers to have high variance to achieve better generalization performance. Thus, CART is preferred to the more stable weak learners (e.g., linear regression). Implementation In regression task, the loss function would be ${(y - F_{k-1}(x))}^2$, then the negative gradient would be $2(y - F_{k-1}(x))$.Thus, we can find the negative gradient simply via $y - F_{k-1}(x)$. In classification task, we aims at fitting the logarithmic probability $\\log \\frac{p}{1-p}$ with linear model $Wx+b$. The loss function would be Cross Entropy Loss: $Loss &#x3D; -y \\log p - (1-y)log(1-p)$. Then, we can have$$ F_{k-1}(x) &#x3D; \\log\\frac{p_{k-1}}{1-p_{k-1}} \\Rightarrow p_{k-1} &#x3D; \\frac{1}{1 + e^{-F_{k-1}}} $$$$ Loss &#x3D; -y\\log \\frac{1}{1 + e^{-F_{k-1}}} - (1-y)\\log \\frac{e^{-F_{k-1}}}{1 + e^{-F_{k-1}}} $$ Simplify the expression, we have $Loss &#x3D; (1-y)F_{k-1} + \\log(1 + e^{-F_{k-1}})$. $-\\frac{\\partial Loss}{\\partial F_{k-1}} &#x3D; y - p_{k-1}$ XGBoostXGBoost stands for eXtreme Gradient Boosting, which is an algorithm based on GBDT. It makes multiple improments, including: Apply second-order Taylor Formula Expansion to better approximate various loss functions (and faster convergence). Introduce regularization term to prevent overfitting. Use Block to store the structure for parallel processing Math TheoryThe objective function of XGBoost consists of loss function and regularization term.The objective function can be written as:$$ Loss &#x3D; \\sum_{i&#x3D;1}^n l(y_i, \\hat y_i) + \\sum_{k&#x3D;1}^K \\Omega(f_k) $$We have $n$ pairs of training samples and a total of $K$ trees. $\\Omega(f_k)$ is the regularization term which measures the model’s complexity. Since XGB is implemented by Boosting, we also have: $\\hat y_i^t &#x3D; \\hat y_i^{t-1} + f_t(x_i)$. Here $f_t(x_i)$ is the most recently appended weak learner. Taylor ExpansionWe already know that $f(x) \\approx f(x_0) + f’(x_0)(x - x_0) + \\frac{f’’(x_0)}{2}(x - x_0)^2$.Then, let $l(x) &#x3D; l(y_i, x)$, find the 2nd order Taylor Expansion at $x_0$, we can have $l(y_i, x) \\approx l(y_i, x_0) + l’(y_i, x_0)(x - x_0) + \\frac{l’’(y_i, x_0)}{2}(x - x_0)^2$.Similarly, we have $l(y_i, x) \\approx l(y_i, \\hat y_i^{t-1}) + l’(y_i, \\hat y_i^{t-1})(x - \\hat y_i^{t-1}) + \\frac{l’’(y_i, \\hat y_i^{t-1})}{2}(x - \\hat y_i^{t-1})^2$. Note that we have $x &#x3D; \\hat y_i^{t-1} + f_t(x_i)$, denote $g_i &#x3D; l’(y_i, \\hat y_i^{t-1})$, $h_i &#x3D; l’’(y_i, \\hat y_i^{t-1})$, we have:$$ l(y_i, \\hat y_i^{t-1} + f_t(x_i)) \\approx l(y_i, \\hat y_i^{t-1}) + g_if_t(x_i) + \\frac{h_i}{2}f_t^2(x_i)$$Here we finish the derivation. Unfold regularization termNote that $l(y_i, \\hat y_i^{t-1})$ is a constant, we can simply remove it. Also, we unfold the regularization term $\\sum_{k&#x3D;1}^K \\Omega(f_k) &#x3D; \\Omega(f_t) + \\sum_{k&#x3D;1}^{t-1}\\Omega(f_k)$. The structure of the previous $t-1$ trees would not change, thus we can view their sum as a constant and remove, to have the polished loss function:$$ {Loss}^t &#x3D; \\sum_i^n[g_if_t(x_i) + \\frac{h_i}{2}f_t^2(x_i)] + \\Omega(f_t) $$ Organize the objective function In order to define a Tree, we need the leave nodes’ weight vector $\\omega \\subset R^T$ and the mapping relationship $q: R^d \\rightarrow 1,2,…,T$, T is the number of the leave nodes. Thus, we can express a tree as $f_t(x) &#x3D; \\omega_{q(x)}$. We then define the complexity, i.e., $\\Omega$. We define it by: the number of the leave nodes $T$, and the L2 Norm of the weight vectors of the leave nodes. Thus, we define $\\Omega(f_t) &#x3D; \\gamma T + \\frac{1}{2}\\lambda\\sum_{j&#x3D;1}^T\\omega_j^2$. Merge the terms according to their order: $$ {Loss}^{(t)} &#x3D; \\sum_{j&#x3D;1}^T[(\\sum_{i \\in I_j}g_i)w_j + \\frac{1}{2}(\\sum_{i \\in I_j}h_i + \\lambda)w_j^2] + \\gamma T$$ We can denote $G_j &#x3D; (\\sum_{i \\in I_j}g_i)$ and $H_j &#x3D; \\sum_{i \\in I_j}h_i$, these two stands for sum of the 1 &#x2F; 2 - order partial derivative of the samples contained by leave node $j$. Note that $G_j$, $H_j$ are constants. Optimal SolutionWe can tell that the objective function $f(w_j) &#x3D; G_jw_j + \\frac{1}{2}(H_j + \\lambda) w_j^2$ is a 2nd-order function about $w_j$. Thus, we can tell that the minial ($f(w_j) &#x3D; -\\frac{G_j^2}{2(H_j + \\lambda)}$) is reached at $w_j &#x3D; - \\frac{G_j}{H_j + \\lambda}$. LightGBMMotivation Reduce the occupancy of memory. Utilize as much as possible data on single machine, without sacrificing the speed. Reduce the overhead of communication, realize linear acceleration in case of Multiprocessor parallel. Difference from XGBoost These two alogrithms both use the negative gradient of the loss function as the approximation of the residual of current decision tree, to fit the new decision tree. LightGBM would grow in the vertical direction, and other algorithms would grow in the horizontal direction. LightGBM would grow in the leave node with maximal error, to reduce loss as possible. Histogram AlgorithmHistogram Algorithm is proposed to substitute the pre-sorted algorithm of XGBoost. The pre-sorted algorithm would first sort the samples according to their feature values, then find the optimal split point from all the possible feature values. Thus, for each feature, the number of candidate split points is proportional to the number of samples. At the same time, Histogram Algorithm would discretize the continuous feature values into a constant number (e.g., 255) of bins. Thus, the candidate split points would reduce to (num_bins - 1) from (num_unique_values - 1). With this manner, instead of storing the feature values with float_32, we can now use uint_8 to store the index of bucket (with hash algorithm). At the same time, there comes the tradeoff of losing accuracy to raise efficiency (However, single Decision Tree itself is a weak model, it is not that important for the split points to be accurate. Coarse split points may have the regularization effect and the result can stay robust under Gradient Boosting framework.).Acceleration of Histogram via finding differenceThe histogram of a leaf can be found by solving the difference of its father node’s histogram and its brother’s histogram. LightGBM can solve the histogram of a leaf node (with small sample number) and quickly find the histogram of its brother. It should be noted that XGB and LightGBM only consider non-zero values. Leaf-wise Algorithm with Depth LimitMost GBDT algorithms use the level-wise grow strategy, i.e., all the leave nodes in the same level would be splitted, under a single traverse (then post-pruning would be executed). It is friendly for multi-thread optimization, controlling of model complexity and resist over-fitting. However, for many leaves, the split gain is very low and result in useless computational overhead. LightGBM uses the leaf-wise grow strategy. At a time, a leaf with max split gain is selected and splitted. Thus, with the same number of splition, we can come up with better accuracy and lower error. However, it also has the advantage of possibility of overfitting, as a deep decision tree may be generated. Thus, a max_depth is introduced to control the depth of a tree. GOSS - Gradient Based One-Side SamplingGOSS aims at omitting most of the samples with small gradient and only use the remaining samples to compute the information gain. (We assume that the samples with smaller gradients are already well-trained.) However, simply dropping the data with small gradient would change the overall distribution of dataset. Thus, GOSS would sort all the possible values of the feature to be splitted order by the absolute value and select the $k$-largest ones. Then, $m-k$ samples and randomly selected from the remaining ones (and assigned with a constant $fact$ to scale the samples with small gradient). EFB - Exclusive Feature BundlingHigh-dimensional data is usually sparse. Such sparsity inspires us to develop a lossless method to reduce the dimension of features. If two features are exclusive, it means that their feature would not be non-zero values at the same time. Then, we can simply find the sum of these two (Bundling), without losing information. If they are not completely exclusive, we can measure the conflict ratio (ratio of having non-zero values at the same time) of a pair of features. If conflict ratio is small, we can still bundle them without impair the final accuracy. Exclusive Feature Bundling, EFB points out that if we conduct fusion and bundling on some features, we can reduce the number of features for better performance. Decide which features to bundleBundling pair-wise independent features is a NP-Hard problem. The EFB algorithm of LightGBM reduces this problem into Graph Coloring Algorithm. Each feature would be viewed as a vertex of the graph, and we draw an edge between two features which are NOT completely independent and the weight of the edge is the conflict ratio of these two features. Then our task is to color the vertices to be bundled in the same color. The heuristic alogrithm is like: Construct such a graph, sort the vertices by the degree (larger degree means higher degree of conflict). Traverse each vertex (feature), allocate it to one of the existing feature bundles or create a new one, to minimize the total conflict. When the scale of data is very large, the efficency of graph operation would get too low. LightGBM proposes a non-graph algorithm, which sort the features by the number of non-zero values. Implement the bundlingWe can distribute values of different features into different bins of a bundle (in order to make sure the values of raw features are recognizable before bundling, by adding a shifting constant to the feature value). SummaryIt is not recommended to use LightGBM on small dataset as it is sensitive to overfitting. (Recommend to use LightGBM on dataset with a size larger than 10K) Categorical FeatureIt should be noted that, one-hot encoding is not recommended for Decision Tree (especially when the number of classes is large). The split would be imbalanced, which results in very small splitting gain. Using one-hot encoding means that on each decision node, we can only use one vs rest splitting (e.g., decide a sample is dog or not). It is almost equivalent to not splitting at all. One-hot encoding would cut samples of the same class into many small subspaces &#x2F; groups (statistics on small space may be inaccurate). LightGBM use many-to-many to solve this issue (e.g., a single node is $X &#x3D; A || X &#x3D; C$). Acceleration method: Before traverse all the candidate splitting points, first sort the histogram by the mean value of the corresponding labels, then search the optimal splitting point according to the order. This method can easily overfitting, so it needs further constraints and regularization. Source CodeWhen running in parallel, LightGBM would store all the training data in each node (server) to save the communication cost (instead of dividing the data vertically and distributing to each node). LightGBM would also use Reduce Scatter to distribute the task of merging the histogram. Finally, it applies parallizing based on voting (only the Top K features of each node would be considered and merged). Also, the histogram algorithm LightGBM applied is naturally more friendly to Cache, because it reduce the random access and do not need a data structure to store the mapping of row indexes to leave indexes. Recommended web for source code studying: https://mp.weixin.qq.com/s/XxFHmxV4_iDq8ksFuZM02w Summary on Bagging and Boosting Sample Selection: Bagging: sampling with replacement, different training sets are independent from each other Boosting: The training set remains unchanged, only the weight of each sample alters (decided by the performance of last round). Weight of sample: Bagging: Each sample has the same weight. Boosting: Weight decided by the error rate (higher error rate corresponds to higher weight). Weight of learner: Bagging: Each learner has the same weight. Boosting: Each weak learner has different weight (more accurate one has higher weight). Parallel computing: Bagging: Able to parallel. Boosting: Each learner should be generated in sequential order. Essence: Bagging: Reduce variance (via voting). Boosting: Reduce bias.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"General Knowledge","slug":"Data-Science/General-Knowledge","permalink":"https://umiao.github.io/categories/Data-Science/General-Knowledge/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://umiao.github.io/tags/Machine-Learning/"},{"name":"GBM","slug":"GBM","permalink":"https://umiao.github.io/tags/GBM/"},{"name":"XGBoost","slug":"XGBoost","permalink":"https://umiao.github.io/tags/XGBoost/"}]},{"title":"SQL-Study-Note-16 Optimization Discussion 3 - Optimize the condition setting and table creating","slug":"SQL-Study-Note-16","date":"2022-05-14T21:02:18.000Z","updated":"2022-05-15T07:25:39.805Z","comments":true,"path":"2022/05/14/SQL-Study-Note-16/","link":"","permalink":"https://umiao.github.io/2022/05/14/SQL-Study-Note-16/","excerpt":"The setting of query conditions, together with table creating can be further optimized.","text":"The setting of query conditions, together with table creating can be further optimized. Optimization of conditionsFor complex query, you can use temporary table to store the intermediate result. (Maybe CTE) Optimize the GROUP BY clauseBy default, MySQL would sort all the values of the groups generated by GROUP BY.The result of GROUP BY col1, col2, ... is equivalent to ORDER BY col1，col2，... explicitly. If you do not want to sort the result of GROUP BY, you can specify ORDER BY NULL to ban the sorting: 12SELECT col1, col2, COUNT(*) FROM table GROUP BY col1, col2 ORDER BY NULL ; Optimize the JOINIn MySQL, you can use subquery to generate a single column result and use it as another query’s filtering condition.By applying subquery, a SQL operation with multiple logical steps can be done at a time, while avoiding the deadlock of transaction and table and being easy to write. However, sometimes, the subquery can be more efficiently replaced with JOIN (this is because MySQL does not have to create temporary virtual table and able to utilize the index). Optimize the UNIONMySQL execute the UNION command via creating and filling temporary table. UNION ALL is suggested, unless you do need to eliminate the duplicates of record.MySQL would defaultly add DISTINCT keyword to the temporary table (under the UNION command), which would be costly. Avoid large transaction You can split complex SQL into multiple small SQLs Simple SQL can better utilize the QUERY CACHE of MySQL Reduce the lasting time of locks. Especially for the tables stored under the MyISAM engine. Better utilize multi-core CPU. Use TRUNCATE to replace DELETEThe DELETE operation would be recorded in the undo block and the deleting record would be recorded in binlog. If you are deleting the entire table, large binlog would be created and mass undo data blocks would be occupied. Using TRUNCATE would not record the information used for recovery. Thus, it corresponds to very small cost of resources and high time efficiency. Also, it can make auto-increment attribute back to $0$. Improve the paging strategyReasonable paging strategy can raise the efficiency of paging:Example: 12select * from t where thread_id = 10000 and deleted = 0 order by gmt_create asc limit 0, 15; The above method would extract all the records which satisfy the condition and return.In this case, we have: Cost_of_visiting = IO_of_index + IO_of_all_the_records_dataThus, in case of table with large data, the efficiency would be very low with high latency. This SQL is suitable for application scenarios with small intermediate result set (less than 10000 lines) or with complex query condition (related with multiple attributes of multiple tables) Solution: 123select t.* from (select id from t where thread_id = 10000 and deleted = 0 order by gmt_create asc limit 0, 15) a, t where a.id = t.id; In this case, we require the id to be the primary key and we have a overlap index of (thread_id, deleted, gmt_create). We first extract the primary key id for sorting, and then extract the other attributes with JOIN.we have: Cost_of_visiting = IO_of_index + IO_of_the_records_data_after_paging (15 lines)This works well in the scenarios where we have the needed overlap index and a large intermediate result set. Optimization of table creatingSet up indexFirst consider the attributes which would be used in WHERE &#x2F; ORDER BY, … to create index on. Use numerical attribute as possible E.g., you can map male &#x2F; female to 0 &#x2F; 1.If an attribute only contains numerical information, do not store it as a string. Otherwise, the performance of querying and connecting would drop and the storage overhead would increase. This is because for string, the engine would conduct char-wise comparison, while numerical data only needs one comparison. Divide’n Conquer table with large dataQuery over large table can be very slow due to too many lines to scan. Thus, we can use window function to split and iterate the entire table, and union the query result for display: 123SELECT * FROM (SELECT ROW_NUMBER() OVER(ORDER BY ID ASC) AS rowid,* FROM infoTab)t WHERE t.rowid &gt; 100000 AND t.rowid &lt;= 100050-- Specify the range of line number Use VARCHAR &#x2F; NVARCHAR to replace CHAR &#x2F; NCHARUse VARCHAR &#x2F; NVARCHAR to replace CHAR &#x2F; NCHAR as possbile, this is because: The varaible type (which takes up smaller space) can save the cost of memory &#x2F; type. It is more efficent to search in a smaller search space (less thing to compare). NULL also takes up space, if the length is already given, e.g., CHAR(100). However, for VARCHAR, the NULL would not occupy space. Design of related tables One to Many (1 to N): Use Foreign Key Many to Many (N to N): Create a new table to decomposite Many to Many to two One to One One to One (1 to 1): Usually use the same Primary Key, or add a Foregin Key","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"SQL-Study-Note-15 Optimization Discussion 2 - Optimize the SELECT and Data Manipulation","slug":"SQL-Study-Note-15","date":"2022-05-14T18:14:50.000Z","updated":"2022-05-15T17:12:02.143Z","comments":true,"path":"2022/05/14/SQL-Study-Note-15/","link":"","permalink":"https://umiao.github.io/2022/05/14/SQL-Study-Note-15/","excerpt":"Other issues which deserve attention when writing SELECT clause.","text":"Other issues which deserve attention when writing SELECT clause. Optimize the SELECTAvoid SELECT *It is not a good habit of writing SQL, since it would disable the optimizer from conducting optimization like scanning of overlay index, influencing the selection of execution plan, increasing the overhead of bandwith, IO, memory and CPU. It is suggested to specify the required columan names only. Avoid the use of functions with undefined resultIn business scenarios such as master-slave replication, the slave would only duplicate the statements executed by the master.Thus, applying functions such as now(), Rand(), sysdate(), current_user() would generate different result on the master and slave. In addition, for functions with uncertain outputs, the generated SQL statements cannot utilize the query cache. Place small table ahead of big tableWhen conducting relationship query, MySQL would scan the tables after the FROM keyword from left to right (For Oracle, it is right to left).Thus, there would be a full table scan for the first table, so it would be faster and more efficient if the first table has fewer records. Use alias of tableWhen connecting multiple tables in SQL, you should assign alia for each table and add the table alia prefix to the column names. Then the time of parsing together with the syntax error raised by ambiguity can be reduced. Use WHERE instead of HAVINGYou should avoid using HAVING, as it only filters the result after all the records are selected. However, WHERE would filter records before aggregating to reduce the number of kept records and overhead.HAVING should be used for the filtering of aggregate results only. Modify the connection order of WHERE clauseMySQL would parse the WHERE clause from left to right, up to down. Thus, we should place the condition which can filter out the most data to the very beginning. Do not use ORDER BY RAND()In this case, a random number would be generate for each row and then conduct sorting using the random number as key.E.g., select * from student order by rand() limit 5.Thus, it results in super low efficiency. It is recommended to generate primary key at random, and filter by primary key. Optimization of Data Manipulation Language statementsInsert data in batchIn case of large scale data insertion, INSERT clause with multiple values is recommended as it is faster: 1234567insert into T values(1,2); insert into T values(1,3); insert into T values(1,4);-- Insert one value at a time, slowerInsert into T values(1,2),(1,3),(1,4);-- Insert multiple values Reasons of applying the latter one: Reduce the parsing operation of SQL Reduce the number of connecting to the DB Shorter SQL clause can reduce the Network IO cost.Proper use of COMMITCOMMIT is able to release some resources (after the transaction is finished) to save the cost: The undo data block of transaction The data block recorded in redo log Release the lock of transaction to alleviate the contention of lock. Avoid query the updated data repeativelyMySQL does not support PostgreSQL’s syntax of UPDATE RETURNING.This can be realized with variable. An example of query the updated data: 12Update t1 set time=now() where col1=1; Select time from t1 where id =1; Optimized using variable: 12Update t1 set time=now () where col1=1 and @now: = now (); Select @now; Both the two methods require 2 Network IO. However, the latter avoids visiting the table again, which is much more efficient. Setting the priority of query &#x2F; updateMySQL allows to change the priority of different statements for better collaboration of multiple clients (reduce the waiting casued by lock).We should first find out the type of the application, i.e., query based or update based so that we can sacrifice one’s efficiency to speed up the other’s. The default scheduling strategy is : Write has higher priority over read. At a moment, the write operation for a certain table can happen only ONCE. The write requests are handled in the arriving order. Multiple read operations can happen at the same time. MySQL allows you to edit its scheduling via: LOW_PRIORITY: used for DELETE &#x2F; INSERT &#x2F; LOAD DATA &#x2F; REPLACE &#x2F; UPDATE HIGH_PRIORITY: used for SELECT &#x2F; INSERT DELAYED: used for INSERT and REPLACE It should be noted that if write becomes a LOW_PRIORITY request, then it may be blocked forever if read requests continue to come. The modification can also be only temporarily (appended to the end of SQL).","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"SQL-Study-Note-14 Optimization Discussion 1 - Always use index","slug":"SQL-Study-Note-14","date":"2022-05-14T06:31:36.000Z","updated":"2022-05-14T18:14:23.730Z","comments":true,"path":"2022/05/13/SQL-Study-Note-14/","link":"","permalink":"https://umiao.github.io/2022/05/13/SQL-Study-Note-14/","excerpt":"The optimization of SQL script can be extremely important, just as the importance of algorithm theory &amp; analysis to programming.","text":"The optimization of SQL script can be extremely important, just as the importance of algorithm theory &amp; analysis to programming. IntroductionThe optimization of SQL is the most significant way of leveraging the system’s performance (with the lowest cost and the best effect). The cost of optimization: hardware &gt; system configuration &gt; table structure of database &gt; SQL &amp; index Effect of optimization: hardware &lt; system configuration &lt; table structure of database Principles of MySQL Optimization Reduce the visiting of data: Setting reasonable type for each attribute; Apply compression and index to reduce the drive’s IO Return less data: Only return the needed attributes (avoid using the wildcard *); use paging of data to reduce the IO of drive and network Reduce the number of interaction: Batch DML operation; Reduce the number of connection to database via function &#x2F; stored procedure Reduce the CPU overhead: Reduce the sorting and scan of full table; to reduce the occupancy of CPU Fully use the resources: Introduce parallel processing and table partitioning Order of syntax and executionOrder of syntax123456789101. SELECT 2. DISTINCT &lt;select_list&gt;3. FROM &lt;left_table&gt;4. &lt;join_type&gt; JOIN &lt;right_table&gt;5. ON &lt;join_condition&gt;6. WHERE &lt;where_condition&gt;7. GROUP BY &lt;group_by_list&gt;8. HAVING &lt;having_condition&gt;9. ORDER BY &lt;order_by_condition&gt;10.LIMIT &lt;limit_number&gt; Order of execution FROM: Select table. Would use Cartesian Product to merge multiple tables into one. ON: Filtering based on the virtual table generated by Cartesian Product JOIN: Specify the type of JOIN. E.g., LEFT JOIN would add the remaining data of the left table into the virtual table. WHERE: Filtering based on the above virtual table. GROUP BY: Grouping the data. HAVING: Filtering after the exection of GROUP BY. Logical judgement on the aggregate result can only be placed here. SELECT: Select the desired attributes from the filtered result. DISTINCT: Keep distinct result only. Modify the name of attribute (column). ORDER BY: Sorting the result. LIMIT: Limit the row number of the returned result. Avoid the scenario which invalidaes the indexDo not use fuzzy matching at the beginning of textThis would result in a full table scanning, rather than a query with index. 1234SELECT * FROM t WHERE name LIKE &#x27;%J&#x27;-- This would result in full table scanningSELECT * FROM t WHERE name LIKE &#x27;J%&#x27;-- In this case, the index would be used If you MUST use the fuzzy matching at the very beginning, you should: Use function INSTR(str, substr) for matching, which is similar to indexOf() (return the position of queried string) Use FullText index, and query with MATCH AGAINST. Apply ElasticSearch &#x2F; solr, for big data scenario Simply use LIKE &#39;%J&#39; in small data scenario Avoid using IN &#x2F; NOT INThis would also result in the DB engine conduct a full table scan. 12345678910SELECT * FROM t WHERE id IN (2,3)-- Original querySELECT * FROM t WHERE id BETWEEN 2 AND 3-- If the value is continuousselect * from A where A.id in (select id from B);-- Index deactivatedselect * from A where exists (select * from B where B.id = A.id);-- Index activated Avoid using ORThis would also result in the DB engine conduct a full table scan. 12345678SELECT * FROM t WHERE id = 1 OR id = 3-- Original querySELECT * FROM t WHERE id = 1 UNIONSELECT * FROM t WHERE id = 3-- Optimized with UNION Avoid comparing with NULLThis would also result in the DB engine conduct a full table scan. 123456SELECT * FROM t WHERE score IS NULL-- Original querySELECT * FROM t WHERE score = 0;-- Set a default value 0 for this attribute, and filter by value 0 Avoid calculating expression &#x2F; function in the left part of the condition of WHEREThis would also result in the DB engine conduct a full table scan. 123456SELECT * FROM T WHERE score/10 = 9-- Full table scanSELECT * FROM T WHERE score = 10*9-- Use index Avoid using WHERE 1&#x3D;1 with big dataFor the convenience of adding condition to the query, we would use WHERE 1&#x3D;1 defaultly. In this case, the DB engine would scan the whole table. 12SELECT username, age, sex FROM T WHERE 1=1-- Original query We can further optimize this query. If we do not need the WHERE condition, we can remove WHERE 1=1 and if we need, we can add AND to the condition part. Avoid &lt;&gt; &#x2F; !&#x3D; in the filter conditionWhen an index column is used in the condition expression, we should avoid using &lt;&gt; or !=. If this query (and Not Equal operator) is really required in business, we should evaluate the setup of index and avoiding building index on this attribute (and build index on other attributes used in the query instead). Leftmost Prefix Matching PrincipleFor example, if a composite (union) index includes three columns, key_part1, key_part2, key_part3 but the SQL query does not include the leftmost column key_part1, then the index would not be used. This is because the Leftmost Prefix Matching Principle of SQL. 12select col1 from table where key_part2=1 and key_part3=2-- key_part1 is missing Implicit type conversionIn the following example, the type of this column is varchar while the given value 123 is a number. This would introduce Implicit type conversion and prevent the DB engine from using index. 1select col1 from table where col_varchar=123; The condition of ORDER BY should be the same as WHERE123456SELECT * FROM t order by age;-- age&#x27;s index is NOT usedSELECT * FROM t where age &gt; 0 order by age;-- age&#x27;s index is used For the above SQL, the processing order of DB is: Fetch the data according to the execution plan described by WHERE condition. Sort the data. When processing ORDER BY clause, the DB would check if the attribute used for sorting uses index in the execution plan. If yes, the data is sorted by the index. Otherwise, external sorting is applied. Return the sorted data. Thus, we require the attribute specified by ORDER BY appears in the WHERE condition, to avoid external sorting. More specifically, this attribute should use the index during execution. Similar conclusion holds for other keywords requiring sorting: GROUP BY, UNION, DISTINCT, etc. Proper use of HINTIn MySQL, you can use HINT to require the optimizer to select &#x2F; omit certain index during execution. Generally, we recommend using ANALYZE table to collect statistical information. However, sometimes specify the HINT can result in better execution plan. Appoint the index you would like to use with USE INDEX (to the end of the query). Omit certain index with IGNORE INDEX. Force MySQL to use certain index with FORCE INDEX. Generally, the DB system would auto select a proper index but it is not guaranteed to be optimal. If we know how to select the index, we can use FORCE INDEX to specify the index to use.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"SQL-Study-Note-13 Window Function","slug":"SQL-Study-Note-13","date":"2022-05-05T21:26:39.000Z","updated":"2022-05-10T19:35:30.267Z","comments":true,"path":"2022/05/05/SQL-Study-Note-13/","link":"","permalink":"https://umiao.github.io/2022/05/05/SQL-Study-Note-13/","excerpt":"Window function is also known as Online Analytical Processing function (LAP), which is able to conduct realtime processing and analyzing on the database data.","text":"Window function is also known as Online Analytical Processing function (LAP), which is able to conduct realtime processing and analyzing on the database data. MotivationIn order to solve the following problems: Ranking: rank each department with its own performance TOP N: find out the top $N$ department interms of performance Syntax12345678SELECT *, &lt;window function&gt; OVER(PARTITION BY &lt;col_name used for grouping&gt;ORDER BY &lt;col_name used for sorting&gt;)AS rankingFROM table Types of window functionsThe above can be replaced by two types of functions: Specialized Window Functions: RANK, DENSE_RANK, ROW_NUMBER Aggregate Window Function: SUM, AVG, COUNT, MAX, MIN Specialized Window FunctionsConsidering that we have a column A with value of $\\{3,3,3,7\\}$ and we would get the ranking in the form of RANK() OVER A. RANK: Multiple rows with the same value would occupy their following ranking place. Result: $\\{1,1,1,4\\}$2. DENSE_RANK: Multiple rows with the same value woud NOT occupy their following ranking place.Result: $\\{1,1,1,2\\}$3. ROW_NUMBER: Multiple rows with the same value woud NOT share the same ranking. The tie would be broken via a lexicographical order or something.Result: $\\{1,2,3,4\\}$ Example of these three functions shown below: Summary It should be noted that, the window function operates on the intermediate result after WHERE &#x2F; GROUP BY. Thus, it should be written in the SELECT clause. The PARTITION can be ignored (means sorting without grouping). Difference between PARTITION BY and GROUP BY: GROUP BY has the summary functionality, which would summarize multiple records in a group into one (reducing the number of rows), while PARTITION would keep all these records (leave the number of rows unchanged). Specialized Window Functions realize the functionality of sorting and grouping at the same time, and would NOT reduce the number of rows. Examples: Get each student’s ranking by class: 123SELECT *, DENSE_RANK() over (order by 成绩 desc) as dese_rank from 班级表; Solve TOP N question: 123456789SELECT * FROM(SELECT *, ROW_NUMBER() OVER (PARTITION BY col_name_to_be_partORDER BY col_name_to_be_sort DESC) AS rankingFROM table ) AS AWHERE ranking &lt;= N; Find the grades which are above average: 1234567SELECT * FROM(SELECT *, AVG(grade) OVER (PARTITION BY course_id) AS avg_gradeFROM table ) AS AWHERE grade &gt; avg_grade; This is solved with a 2-step manner. For each row, the corresponding average value is found. Then, filter those courses whose grades are above average. Find the cumulative SUM &#x2F; AVG &#x2F; MAX … of each course: 1234567SELECT *,SUM(grade) over w as curr_sum,AVG(grade) over w as curr_avg,MAX(grade)over w as curr_max,MIN(grade) over w as curr_minFROM scoreWINDOW w AS (partition by course_id order by student_id) If the aggregate functions of AVG() BIT_AND() BIT_OR() BIT_XOR() COUNT() JSON_ARRAYAGG() JSON_OBJECTAGG() MAX() MIN() STDDEV_POP(), STDDEV(), STD() STDDEV_SAMP() SUM() VAR_POP(), VARIANCE() VAR_SAMP() is followed by OVER() clause, it would become aggregate window function.However, if you want to get cumulative result rather than a constant result of the entire group, you must specify PARTITION and ORDER BY at the same time. It should be noted that the WINDOW keyword can apply an alia to a window, so that it can be referred for multiple times.The expected result is shown below: Sliding WindowThe range of Sliding Window can be specified either by ROWS or by RANGE. By ROWS123456789SELECT *,AVG(grade) OVER (ORDER BY id ROWS 2 PRECEDING ) -- Ways to decide the range of window by specifying rows(ORDER BY id ROWS 2 FOLLOWING ) -- Two end interval:(ORDER BY id ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) AS current_avgFROM class The PRECEDING &#x2F; FOLLOWING clause is also known as Frame clause. By RANGESometimes, the range cannot be represented by neighbor rows. E.g., when you want to select the orders within a time frame of a given date (rathe than a row frame). 123456SELECT *,AVG(price) OVER (ORDER BY id INTERVAL BETWEEN 7 DAY PRECEDING AND 7 DAY FOLLOWING) AS current_avgFROM orders Window Functions with constant frame cume_dist() &#x2F; dense_rank() &#x2F; lag() &#x2F; lead() &#x2F; ntile() &#x2F; percent_rank() &#x2F; rank() &#x2F; row_number() In these cases, built-in rules would specify the frame. MySQL 8.0 source code analysis (on execution process)stage of optimization setup windows: during the optimization process, if select_lex-&gt;m_windows is not NULL, then first call Window::setup_windows; The crucial interface would be Window::check_window_functions(THD *thd, SELECT_LEX *select). a. First judge the current window is dynamic or static. Static window (m_static_aggregates=True) would judge if the lower and upper bound of the window are defined. b. If the conditon is not satisfied, i.e., m_static_aggregates=False. Then further decide if it is based on range (m_range_optimizable) or rows (m_row_optimizable). Then decide if a row_buffer is required to calculate the result (if we need neighbor rows, no matter whether the window is static &#x2F; dynamic). Use Optimize-&gt; make_tmp_tables_info to decide if a temporary table is required as a windodw frame buffer. c. Stack calling: unit-&gt;first_select()-&gt;join-&gt;exec()-&gt;evaluate_join_record()-&gt;sub_select_op() -&gt;QEP_tmp_table::put_record()-&gt;end_write_wf() Code example:SUM(A+FLOOR(B)) OVER (ROWS 2 FOLLOWING) First finish the function calculation (FLOOR) not related with the window frame formulation. Then the result is put into the frame buffer, and frame buffer decides if the set of rows within the frame is already calculated (done in process_buffered_windowing_record). If the result does not satisfy the definition of window frame, then the calculation continues. Otherwise, the result is put into frame buffer and continue on processing the non-window function operators. process_buffered_windowing_record has two strategies of moving the sliding window, native strategy and optimizable strategy.The prior would go through all the rows within the row buffer. However, the latter would find the inverse function to eliminate the out-of-frame rows’ contribution made to the aggregation. Then, a normal aggregation function would add the contribution made by the row which just entered the frame.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"SQL-Study-Note-12 Common Table Expression and Discussion on UNION","slug":"SQL-Study-Note-12","date":"2022-05-04T19:17:03.000Z","updated":"2022-05-05T23:11:11.470Z","comments":true,"path":"2022/05/04/SQL-Study-Note-12/","link":"","permalink":"https://umiao.github.io/2022/05/04/SQL-Study-Note-12/","excerpt":"Common Table Expression (CTE) is viewed as a better way to realize the functionality of subquery.","text":"Common Table Expression (CTE) is viewed as a better way to realize the functionality of subquery. Supported by MySQL &gt;&#x3D; 8.0 Generate a named temporary table, only survives during the query Comparing with subquery: CTE can be referred multiple times within one query, and is able to refer itself (in recursive manner) Syntax (of Common Table Expression)12345678WITH cte(col1, col2) AS -- Name the temporary table here, col_name is brackets(SELECT 1, 2 UNION ALL SELECT 3, 4)SELECT col1, col2 FROM cteUNION ALLSELECT * FROM cte -- Can be referred for multipletimes, subquery can be used only onceORDER BY col1 Recursively generate sequence12345678WITH RECURSIVE test as -- USE recursive keyword to call itself(SELECT 1 AS UNIONUNION ALLSELECT 1 + n FROM test -- call itselfWHERE n &lt; 10 -- break when n &gt; 10)SELECT * FROM test The script above would generate results like: 12345n12...10 Another example about querying the quest &#x2F; reply pairs recursively 12345678910111213141516171819202122232425WITH RECURSIVE replay ( quest_id, quest_title, user_id, replyid, path ) AS ( SELECT -- Select all the answers without reply quest_id, quest_title, user_id, replyid, cast( quest_id AS CHAR ( 200 ) ) AS path FROM imc_question WHERE course_id = 59 AND replyid = 0 -- 0 means that there does not exist reply UNION ALL-- search the reply / comments recursively SELECT a.quest_id, a.quest_title, a.user_id, a.replyid, CONCAT( b.path, &#x27; &gt;&gt; &#x27;, a.quest_id ) AS path FROM imc_question a -- table a stores the reply of table b JOIN replay b ON a.replyid = b.quest_id -- recursively join on table &#x27;reply&#x27;, that is this very CTE ) SELECT * FROM replay UNION V.S. UNION ALLAs discussed previously, MYSQL UNION is able to combine the results of multiple queries: 123SELECT column, ... FROM table 1UNION \\ [ALL\\]SELECT column, ... FROM table 2 In these SELECT clauses, the corresponding columns should have the same attributes (column names), and the attribute name in the FIRST appearing clause would be used as the result’s attribute name. The main difference of UNION &#x2F; UNION ALLWhen using UNION, MySQL would remove the duplicates in the query result. When using UNION ALL, MySQL would return all the results, with a higher efficiency comparing with UNION. Using ORDER BY in UNION sub clauseIf ORDER BY is used in the sub clause (of SELECT), the result of the sub clauses would first be sorted, before combined.Besides, the entire sub clause should be wrapped with brackets, including LIMIT: 123(SELECT aid,title FROM article ORDER BY aid DESC LIMIT 10) UNION ALL(SELECT bid,title FROM blog ORDER BY bid DESC LIMIT 10) Using ORDER BY in entire query with UNIONIf you want to use ORDER BY &#x2F; LIMIT to restrict or classify the combined result of UNION, you should add brackets to each single SELECT clause: 1234(SELECT aid,title FROM article) UNION ALL(SELECT bid,title FROM blog)ORDER BY aid DESC When alia is usedIf alia is used, then ORDER BY clause MUST refer the alia: 1(SELECT a AS b FROM table) UNION (SELECT ...) ORDER BY b","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"DS-Study-Note-8 Random Forest","slug":"DS-Study-Note-8","date":"2022-05-04T05:48:16.000Z","updated":"2022-05-04T17:42:41.778Z","comments":true,"path":"2022/05/03/DS-Study-Note-8/","link":"","permalink":"https://umiao.github.io/2022/05/03/DS-Study-Note-8/","excerpt":"Random Forest inherits the idea of bagging, which is part of Ensemble Learning paradigm.","text":"Random Forest inherits the idea of bagging, which is part of Ensemble Learning paradigm. Introduction to Ensemble Learning It can be simply categorized into Boosting, Bagging and Stacking. Stacking: use Logistics Regression to integrate multiple prediction results and output one single prediction. It can be viewed as a more complicated form of voting (most commonly appeared in classification tasks, take the result with most votes). Bagging and Boosting both somehow combine existing classification &#x2F; regression methods to form a stronger classifier (utilize some sort of group intelligence). The difference lies in the combining method. BaggingAlso known as Bootstrap aggregating. ProcedureThe idea is to : 1. sample $n$ data samples with bootstraping method (sample with replacement) from the dataset, to form $k$ training sample sets by repeating $k$ times. That is to say, some samples can be contained in multiple training sample sets, while some samples may not be contained by any training sample sets. 2. We can tell that the $k$ training sets are independent with each other. 3. Execute learning algorithm on the $k$ training sets to achieve $k$ models. 4. Receive the classification &#x2F; regression results by integrating the $k$ outputs of these models (with voting &#x2F; averaging). Characteristics: Highly Parallelizable. The generated models are highly independent with each other. All the models have the same significance (equally important). Representative Work: Random Forest BoostingThe idea is to combine multiple ‘weak’ classifiers to form only ONE ‘strong’ classifier to generate ONE prediction, rather generating and processing $k$ individual predictions. This method runs under the Approximately Correct (PAC) framework. This theory supports that you are bound to leverage multiple weak classifiers into a stronger one.Implementation In each round, the weight distribution of the training data would be altered. The weight of samples which are falsely classified would increase while the weight of the correctly classified samples would be reduced to accelerate the iteration &#x2F; convergence. Different ways of combining the weak classifiers: Use additive model to generate a linear combination. AdaBoost: Through the weighted majority voting method, the weight of classifiers with less error rate would be increased. Weight of classifiers with high error rate would decrease. Boosting Tree: The classifers establish series connection and each classifier fits the residual of the prior one. Use the additive sum of all the classifiers as the predicted value. Representative Work: GBDT &#x2F; XGBoost Summary on Bagging and Boosting Bagging: would generate multiple training sets with replacement. Each sample &#x2F; classifier has the same weight. Can be easily Parallelization. Boosting: would not modify the training set but the samples’ weight. The weight of each sample &#x2F; classifier would be changed according to error rate. The training of classifiers should be in sequential order. Model setup &#x2F; combination Bagging + Decision Tree &#x3D; Random Forest AdaBoost + Decision Tree &#x3D; Boosting Tree Gradient Boosting + Decision Tree &#x3D; Gradient Boosted Decision Trees (GBDT) Decision TreeHow it is built General description: Randomly sample from the dataset to train decision tree. (First select $N$ samples to train a decision tree which is to be placed at the root node) Randomly select attribute (feature) used for training. (If the data has $M$ attributes, then randomly select a subset with size of $m$) Decide the attribute used for node splitting. (Use some metirc (information gain, gini impurity) to select one attribute as the splitting attribute of this node) Repeat such process untill reach the preset boundary (Unable to split &#x2F; reach required depth). (If the selected splitting attribute is already used by the father node, it also indicates that we have reached the leaf node and no further splitting is required) Build massive Decision Trees to form a forest. Pros and ConsAdvantages Able to process data with high dimensions without dimensional reduction and feature selection. Indicates the importance and correlation between features. Resist overfitting. High training speed, easy to parallelize. Easy to implement. Able to alleviate the imbalance in dataset distribution. Able to resist missing data partially.Disadvantages Overfitting on some regression and classification tasks with high noise volumn. In favor of attributes with more possible ways of splitting (in discrete cases).With more choices of splitting &#x2F; more unique values, an attribute can cast greater impact on the Random Forest classifier, making the result unreliable.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"General Knowledge","slug":"Data-Science/General-Knowledge","permalink":"https://umiao.github.io/categories/Data-Science/General-Knowledge/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://umiao.github.io/tags/Machine-Learning/"},{"name":"Random Forest","slug":"Random-Forest","permalink":"https://umiao.github.io/tags/Random-Forest/"}]},{"title":"DS-Study-Note-7 L1 & L2 Regularization","slug":"DS-Study-Note-7","date":"2022-05-02T23:37:57.000Z","updated":"2022-05-04T05:48:01.710Z","comments":true,"path":"2022/05/02/DS-Study-Note-7/","link":"","permalink":"https://umiao.github.io/2022/05/02/DS-Study-Note-7/","excerpt":"The essence of L1 and L2 regularization (with corresponding L1 &#x2F; L2 Norm): the projection of a vector to the domain of positive real number. They can both be viewed as metrics of distance.","text":"The essence of L1 and L2 regularization (with corresponding L1 &#x2F; L2 Norm): the projection of a vector to the domain of positive real number. They can both be viewed as metrics of distance. Summary L1 normalization would make many parameters become zero (equivalent of removing these parameters) due to the property of sparsification. L2 normalization is easier to calculate and avoid the issue of discussion on the absolute value function. Only one optimal prediction exists with L2 while multiple optimal solutions may exist with L2 (bacause of the non-linear point at $0$). TheoryWhenever we apply Gradient Descend algorithm for parameter optimization, we need to find the gradient (derivative) and use the result for parameter update:$$ \\theta &#x3D; \\theta - \\alpha \\frac{\\partial}{\\partial \\theta} J(\\theta) $$Here, $\\theta$ stands for the model parameters, $\\alpha$ stands for the learning rate and $J$ stands for the objective &#x2F; loss function. The function and derivative of L1 &#x2F; L2 norm is shown in the above image. We can tell that $$ \\frac{dL_1(w)}{dw} &#x3D; sgn(w) \\\\ \\frac{dL_2(w)}{dw} &#x3D;w $$ It is easy to find that, whenever the gradient is computed and used for update, the gradient of L1 function (if not equals zero), can only be $1$ or $-1$. Thus, for some parameters, they would head towards $0$ with steady pace (this is the cause of sparsity).However, for L2 function, the gradient’s value would vanish when a certain parameter $w$ becomes closer to $0$. This means that with L2 regularization, some parameters may become close to $0$ but would never reach $0$.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"General Knowledge","slug":"Data-Science/General-Knowledge","permalink":"https://umiao.github.io/categories/Data-Science/General-Knowledge/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://umiao.github.io/tags/Machine-Learning/"},{"name":"Regularization","slug":"Regularization","permalink":"https://umiao.github.io/tags/Regularization/"}]},{"title":"DS-Study-Note-6 Naive Bayesian modeling","slug":"DS-Study-Note-6","date":"2022-05-02T16:39:44.000Z","updated":"2022-05-02T23:36:40.323Z","comments":true,"path":"2022/05/02/DS-Study-Note-6/","link":"","permalink":"https://umiao.github.io/2022/05/02/DS-Study-Note-6/","excerpt":"Naïve Bayesian Classifier: is a typical learning based method which make hypothesis on the distribution of prediction target.","text":"Naïve Bayesian Classifier: is a typical learning based method which make hypothesis on the distribution of prediction target. The discrete caseLet $X$ be the input feature vector and $Y$ be the labels, then our target is to find out the $Y$ which maximizes the conditional probability $P(Y|X)$, with given $X$.In the discrete case, we assume the conditional probabilities (of different channels of the input feature vector) are independent from each other, $$P(X^1, …, X^D|Y) &#x3D; \\prod_{d&#x3D;1}^DP(X^d|Y)$$Then, we can simply use the total probability formula to traverse the existing data to find out the desired $Y$ which corresponds to the maximal conditional probability.$$ P(Y&#x3D;k|x^1, …, x^D) &#x3D; \\frac{\\prod_{d&#x3D;1}^DP(x^d|Y&#x3D;k)P(Y&#x3D;k)}{\\sum_j\\prod_{d&#x3D;1}^DP(x^d|Y&#x3D;j)P(Y&#x3D;j)} $$ $$ Y&#x3D;{argmax}_k \\frac{\\prod_{d&#x3D;1}^DP(x^d|Y&#x3D;k)P(Y&#x3D;k)}{\\sum_j\\prod_{d&#x3D;1}^DP(x^d|Y&#x3D;j)P(Y&#x3D;j)} \\\\ &#x3D;{argmax}_k \\prod_{d&#x3D;1}^D P(x^d|Y&#x3D;k)P(Y&#x3D;k) $$ Apply to limited dataset$$ P(X^i&#x3D;v_j|Y&#x3D;k)&#x3D;\\frac{samples \\quad with \\quad X^i&#x3D;v_j \\quad and \\quad Y&#x3D;k}{samples \\quad with \\quad Y&#x3D;k} $$ For some outliers, i.e., for given input $X$, there does not exist such data (Y), you can use smoothing method for Interpolation (e.g., set a default value which equals the mean). Scaling factorYou can introduce a scaling factor $I$ to adjust the weight between the training data and the default mean value: $$ P(X^i&#x3D;v_j|Y&#x3D;k)&#x3D;\\frac{(samples \\quad with \\quad X^i&#x3D;v_j \\quad and \\quad Y&#x3D;k) + l}{(samples \\quad with \\quad Y&#x3D;k) + lM} $$$$ P(Y&#x3D;k)&#x3D;\\frac{(samples \\quad with \\quad label \\quad k) + l}{(data \\quad samples) + lK} $$Here $M$ stands for the number of unique values of $X$ (input) and $K$ stands for the number of unique values of $Y$ (output). The continuous caseWhen processing continuous functions, we need to assume the distribution of the target function, and Normal Distribution is most commonly used. Normal Distribution can be described with two parameters, the expectation (mean) $\\mu$ and the variance $\\sigma$. These two can be estimated with statistics, so we select Normal Distribution to describe the conditional probability.Of course, we use assumption not only about distribution but also about independence. We assume $P(Y|x_1, x_2)&#x3D;P(x_1|Y) \\times P(x_2|Y)$. $$ \\mu_j^i &#x3D; E[X^i|Y&#x3D;j] \\\\ \\sigma_j^{2^i} &#x3D; E[(X^i - \\mu_j^i)^2|Y&#x3D;j]$$ Generally, the essence is to determine a distribution relying on the statistics and then use the determined distribution to fit the real distribution. Pros and Cons Simple and straightforward. Provide the probabilistic distribution function Explainable &#x2F; interpretable Require domain knowledge. Require dataset for learning Performs well even the i.i.d assumption is not satisfied Using the normal distribution for modeling provides some good properties, but may be contrary to the facts Correction of distribution modeling with Gaussian distributionApplying the Gaussian &#x2F; Normal distribution can introduce good properties but may not be reasonable. E.g., the Gaussian distribution has two long tails at the left and right, and it has exactly one peak.In order to model the distribution which may have multiple peaks, you can use the sum of multiple Gaussian functions for modeling. You can still use the training data to estimate the parameters of these Gaussian distributions. EM (Expectation Maximization) Algorithm for parameter estimationIt includes two steps: the expection-step (E-step) and maximization-step (M-step). The prior estimate the parameters by observing the data and existing model, and then computes the expection of the likelihood function with the parameters. The latter find the parameters which maximizes the likelihood function. The algorithm assures that after each iteration, the value of likelihood function would increase, so that the function is bound to converge. Instance of EM algorithmGiven two coins with different distribution, we need to estimate their expection of head-up probability after flipping. However, the experiment records do not specify which coin a single record corresponds to. Thus, we need to estimate the coin an experiment record corresponds to, and the expection of getting a head after flipping at the same time. We first initialize the expection of the two coins to be different values. For each experiment record, find out the distribution of the mapping relationship $Z$, to complete a E-Step (e.g., 0.7 belongs to coin A, 0.3 belongs to coin B). Use the achieved $Z$ to assign weights to the experiment records. (View each record as a mixture of using coin A and B). Update the experiment results to: the estimate experiment result when flipping coin A &#x2F; B. (times the distribution of $Z$ with the experiment results) Base on the priciple of maximize the likelihood, use the updated experiment records to calculate the expectation of the two coins iteratively, as the M-Step Apply EM algorithm to solve Gaussian mixture modelEach observation (experimente record) corresponds to the overlap of multiple normal distributions and the parameters are unkown. Then, we can randomly initialize the parameters for the distributions, and find out how each data point can be decomposed into these distributions. Then, we can get the datapoints weighted so that it only includes one distribution. Then we can correct the parameters of normal distribution in an iterative manner. It should be noted that the guarantee of convergence does not mean that the EM algorithm can converge at the global optimal point, because the given initial parameters decide the upper-bound of performance to a large extent. AppendixThis is about the deduction of EM algorithm’s property of convergence guarantee. Jensen Inequityif $f$ is a Concave function, $X$ is a random variable, then $E[f(X)]\\le f(E[x])$.Similar conclusion holds, with the unequal sign in the opposite direction, when $f$ is a Convex function. Let $P(x,z)$ be the distribution with latent variable $z$ (the weight &#x2F; contribution of a certain Gaussian distribution made to a datapoint).$$ \\sum_{i&#x3D;1}^M\\sum_{z&#x3D;1}^NlnP(x,z) &#x3D; \\sum_{i&#x3D;1}^Mln\\sum_{z&#x3D;1}^NQ(z) \\frac{P(x,z)}{Q(z)}$$With Jensen inquity (log is a concave function):$$ \\sum_{i&#x3D;1}^Mln\\sum_{z&#x3D;1}^NQ(z) \\frac{P(x,z)}{Q(z)} \\ge \\sum_{i&#x3D;1}^M\\sum_{z&#x3D;1}^NQ(z) ln\\frac{P(x,z)}{Q(z)} $$ The key step is to adjust $Q(z)$ so that makes the right part equals the left part (reach the $&#x3D;$). The $&#x3D;$ can be reached when the input function is a constant function, i.e., $\\frac{P(x,z)}{Q(z)}&#x3D;c$.Then we have $\\sum_z P(x,z) &#x3D; c\\sum_z Q(z) \\Rightarrow Q(z) &#x3D; \\frac{P(x,z)}{\\sum_zP(x,z)} &#x3D; \\frac{P(x,z)}{P(x)} \\Rightarrow P(z|x)$ Then, we solved the issue of how to select $Q(z)$ – by selecting the posterior probability. This is the essence of E-step which is to build lower-bound for the likelihood function $L(\\theta) &#x3D; \\sum_{i&#x3D;1}^M\\sum_{z&#x3D;1}^NlnP(x,z)$, where $\\theta$ is the parameter set of these distributions.The following M-step aims at adjusting $\\theta$ to maximize the lower-bound of $L(\\theta)$.It should be noted that this process is guaranteed to converge, but it may get into local optimal rather than the real parameter values (reach the global optimal). This is determined by the initialization of parameters.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"General Knowledge","slug":"Data-Science/General-Knowledge","permalink":"https://umiao.github.io/categories/Data-Science/General-Knowledge/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://umiao.github.io/tags/Machine-Learning/"},{"name":"Naive Bayes","slug":"Naive-Bayes","permalink":"https://umiao.github.io/tags/Naive-Bayes/"}]},{"title":"Missing Values Patterns in Time Series Data","slug":"Missing-Values-Patterns-in-Time-Series-Data","date":"2022-05-02T03:36:26.000Z","updated":"2022-05-02T05:18:55.133Z","comments":true,"path":"2022/05/01/Missing-Values-Patterns-in-Time-Series-Data/","link":"","permalink":"https://umiao.github.io/2022/05/01/Missing-Values-Patterns-in-Time-Series-Data/","excerpt":"It is meaningful and believed to be possible to discover the pattern of the missing parts of the time series data. Such patterns may vary in different scenarios and sources and may be related with physical devices and configurations.","text":"It is meaningful and believed to be possible to discover the pattern of the missing parts of the time series data. Such patterns may vary in different scenarios and sources and may be related with physical devices and configurations. Algorithms for pattern detection In the above image, we can find out the distribution and comparison between the imputed values VS the known patterns. A pipeline of missing data pattern detecting is proposed in this paper.1 Create base matrices to represent data. An algorithm to quantify and categorize missing values slots. Evaluate the frequencies of time attributes to determinate the most crucial time scenarios to analyze. Use this time attributes to find patterns over classified slots applying Kernel Density Estimation (KDE) that is considered as a statistical model to understand the shape and features of data. Data missing Around 48% of studies process dataset with missing values.2 You can discard the records with missing values, or use data imputation methods to recover the missing values. (this can be especially common in time series data) It would be helpful if you can find out the mechanism of data missing so that you can select a suitable imputation method. Avoid the missing of values during collection would always be the best solution! Missing value mechanism Missing At Random (MAR): (may work well with statistical based methods) Missing Completely At Random (MCAR): (may work well with Hot-Deck) Missing Not At Random (MNAR): (may works well with learning algorithm, like Random Forest) Problem formulationRepresentationConsidering $n$ IOT devices (sensors), each report one attribute and they monitor in a period of $t$ time slots. The, we can denote the data with $x$ and specify a value with $x(t, n)$ (at the $t$th time and collected by the $n$th device). The $t$ is expected to be in the form of a timestamp and $x$ can be viewed as a 2D matrix. We can also introduce a binary matrix (as an indicator) to mark if an element of the above matrix is missing (equals null). We can define$$BM &#x3D; X(t, n) &#x3D; \\begin{cases} 0,\\quad (x(t, n) \\quad is \\quad null) \\\\ 1 \\quad (otherwise) \\end{cases} $$ Feature selectionA series of papers propose different feature selection strategies based on the matrix $x$ to formulate new feature sequences, including: finding the cumsum, finding the indexes of missing values, transpose of the missing value indexes, record the missing values’ count and span, etc. The missing value spans can also be pre-categorized into different levels (e.g., minute &#x2F; hour &#x2F; day level). KDE: Kernel Density EstimationSelect a bandwidth parameter $h$ (may be viewed as the window’s length) and a kernel function $K(x;h)$. The function $K$ can be selected from: gaussian, tophat, epanechnikov, exponential, linear or cosine. The 1-D time series case:$$ \\hat f_K(x)&#x3D;\\sum_{i&#x3D;1}^n K_{h_x}(x-x_i) $$The 1-D time series case: (the timestamp may have 2 or more channels)$$ \\hat f_K(x,y)&#x3D;\\sum_{i&#x3D;1}^n K_{h_x}(x-x_i) K_{h_y}(y-y_i)$$Use the above function to retrieves the points to make a bivariate scatter diagram to visualizeand understand the shape and features of missing data periods. Here $x$ and $y$ may represent the start and end time of a missing part. The mined patterns would be like the visualization results shown above. We can find the common parts shared by the periodical data and view the rest parts as random noise. References [1] [Lima, Juan-Fernando, Patricia Ortega-Chasi, and Marcos Orellana Cordero]”A novel approach to detect missing values patterns in time series data.” Conference on Information Technologies and Communication of Ecuador. Springer, Cham, 2019. [2] [Dong, Y., Peng, C.Y.J.]Principled missing data methods for researchers. Springer-Plus 2(1), 222 (2013).","categories":[{"name":"UCLA","slug":"UCLA","permalink":"https://umiao.github.io/categories/UCLA/"},{"name":"Course Study","slug":"UCLA/Course-Study","permalink":"https://umiao.github.io/categories/UCLA/Course-Study/"},{"name":"ECE209 in 2022 spring","slug":"UCLA/Course-Study/ECE209-in-2022-spring","permalink":"https://umiao.github.io/categories/UCLA/Course-Study/ECE209-in-2022-spring/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"UCLA","slug":"UCLA","permalink":"https://umiao.github.io/tags/UCLA/"}]},{"title":"DS-Study-Note-5 Support Vector Machine (SVM)","slug":"DS-Study-Note-5","date":"2022-04-27T06:51:47.000Z","updated":"2022-05-01T08:11:21.602Z","comments":true,"path":"2022/04/26/DS-Study-Note-5/","link":"","permalink":"https://umiao.github.io/2022/04/26/DS-Study-Note-5/","excerpt":"SVM is a machine learning model which aims at finding a Decision Boundary with a subset of the training set. The SVM is a non-probabilistic binary classifier.","text":"SVM is a machine learning model which aims at finding a Decision Boundary with a subset of the training set. The SVM is a non-probabilistic binary classifier. Linear SeparableLinear Separable stands for an attribute that two class of points can be completely divided by a hyperplane (more specifically, a line in the 2-D space).A hyperplance can be determined with normal vector $W$ and intercept $b$, i.e., $$ X^TW + b&#x3D;0 $$. For the two separable groups, they would satisfies $X^TW + b&gt;0$ and $X^TW + b&lt;0$, respectively. In order to enhance the robustness, we additionally require the best-fit hyperplane to separate these two classes with maximum margin &#x2F; interval, which is called Maximum Margin Hyperplane. SVM (Support Vector Machine)In the training set, the points which are nearest to the hyperplane are named Support Vector. After generalized to $n$ dimensional space, a point $x &#x3D; (x_1, …, x_n)$’s distance to the hyperplane $w^Tx+b&#x3D;0$ is $\\frac{|w^Tx+b|}{||w||}$ (the denominator corresponds to 2-norm). We are interested with these support vectors and optimize the hyperplane in order to maximize the margin between the support vectors which belong to different classes. If the maximized distance equals $d$, for all the support vectors, we further unfold the absolute value expression to have:$$ \\frac{w^Tx+b}{||w||} \\ge d, y&#x3D; 1; \\quad \\frac{w^Tx+b}{||w||} \\le -d, y&#x3D; -1$$while $y$ marks different classes.Ignore the constant factor, we can have $$ w^Tx+b \\ge 1 (y&#x3D;1); \\quad w^Tx+b \\le -1 (y&#x3D;-1)$$which equals $$ y(w^Tx+b) \\ge 1 $$, so that we use hyperplanes $w^Tx+b&#x3D; \\pm 1$ to seperate the two classes. Replace the numerator of the distance expression with the two hyperplanes to have our optimization target:$$ \\max_{w, b} margin \\Leftrightarrow \\max(\\frac{2}{||w||}) \\Leftrightarrow \\max(\\frac{2}{||w||})^2 \\Leftrightarrow \\min({||w||}^2), \\quad y(w^Tx+b) \\ge 1 $$ We can add a constant factor of $\\frac{1}{2}$ to absorb the constant factor after get derivative of $w^2$. Support VectorsNow we can tell that support vectors are all the vectors on the lines of $wx^T +b &#x3D; \\pm 1$. Only the support vectors would contribute to the classification. Primal-Dual TransformationIn order to solve the primal problem of $\\frac{1}{2} {||w||}^2$, we can use Method of Lagrange Multiplier to solve its Dual Problem. Solving the corresponding dual problem has the advantages of: Easier to solve with simpler constraints and only need to optimize one variable $\\alpha$ Able to introduce kernel function to generalize to non-linear cases Method of Lagrange Multiplier Problem formulation$$ \\min f(x_1, …, x_n) s.t. h_k(x_1, …, x_n)&#x3D; 0, k&#x3D;1,2,…,l $$That is to say, we decide to optimize function $f(x_1, …, x_n) $ with $l$ extra constraints. We can let $$ L(x, \\lambda &#x3D; f(x) + \\sum_{k&#x3D;1}^l \\lambda_k h_k(x) $$, where $L(x, \\lambda)$ is named Lagrange Function, and $\\lambda$ is NOT required to be non-negative. When solving the problem, we need to find $\\frac{\\partial L(x, \\lambda)}{\\partial x_i}$ for each $i \\in {1, …, n}$ and let them be $0$. This is called necessary condition of equality constraints (in order to get extremum). Strong DualityWe want to transform $$ \\min_w \\max_\\lambda L(w, \\lambda) \\Rightarrow \\max_\\lambda \\min_w L(w, \\lambda), \\quad (\\lambda_i \\ge 0)$$For function $f$, if we have $\\min \\max f \\ge \\max \\min f$, that is, the minimum of the possible maximums is still greate than the maximal possible minimums, we say there exists weak duality. If $f$ is convex optimization problem, we have strong duality.In order to handle the SVM problem, we require the Karush-Kuhn-Tucker (KTT) condition as the necessary and sufficient condition of strong duality. Karush-Kuhn-Tucker (KTT) conditionWe need to transform the optimization of inequality into optimization of equality:$$ \\min f(w)&#x3D;\\min \\frac{1}{2} {||w||}^2, g_i(w) &#x3D; 1 - y_i(w^Tx_i+b)\\le 0 $$into$$ L(w, \\lambda, a) &#x3D; f(w) + \\sum_{i&#x3D;1}^n \\lambda_i h_i(w) &#x3D; f(w) + \\sum_{i&#x3D;1}^n \\lambda_i [g_i(w) + \\alpha_i^2], \\lambda_i \\ge 0$$$g_i(w)$ is guaranteed to be $\\le 0$ for $\\forall i$, we can solve a series of non-zero values (a_i^2) to make every term of constraints $0$. Then we require $L$’s’ partial derivative to $w$, $\\lambda$ and $a$ equals $0$ to derive the KKT condtion:$$\\min L(w, \\lambda, a) \\Rightarrow \\min L(w, \\lambda) &#x3D; f(w) + \\sum_{i&#x3D;1}^n \\lambda_i g_i (w)$$The achieved minima must be greater than $0$, we have $\\sum_{i&#x3D;1}^n\\lambda_i g_i(w)\\le 0 $, $L(w, \\lambda) \\le p$ as a natural upper bound. So that our objective can be changed to $\\max_\\lambda L(w, \\lambda)$.The dual optimization problem after transformation: $$\\min_w \\max_\\lambda L(w, \\lambda) , \\lambda_i \\ge 0$$ Procedure of solving SVM problem Construct Lagrange Function $\\min_{w,b}\\max_{\\lambda}L(w,b,\\lambda)&#x3D; \\frac{1}{2} {||w||}^2 + \\sum_{i&#x3D;1}^n \\lambda_i [1 - y_i(w^Tx_i+b)] $ Transform to dual problem $\\max_\\lambda \\min_{w,b}L(w,b,\\lambda)$Find derivatives: $\\frac{\\partial L}{\\partial w}&#x3D;w - \\sum_{i&#x3D;1}^n \\lambda_i x_iy_i&#x3D;0$,$\\frac{\\partial L}{\\partial b}&#x3D;\\sum_{i&#x3D;1}^n \\lambda_i y_i&#x3D;0$ to get $\\sum_{i&#x3D;1}^n \\lambda_i x_iy_i&#x3D;w$, $\\sum_{i&#x3D;1}^n \\lambda_i y_i&#x3D;0$.Substitue into function:, that is, $$ \\min_{w,b}L(w,b,\\lambda)&#x3D;\\sum_{j&#x3D;1}^n\\lambda_i - \\frac{1}{2} \\sum_{i&#x3D;1}^n\\sum_{j&#x3D;1}^n \\lambda_i\\lambda_jy_iy_j(x_i \\cdot x_j) $$ We can tell that the above question is a quadratic programming problem and its scale is proportional to the number of training samples. It can be solved with Sequential Minial Optimization (SMO) algorithm. It optimizes one parameter a time and fixed the others. We just mentioned that SMO algorithm optimize only one parameter a time. However, we have a constraint $\\sum_{i&#x3D;1}^n \\lambda_i y_i&#x3D;0$ which must be satisfied. Thus, we update two parameters a time to solve this issue. With two selected parameters $\\lambda_i$ and $\\lambda_j$, we fix the other parameters and we have $$\\lambda_i y_i + \\lambda_j y_j &#x3D; c, \\lambda_i \\ge 0, \\lambda_j \\ge 0, c&#x3D; -\\sum_{k \\ne i,j}\\lambda_ky_k $$ Then we can have $\\lambda_j &#x3D; \\frac{c-\\lambda_iy_i}{y_j}$ so that we can replace $\\lambda_j$ with expression of $\\lambda_i$, find the partial derivative for $\\lambda_i$ and update the two paramteres, keep iterating until converge. By find partial derivative, we can have $w&#x3D;\\sum_{i&#x3D;1}^m \\lambda_i y_i x_i$. All the points corresponding to a $\\lambda_i &gt; 0$ are support vectors (non-support vectors correspond to $\\lambda_i&#x3D;0$), and we can find an arbitary support vector $x_s$ and substitute x to have $y_s(wx_s+b)&#x3D;1 \\Rightarrow y_s^2(wx_s+b)&#x3D;y_s$ to find $b$. $y_s^2&#x3D;1$, thus $b&#x3D;y_s - wx_s$. We can also find b via the mean value of the support vectors: $$b&#x3D;\\frac{1}{|S|}\\sum_{s\\in S}(y_s - wx_s) $$ With $w$ and $b$ found, we can construct the hyperplane $w^Tx+b&#x3D;0$ and use $f(x)&#x3D;sign(w^Tx+b)$ to decide the classification result. Parameters to be searched (tuned) in SVM C: decide the level of regularization. In fact, $C&#x3D;\\frac{1}{\\lambda}$ and $\\lambda$ is the regularization coefficient. (Is a squared l2-penalty) kernel: The kernel function adopted by SVM model. degree: The highest degree, when adapting Polynomial kernel function. gamma: Coefficent of the kernel function.Beside, you can decide whether to use heuristic shrinking, tolerance (the desired accuracy at convergence), size allocated to kernel cache, iteration times. You can also appoint random state for reproducing results. Soft MarginThe original SVM require the problem to be solved is linear separable. When this prerequisite is not satisfied, we can use soft margin to solve this problem. Originally, we require the two classes correspond to different signs, in a hyperplane expression. Now, we can allow SVM to misclassify on a handful of vectors.$$ y_i(w^Tx_i+b) - \\xi_i \\ge 1$$ which means that for such $i$, they do not meet the constraint and need the help of relax coefficient $\\xi_i$. In order to minimize the number of misclassified vectors, we would fix our objective (loss) (by appending a regularization term for the relax coefficients):$$ \\min_w \\frac{1}{2}{||w||}^2 + C \\sum_{i&#x3D;1}^m \\xi_i, \\quad g_i(w,b)&#x3D;1-y_i(w^Tx_i+b)-\\xi_i \\le 0, \\xi_i \\ge 0 $$Here, $C &gt; 0$ and can be understood as the penalty to the misclassified samples. If $C \\rightarrow \\infty$, then $\\xi_i \\rightarrow 0$ and the SVM (with soft margin) becomes linear separable SVM. Kernel FunctionThe essence of kernel function $K$ is to map vectors from low-dim Hilbert Space to high-dim. By this mean, we expect the samples which are not linear separable become separable, in the high-dim space.$$K(x,z)&#x3D;\\phi(x) \\cdot \\phi(z)$$Prerequisite: The Gram matrix formed by the set of all the points of the function is semi-positive definite. (a.k.a. complete inner product space) The introduction of kernel function is to solve the case of linear unseparable. It is costly to map samples to high-dimensional space before calculating dot product, thus kernel function can realize effectively calculating the dot product result of high-dimensional space, in low-dimensional space. Types of kernel functions Linear kernel: $x_i^Tx_j$ Polynomial Kernel: $(x_i^Tx_j+c)^d$ Radial basis function kernel: $exp(-\\frac{||x_i-x_j||_2^2}{2\\sigma ^2})$ Hyperbolic tangent kernel: $tanh(Kx_i^Tx_j+c), \\quad K&gt;0, c&lt;0$It is clear that only the latter three needs parameter tuning. Pros and ConsAdvantages: Supported by math theory, highly interpretable Do not rely on statistical method, simplify the regular classification and regression problem (solely rely on the support vectors, which are deterministic and crucial samples) Applying of kernel function can handle the unlinear tasks Computational complexity relies on the number of support vectors(rather than the dimensional of the sample space), avoid the curse of dimensionDisadvantages: Long training time (every time select a pair of parameters for optimization to maintain the sum unchanged). The complexity would be $\\mathcal{O}(n^2)$ where $N$ equals the number of training samples. If applied kernel function and need to store the matrix, extra space of $\\mathcal{O}(n^2)$ is required. The prediction rate is inversely proportional to the number of support vectors, leading to high complexity. Not suitable for scenarios which have millions, or even hundreds of millison of samples.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"General Knowledge","slug":"Data-Science/General-Knowledge","permalink":"https://umiao.github.io/categories/Data-Science/General-Knowledge/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://umiao.github.io/tags/Machine-Learning/"},{"name":"SVM","slug":"SVM","permalink":"https://umiao.github.io/tags/SVM/"}]},{"title":"DS-Study-Note-4 Metrics","slug":"DS-Study-Note-4","date":"2022-04-27T05:28:19.000Z","updated":"2022-04-27T06:47:18.035Z","comments":true,"path":"2022/04/26/DS-Study-Note-4/","link":"","permalink":"https://umiao.github.io/2022/04/26/DS-Study-Note-4/","excerpt":"Metrics are used for model training and evaluation. It reveals a model’s performance on a given dataset.","text":"Metrics are used for model training and evaluation. It reveals a model’s performance on a given dataset. PreliminaryWe use: $f$ to denote our model (function) $D$ to denote the dataset used, $m$ as the sample it contains. Error rateDefined as the number of incorrectly classified samples divide the number of total samples.$$ E(f;D) &#x3D; \\frac{1}{m}\\sum_{i&#x3D;1}^m \\mathbb{I}(f(x_i) \\ne y_i) $$Similarly, the continuous form:$$E(f;D) &#x3D;\\int_{x \\sim D} \\mathbb{I}(f(x_i) \\ne y_i)p(x)dx $$ AccuracyDefined as the number of correctly classified samples divide the number of total samples.$$ acc(f;D) &#x3D; 1 - E(f;D) &#x3D; \\frac{1}{m}\\sum_{i&#x3D;1}^m \\mathbb{I}(f(x_i) &#x3D; y_i) $$Similarly, the continuous form:$$acc(f;D) &#x3D; \\int_{x \\sim D} \\mathbb{I}(f(x_i) &#x3D; y_i)p(x)dx $$ Recall and Precision RateWhen the distribution of different classes are NOT balanced, we may be interested in how many samples of interest (positive samples) is found (Recall Rate) and how many of the filter samples of interest are correct (Precision Rate). We use TP &#x2F; FP &#x2F; TN &#x2F; FN to denote the frequency of True Positive &#x2F; False Positive &#x2F; True Negative &#x2F; False Negative. Precision Rate$$ P &#x3D; \\frac{TP}{TP + FP} $$ (TP + FP stands for all the samples classified as positive) Recall Rate$$ R &#x3D; \\frac{TP}{TP + FN} $$ (TP + FN stands for all the samples whose labels are positive) Tradeoff Pursuing Recall Rate and Precision Rate at the same time is often contradictory. This is because a high Precision means that the model would be cautious as possible so that many positive samples with lower confidence would be classified as negative. PR CurveIn order to unify the Precision and Recall rate and compare the performance of different model, PR Curve is proposed. The model is required to sort all the samples by the confidence (of being positive sample). Normally, for the first sample, the Precision would be 1 while the Recall would be the minima (close to 0). Predict the current sample to be positive, one-by-one, in the sorted order and keep calculating the corresponding Precison and Recall Rate. When all the samples are predicted to be positive, we have a Recall of 1 and the Precision reaches the minima. Break-Event Point of PR CurveIn order to compare the performance of two models, we can use the area under the PR curve as the metric. However, this value can be hard to calculate. Thus, we use the Break-Event Point to reach a balance between the Recall and Precision.It is calculated by the x-coordinate of the intersection point of the PR Curve and function $y&#x3D;x$.Ideally, we want both the Precision and Recall to be as high as possible. F1-ScoreF1-Score is a more commonly used metric to reach a balance between Precision and Recall, it is defined as the Harmonic Mean of these two:$$ \\frac{1}{F1} &#x3D; \\frac{1}{2} \\times (\\frac{1}{P} + \\frac{1}{R}) $$ Macro scope (Macro-F1) For N-class classification problem, first calculates $N$ F1-Scores on $N$ Confusion Matrix. Find the arithmetic mean of the $N$ F1-Scores. Micro scope (Micro-F1) Find $N$ groups of $TP$, $FP$, $TN$, $FN$ and find the arithmetic mean for each of the four. Calculate F1-Score with the mean $TP$, $FP$, $TN$, $FN$. GeneralizationWith the method describe above, you can similarly define micro-P, macro-P, micro-R, macro-R. Receiver Operating Characteristic (ROC) Curve Similar to PR Curve, sort all the samples by the confidence of predicting as positive. Find $$ TPR &#x3D; \\frac{TP}{TP + FN} $$, $$ FPR &#x3D; \\frac{FP}{TN + FP} $$.These two values stand for the ratio of correctly classified positive samples and incorrectly classified negative samples. For a 0-1 Classification task, a model based on random guess‘s ROC Curve should correspond to $y&#x3D;x$ and an accuracy of $0.5$. Area Under ROC Curve (AUC) With finite samples, you can draw the coordinate $(TPR, FPR)$ for each point and find the area under the ROC Curve. This is a common metric in evaluating models. Find a confidence threshold for ROC Curve Set the threshold to $\\inf$ which exceeds the confidence (score) of all the positive samples and they are all predicted as negative. In this case, both the TPR and FPR equal $0$. With the threshold goes down, these two would gradually raise to $1$. Define Sorting Loss based on AUC For each pair of positive-negative samples, if the positive sample achieves a score lower than the negative sample, add $1$ to the Loss. If equals, add $0.5$. This corresponds to the area above the ROC Curve and thus should be minimized. $AUC &#x3D; 1- L_{rank}$ Similarly, we can assign different weights to different types of mistakes a classifer made and draw the cost curve. We can find the weighted sum and for each point, draw the curve of expected cost (line determined by $(0, FPR)$ and $(1, FNR)$. Then we find the lower bound of all such lines, the area under this curve stands for the cost.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"General Knowledge","slug":"Data-Science/General-Knowledge","permalink":"https://umiao.github.io/categories/Data-Science/General-Knowledge/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://umiao.github.io/tags/Machine-Learning/"}]},{"title":"DS-Study-Note-3 Dimension Curse","slug":"DS-Study-Note-3","date":"2022-04-26T22:19:27.000Z","updated":"2022-04-27T05:26:41.457Z","comments":true,"path":"2022/04/26/DS-Study-Note-3/","link":"","permalink":"https://umiao.github.io/2022/04/26/DS-Study-Note-3/","excerpt":"DefinitionDimension curse stands for the troubles you would meet when processing high-dimensional data. E.g., computation of similarity, distance, neighbour or any metric based processing.","text":"DefinitionDimension curse stands for the troubles you would meet when processing high-dimensional data. E.g., computation of similarity, distance, neighbour or any metric based processing. The reason is that for high-dimensional space, the concept of distance will gradually fail, and even make any two points infinitely inseparable, even if they look different &#x2F; correspond to very different categories. As the dimension (of vector representation) grows, the data becomes more and more sparse and may correspond to higher variance and bias error. These phenomena are essentially caused by the same theory. However, the theoretical derivation is omitted here. Specifically, dimension curse can be categorized into the following more concrete problems: Distance concentration Combinational explosion Hubness Distance concentration As the number of dimension raises, for the queried point, the distance to its closest neighbour would converge to the distance to the furthest point. This also means that the difference of distance between arbitrary two points would be negligible. The difference of distance may be small enough, for a dimension of 20. The distance may remain effective, if there lies inherent clusters in the data and these clusters are distant from each other. Or, if many of the data’s dimensions are redundant (the data can be embedded into a space with much smaller dimension). In high-dimensional space, small change of the neighbourhood radius determines the difference of selecting only ONE point or selecting ALL the datapoints. This is because the volume ratio of a fixed radius hypersphere to a unit radius hypersphere will be close to 1. Increase of relevant features would be beneficial to the model. Increase of irrelevant features would impair the model’s performance. Distance under different dimension and space CANNOT be compared with each other. Combinational explosion As the dimensionality increases, a larger percentage of the training data resides in the corners of the feature space. It is also much more difficult to traverse the increasing search space as the size of the search space grows exponentially, as shown in the above image. A complex search space may correspond to the same configuration (object &#x2F; training &#x2F; testing sample) in the low-dimensional case, resulting in overfitting. Training samples with larger scale are required to suppress overfitting (on data with high dimensions). Algorithms like Random Forest can restrict the number of features used (in one tree). Hubness With the increase of dimension, a handful of points are significantly more frequent to become the nearest neighbour of other points. This is also called Hubness. If we record the frequency of each point becoming the nearest neighbour of another point, we can find that is frequency follows Zipf’s Law and is right-skewed heavily. Generally, points close to the mean value of the entire dataset become hubs more easily. When cluster exists, data points close to the mean value of the cluster are more likely to become hubs as well. Outlier &#x2F; Anti-hubs These points are most distant from the majority of other points. Anti-intuitive things may happen, i.e., hubs exist in the low-density region of the high-dimensional space, but the hubs are close to the other points. At the same time, anti-hubs may exist in the high-density region of the high-dimensional space, but the anti-hubs are distant to the other points (which makes them outliers). This can be viewed as a mismatch between the probabilistic density and distance distribution. Solution (to dimension curse) For the cases of Multicollinearity and Duplication of Components(redundancy) happen, we can simply remove the redundant variables after evaluation. In the case when each dimension (feature) contributes equally to the model’s result, methods like Dimensional Reduction (including CNN, Convolutional Neural Network) can be applied. Dimensional Reduction’s Disadvantages: Converted data points DO NOT represent original features. Less interpretable, hard to visualize, weaker theoretical support.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"General Knowledge","slug":"Data-Science/General-Knowledge","permalink":"https://umiao.github.io/categories/Data-Science/General-Knowledge/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://umiao.github.io/tags/Machine-Learning/"}]},{"title":"DS-Study-Note-2 Bias VS Variance","slug":"DS-Study-Note-2","date":"2022-04-23T07:41:01.000Z","updated":"2022-04-26T22:18:22.843Z","comments":true,"path":"2022/04/23/DS-Study-Note-2/","link":"","permalink":"https://umiao.github.io/2022/04/23/DS-Study-Note-2/","excerpt":"The target of Machine Learning is to fit an (unknown) distribution. There lies three possible error: bias, variance and irreducible error.","text":"The target of Machine Learning is to fit an (unknown) distribution. There lies three possible error: bias, variance and irreducible error. The irreducible error CANNOT be avoided with any algorithm as it can be viewed as the result of unknown factor, noise, accidents, etc. Thus, we would focus on the bias and variance error. Definition Bias can be understood as the accuracy of the model, i.e., the ability to estimate the output value accurately. Variance can be understood as the stability of the model, i.e., the ability of resisting the noise and disturbance contained by the input. I also understood this ability as being able to recognize similar inputs and generate similar results for them. Example Applying K-fold cross validation can reduce the influence casted by the outliers and enhance the generalization ability, which reduces the variance error. At the same time, part of the data is not used for training, which impairs the model’s fitting ability and increase the bias error. An intuition is that, a more complex model is more sensitive to the noise contained by the input, which makes the output less stable (higher variance error). At the same time, a simpler model more easily ignores the random noise and difference of distribution between the training set and the testing set. Tradeoff and AnalysisThe variance of the parameters you are estimating can be reduced, at the cost of increasing bias. If you would like to generalize a model trained on a certain training set, then you CANNOT minimize the bias and variance error at the same time. Bias: Bias error comes from the erroneous assumptions of the learning algorithm. High bias error corresponds to underfitting. Bias error measures the closeness between the distribution you modeled and the expectation of the real distribution. Introduction of bias term in linear models aims at simplying the learning process without introducing way more complicated distribution which makes the model hard to generalize. At the same time, such models would fail to solve the more complicated models which do not meet the assumption (that this problem can be approximated by linear model). Variance: Variance error comes from the noise &#x2F; fluctuation &#x2F; disturbance of the training set. High variance error probably means that you are modeling on the random noise of the training set, which cannot be generalized, and this means overfitting. Variance error reveals the level of concentration of your model.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"General Knowledge","slug":"Data-Science/General-Knowledge","permalink":"https://umiao.github.io/categories/Data-Science/General-Knowledge/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://umiao.github.io/tags/Machine-Learning/"}]},{"title":"DS-Study-Note-1 Overfitting and Brief Introduction on Decomposition and Regularization","slug":"DS-Study-Note-1","date":"2022-04-22T16:23:14.000Z","updated":"2022-04-23T07:37:49.936Z","comments":true,"path":"2022/04/22/DS-Study-Note-1/","link":"","permalink":"https://umiao.github.io/2022/04/22/DS-Study-Note-1/","excerpt":"Overfitting is a modeling error in statistics that occurs when a function is too closely aligned to a limited set of data points. —- Definition ranked 1st in Google","text":"Overfitting is a modeling error in statistics that occurs when a function is too closely aligned to a limited set of data points. —- Definition ranked 1st in Google Definition Overfitting stands for making excessive learning steps on the training set, which results in the model extracting noise of the training set as valid pattern to fit the training set’s distribution. In this case, the trained model would show low performance on the testing set and new data (real-world data). Possible Causes: The volume of the training data is too small. The data distribution of the training set data does NOT subject to the testing and real business data. This also means that the iid (identically and independent distributed) assumption is not satisfied. There exists noise in the training set. Too many iteration times. Fail to learn correct features with ability of generalization and representative. The overfitting MAY be phenomenon of information leakage, i.e., too complicated model remembers the training which makes the inference equivalent to the table look-up. Solutions: Discard some features. This can be implemented by Feature Selection or simply randomly discard a subset. This process can be: Conducted manually. Randomly. (Random Forest) Decided by Model Selection Algorithm. E.g., PCA(Principal component analysis). PCA: Solve the eigen-vector of the covariance matrix. It is obvious that the larger the covariance is, the more useful the corresponding eigen-value is. Find the largest k eigen-values and use their corresponding eigen-vectors to form a matrix as the PCA output. (You can also use SVD for such decomposition.) You can also use dimensional reduction tools like LR(lower–upper) decomposition, SVD(Singular Value Decomposition). Introduce regularization. Introduce drop-out layer when training network. Use Early-Stop to achieve the tradeoff the generalization ability and convergence on the training set. (In this case, evaluation set is required for observation.) A combination of methods above. Adopt models like Random Forest. Any method which is believed to be able to control the model’s complexity. E.g., control the depth, number of trees. Selection Bewteen the L1 and L2 Regularization Term L1 - LASSO (Least Absolute Shrinkage and Selection Operator): cast penalty on the sum of the absolute value of the model parameters.Regularize all the parameters equally. Able to transform some parameters into 0. (make the model sparse)$${L1}_{reg} &#x3D; \\lambda \\sum_{j&#x3D;1}^p |\\beta _j|$$ L2 - Ridge Regression: cast penalty on the sum of the square value of the model parameters.$${L2}_{reg} &#x3D; \\lambda \\sum_{j&#x3D;1}^p \\beta _j^2$$ In essence, these two regularization method is to conduct L1 &#x2F; L2 Normalization on the model parameters and add the normalized term to the Loss Function for optimization.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"General Knowledge","slug":"Data-Science/General-Knowledge","permalink":"https://umiao.github.io/categories/Data-Science/General-Knowledge/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://umiao.github.io/tags/Machine-Learning/"}]},{"title":"SQL-Study-Note-11 User and Privilege Management","slug":"SQL-Study-Note-11","date":"2022-04-21T19:39:39.000Z","updated":"2022-04-21T22:06:33.950Z","comments":true,"path":"2022/04/21/SQL-Study-Note-11/","link":"","permalink":"https://umiao.github.io/2022/04/21/SQL-Study-Note-11/","excerpt":"Most data science practitioners would not be granted the privilege of managing the database system (not even the privilege to update &#x2F; delete), so…","text":"Most data science practitioners would not be granted the privilege of managing the database system (not even the privilege to update &#x2F; delete), so… Create and Manage User1234567891011121314CREATE USER join@&#x27;%.google.com&#x27; IDENTIFIED BY &#x27;1234&#x27;;-- You can restrict the domain of user with this method.-- You can also specify an ip after &#x27;@&#x27;.-- &#x27;1234&#x27; Stands for the password.SELECT * FROM mysql.USER;-- Retrieve the information of all the users.-- It is also supported to use GUI interface to manage the user,\\-- their host to log in with, etcDROP USER bob@gmail.com;-- Drop a user.SET PASSWORD (john) = &#x27;1234&#x27;;-- Reset a user&#x27;s password.-- It is also supported to EXPIRE PASSWORD for one user.-- So that he would be required to change the password by next log in. Privilege Management12345678GRANT SELECT, INSERT, UPDATE, DELETE, EXECUTE ON sql_store.* TO user_a;-- An example of granting the privileges.SHOW GRANTS;-- Show all the grantsREVOKE privilege .. ;-- Revoke specified granted privilege. Refer documentation for more related instructions.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"SQL-Study-Note-10 Index","slug":"SQL-Study-Note-10","date":"2022-04-21T17:53:59.000Z","updated":"2022-04-21T19:55:39.737Z","comments":true,"path":"2022/04/21/SQL-Study-Note-10/","link":"","permalink":"https://umiao.github.io/2022/04/21/SQL-Study-Note-10/","excerpt":"Index can be used to find the row (line) numbers corresponding to the value being queried. Index is added to certain columns and is stored in memory (RAM) for most times.","text":"Index can be used to find the row (line) numbers corresponding to the value being queried. Index is added to certain columns and is stored in memory (RAM) for most times. Create IndexIndex can speed up the query, however, it would also increase the size (memory comsuption) of database as well as the cost of maintenance. It is usually implemented by binary tree in database systems. 12CREATE INDEX idx_state ON customers (state);-- Specify a column of a table to create an index. Explain and ANALYZE1EXPLAIN SELECT * FROM ... Add EXPLAIN before a sql query would get explanatory information instead of the query result. E.g., which information is used and how many records are went through, for performance evaluation. 123456SHOW INDEXES IN customers;-- Reveal all the indexes in the customers table.-- You can find out the indexes added, the cardinality and name.ANALYZE TABLE customers;-- Indexes includes primary index, secondary index, etc.-- You can use the ANALYZE command to view the statistics and values of a table Different Index Types prefix indexYou should create prefix index, instead of index on the entire column, for acceleration.12345CREATE INDEX idx_n ON customers (last_name(20));-- In this case, the last_name column would be grouped and indexed according to the first 20 characters only.COUNT (DISTINCT LEFT(last_name, 10));-- You can use this way to analyze the performance of creating prefix index on the first 10 characters. -- If the number of distinct values is large enough, then this prefix is able to differentiate the possible values. Full-Text IndexThe idea of such index is similar to the implementation principles of Search Engines. For all the non-stop words, record the corresponding passages (rows) and the position where they appear.1234567CREATE FULLTEXT INDEX idex_t_body ON posts (title, body);-- FULLTEXT INDEX can monitor multiple columns.SELECT * FROM posts WHERE MATCH(title, body) AGAINST (&#x27;react redux&#x27;);-- Search &#x27;react redux&#x27; in the two columns: title and body. They are viewed as TWO words.MATCH(title, body) AGAINST (&#x27;react -redux +form&#x27; IN BOOLEAN MODE);-- It is possible to exclude some words by adding a &#x27;-&#x27;.-- So the above query matches contents including react/form and without redux. Composite indexesEnable indexing on multiple columns in order to solve the issue that too many results are returned after filtering with the primary index. You can use the appearing order of the columns to decide the filtering priority of the Composite indexes.1CREATE INDEX idx_n ON customers (last_name, state, points); MySQL supports Composite index to include at most 16 columns. However, including 4-6 columns is fairly enough in practice. At the same time, Composite index supports sorting on the columns so that the more frequently used columns appear ahead. In fact, you can decide the priority of the indexes being used, by changing the appearing order of the conditions of the WHERE clause. You can use the command of: 1USE INDEX idx_name; to force MySQL to use certain index, even it is not optimal. You can NOT make full use of the index, if column is included in the sql expression (e.g. in WHERE condtion). So extract the required column first before conducting transformation. Sorting and PerformanceSorting should be avoided as possibleIt is because it is costy. You should utilize Indexes for the purpose of sorting, as possible. 1234SHOW STATUS;-- Show the variables being used in MySQL server.last_query_cost;-- You can measure the cost of last query by this mean. It should be noted that, for column a and b, ORDER BY clause can use the Index for sorting if the order is among one of these: ORDER BY a ORDER BY b ORDER BY a, b ORDER BY a DESC, b DESCAny other order would introduce external sorting operation (which means Scaning the Entire Table!).There is an exception that if WHERE clause can locate to a Single Column, then external sorting would not happen. If you can achieve the result of query solely rely on the index, then it is a Overlay Index. Duplicate index and Redundant Index Duplicate Indexes: Repeatly create the same index, e.g. on columns (a, b, c). Redundant Indexes: One index’s functionality is completely covered by the other. E.g., creating index for column (a) and (a, b) at the same time. (The latter can cover the prior)","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"SQL-Study-Note-9 Data Modeling, Constraint and Normalization Form","slug":"SQL-Study-Note-9","date":"2022-04-20T22:22:22.000Z","updated":"2022-04-21T17:53:30.704Z","comments":true,"path":"2022/04/20/SQL-Study-Note-9/","link":"","permalink":"https://umiao.github.io/2022/04/20/SQL-Study-Note-9/","excerpt":"Data Modelling Pipeline Understand the requirements; Build a conceptual Model; Build a logical model; Build a physical model.","text":"Data Modelling Pipeline Understand the requirements; Build a conceptual Model; Build a logical model; Build a physical model. Foreign Key ConstraintAlthough modify the primary key IS NOT recommended, we would consider the update to the foreign key caused by the primary key anyway. Option (strategy of updating): restrict: restrict modification cascade: update the foreign keys according to the primary key set null: set the corresponding foreign key in the foreign table into NULL no action: reject the update It is highly not recommended to use set null, as it would result in organ record in the corresponding tables (no idea which id it belongs to). Dataset NormalizationNF-1 (First Normal Form):Each record unit (element specified by row and column index) should contain a single value only and contain NO duplicate column.E.g., if you want to add tags to the Course table, you should extract tags into an independent table (and use id mapping to retrieve the tags) for it to be extendable. NF-2 (Second Normal Form):Frist is to satisfy NF-1. Also, every non candidate-key attribute depends on the whole candidate keys (that is to say, they must not depend on a true subset of the candidate keys). That is to say, each table should contain exactly one entity category only. E.g., a table stores course information should NOT contain information like the enroll time of each student. If there is an attribute which does not belong to the entity represented by this table, create a new table to store it. NF-3 (Third Normal Form):Frist is to satisfy NF-2.Also, all the attributes of the table should be determined by candidate key (for example, id) and should NOT be determined by the other non-primary attributes.That is to say, all the columns of the table should NOT be generated &#x2F; derived by the other columns, to avoid errors caused by duplicate storage and erroneous update. Data Model $ \\leftrightarrow$ TableIn MySQL, you can use forward engineer to convert data model into actual tables. Its essence is to generate sql script for database &#x2F; table generation, with the specified data model &#x2F; structual graph.The script would be like: 123CREATE schema IF NOT EXISTS … USE schema … CREATE TABLE … It is also supported to regenerate and fix the table via modifying the data model. In this case, you should select synchronize model instead of forward engineer. For those table with foreign keys, if you want to update the table, foreign keys would prevent you from doing so (as a constraint). You should first drop all the foreign keys and then reconstruct it to link to correlated tables. Reverse EngineerIt is also supported to generate data model (graphs) from existing tables. That is reverse engineering.It is highly recommended that you only put ONE database into a single data model, unless these databases are really highly correlated. Data Management Opertion (Via SQL Script)Create of Database: 12CREATE DATABASE IF NOT EXISTS name;-- Make well use of EXISTS clause to avoid error. Create Table:1234567CREATE TABLE cus ( c_id, INT PRIMARY KEY AUTO_INCREMENT, first_name VARCHAR(50) NOT NULL, points INT NOT NULL DEFAULT 0, email VARCHAR(250) NOT NULL UNIQUE ) Use Alter Table to update Table:1234567ALTER TABLE customers ADD last_name VARCHAR(50) NOT NULL AFTER first_name-- You can also use MODIFY / DROP instead of ADD to edit and delete the existing and known columns. Add Constraints (e.g., Foreign Key):12345FOREIGN KEY fk_col_name (c_id 表中列名) REFERENCES customers(c_id) ON UPDATE NO ACTION ON DELETE NO ACTION -- You cannot drop a table without droping its foreign key constraints in advance. Charset1234567SHOW CHARSET; -- You can use this command to show the charset.CREATE / ALTER DATABASE db_name CHARACTER SET lain1;-- About modifying / altering the charset (for a dataset).CREATE / ALTER TABLE () CHARACTER SET latin1;-- You can set the charset in the table level, too.-- Also, charset can be set in column level, just like adding constriants like &#x27;NOT NULL&#x27; Database Engine &#x2F; Storage Engine1234SHOW ENGINES; -- Show all the engines.ALTER TABLE customers ENGINE = InnoDB;-- Specify the engine for a table.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"Missing Value Imputation in Traffic Data","slug":"Missing-Value-Imputation-in-Traffic-Data","date":"2022-04-19T21:52:23.000Z","updated":"2022-05-05T23:35:56.102Z","comments":true,"path":"2022/04/19/Missing-Value-Imputation-in-Traffic-Data/","link":"","permalink":"https://umiao.github.io/2022/04/19/Missing-Value-Imputation-in-Traffic-Data/","excerpt":"Lost of sensor-generated data can be very common. The methods of imputation can be coarsely categorized into: 1. Prediction methods; 2. Interpolation methods; 3. Statistical Learning methods.","text":"Lost of sensor-generated data can be very common. The methods of imputation can be coarsely categorized into: 1. Prediction methods; 2. Interpolation methods; 3. Statistical Learning methods. Imputation problem &amp; Model Formulation1Let $Y_c$ be the traffic dataset persists for $N$ consecutive days,$$ Y_c &#x3D; [Y(1), …, Y(N)] $$in which the ith $Y(i)$ be noted as 1-D vector$$ Y(i) &#x3D; [y_i(1), …, y_i(D)]^T, i \\in [1, N] $$.Concatenate all the vectors we have together, we would have:$$ Y_{series} &#x3D; [y(1), …, y(D \\times N)]^T $$. Typical traffic data includes the speed and number of vehicles on a certain lane at a time. These data can form a numerical time sequence. Such data can be collected by sensor installed on the roads or along the roadsides. ARIMA-based method2ARIMA stands for Autoregressive Integrated Moving Average. In ARIMA(p, d, q), p denotes the order of the autoregressive part, d is the degree of differencing and q is the order of moving average part: $$ (1 - \\sum_{i&#x3D;1}^p \\alpha_i L^i) (1 - L)^d y(t) &#x3D; (1 + \\sum_{i&#x3D;1}^q \\beta_i L^i) y(t) \\xi(t) $$ and L is the backshift operator, $L_y(t) &#x3D; y(t- 1)$. $\\xi(t)$ is white Gaussian noise. First train this model with the known series and impute missing data one by one. The imputed data would be used as known data for next prediction. We use Akaike information criterion to determine $p$ and $q$. $d$ is suggested to be set to $1$. BNs-based imputation method3BN stands for Bayesian Netowrk. Based on the known dataset, learn the distribution model of multivariable variants $Y_{mv}(t) &#x3D; [y(t-m), …, y(t)]^T$.Assume this learning target as Gaussian mixture model (GMM).Use split and merge expectation maximisation algorithm to determine the model parameters. With a learnt GMM, missing data of $y(t)$ can be estimated as the expectation foregoing value from the latest $m$ items, as: $$ \\hat y(t) &#x3D; E[y(t) | y(t-m), …, y(t-1)] $$ k-NN based imputation method4Weighted k-NN is a non-parametric estimation method. Selection S-step (Selection Step):Use a metric to find $k$ nearest traffic daily flow vectors to the corrupted vector $Y(i)$ in pattern-similar from the dataset $Y_c$.Metrics can be Euclidean distance and Pearson correlation, for examples. Then, the lost entry &#x2F; dimension of $Y(i)$ can be imputed with the mean value of the (entries of the) $k$ vectors. Imputation I-step: (Imputation Step)Because our k-NN algorithm is weighted, then we need to find the weighted average of the k entries (averaged by the correlation coefficent given by the selected metric). Grid search and other optimization methods may be applied to determine best $k$. LLS-based imputation method5Selection S-step (Selection Step):Exactly the same as the above k-NN method. Imputation I-step: (Imputation Step)Decompose k selected vectors dataset into matrix $A$ and $B$. The dimension should be corresponding to the missing part of $Y_{mis}(i)$ and observed part $Y_{obs}(i)$ of $Y(i)$. We would have $$ \\hat Y_{mis}(i) &#x3D; B((A^TA)^{-1}A^T Y_{obs}(i)) $$. This is just a pseudo-inverse, $A$ should be full-ranked&#x2F; MCMC-based imputation method 6, 7First assume the entire data sequence $Y$ follows a certain distribution, e.g., Gaussian distribution.The conditional expectation $E[Y_{mis}|Y_{obs}, \\Phi]$ would be approximated by MCMC(Markov chain Monte Carlo) with DA(Data Augmentation), since the expectation is hard to be solved precisely due to its high dimension. Here $\\Phi$ stands for the parameter of the selected distribution. The MCMC with DA is a special case of Gibbs sampler described as follows: Imputation I-step: (Imputation Step)Given a current estimated model parameter $\\Phi ^k$, this step uses the conditional probability $Y_{mis}^{k+1}&#x3D;p(Y_{mis}|Y_{obs}, \\Phi)$ to simulate missing values for each observation independently. Posterior P-step:Use $p(\\Phi | Y_{obs}, Y_{mis}^{k+1})$ to update model parameter $\\Phi$. In this manner, a Markov chain of $(Y_{mis}^{1},\\Phi ^{1} )$, …, $(Y_{mis}^{N},\\Phi ^{N} )$ should be constructed. The missing data is estimated as$$ \\hat Y_{mis} &#x3D; \\frac{1}{N_{sample} - N_{burn-in}} \\sum_{t &#x3D; N_{burn-in + 1}}^{N_{sample}} Y_{mis}^t $$. The first $N_{burn-in}$ samples would be discarded. One feasible parameter setting is $N_{sample}&#x3D;1500$ and $N_{burn-in}&#x3D;500$. I believe the introduction of burn-in is to allow the model sometime to converge. PPCA-based imputation method8PPCA stands for Probabilistic Principal Component Analysis. It assumes that the observed data depends on latent varaiables $$ Y &#x3D; Wx + \\mu + \\epsilon $$ where $Y$ is a D-dimensional vector of observed data, $x$ is a q-dimensional latent varaible defined Gaussian distribution and $\\epsilon$ is isotropic noise. $x \\sim N(0,1)$, $\\epsilon \\sim N(0, \\sigma ^2 I)$ and $\\mu$ stands for a base mean value. Use Expectation Maximisation (EM) method to find a set of imputed data which best fit the above distribution. Concrete steps of EM method: Expectation E-step:Find out the expectation of completed log-likelihood function with previous estimated parameters $\\Phi ^k$ and observed data part $Y_{obs}$:$$ Q(\\Phi | \\Phi ^k) &#x3D; E_{X, Y_{mis} | Y_{obs}, \\Phi ^k}[log p_c (Y_c, X|\\Phi ^k)] $$We can use this to update our guess of the missing data part $Y_{mis}^k$ and latent data $X^k$. Maximisation M-step:Computiong parameter space $\\Phi$ by maximising the expectation of log-likelihood in E-step:$$ \\Phi ^{k+1} &#x3D; \\arg \\max_{\\Phi} Q(\\Phi | \\Phi ^k) $$ Conditional expectation $E[Y_{mis} | Y_{obs}, \\Phi]$ can be difficult to calculate and can approximate with MCMC with DA. Dataset Intended to Use: ComparisonPrediction and interpolation methods mentioned cannot capture stochastic variations in daily traffic flow. On the contrary, statistical learning methods could achieve traffic flow information by emphasising the statistical characteristics of traffic flow. Shorting coming of such methods: Cannot handle the situations in which neighbour (data points) DO NOT even exist. References [1] [Li, Yuebiao, Zhiheng Li, and Li Li]”Missing traffic data: comparison of imputation methods.” IET Intelligent Transport Systems 8.1 (2014): 51-57. [2] [Ahmed, Mohammed S., and Allen R. Cook]Analysis of freeway traffic time-series data by using Box-Jenkins techniques. No. 722. 1979. [3] [Ueda, Naonori, et al.] “Split and merge EM algorithm for improving Gaussian mixture density estimates.” Journal of VLSI signal processing systems for signal, image and video technology 26.1 (2000): 133-140. [4] [Troyanskaya, Olga, et al.] “Missing value estimation methods for DNA microarrays.” Bioinformatics 17.6 (2001): 520-525. [5] [Kim, Hyunsoo, Gene H. Golub, and Haesun Park.] “Missing value estimation for DNA microarray gene expression data: local least squares imputation.” Bioinformatics 21.2 (2005): 187-198. [6] [Ni, D., Leonard II, J.D.] “Markov chain Monte Carlo multiple imputation using Bayesian networks for incomplete intelligent transportation systems data”, Transp. Res. Rec., 2005, 1935, (1), pp. 57–67 [7] [Gilks, W.R., Richardson, S., Spiegelhalter, D.J.]”Markov chain Monte Carlo in practice” (Chapman &amp; Hall, London, 1996) [8] [Tipping, M.E., Bishop, C.M.]”Mixtures of probabilistic principalcomponent analyzers”, Neural Comput., 1999, 11, (2), pp. 443–482","categories":[{"name":"UCLA","slug":"UCLA","permalink":"https://umiao.github.io/categories/UCLA/"},{"name":"Course Study","slug":"UCLA/Course-Study","permalink":"https://umiao.github.io/categories/UCLA/Course-Study/"},{"name":"ECE209 in 2022 spring","slug":"UCLA/Course-Study/ECE209-in-2022-spring","permalink":"https://umiao.github.io/categories/UCLA/Course-Study/ECE209-in-2022-spring/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"UCLA","slug":"UCLA","permalink":"https://umiao.github.io/tags/UCLA/"}]},{"title":"SQL-Study-Note-8 - Data Type of MySQL","slug":"SQL-Study-Note-8","date":"2022-04-17T05:47:13.000Z","updated":"2022-04-20T22:19:31.193Z","comments":true,"path":"2022/04/16/SQL-Study-Note-8/","link":"","permalink":"https://umiao.github.io/2022/04/16/SQL-Study-Note-8/","excerpt":"Data Type of MySQL Suggestion for Data Type Selection","text":"Data Type of MySQL Suggestion for Data Type Selection VARCHAR: For short string set to 50, for long string set to 255. Maximum length is 65535, 64KB. (Note that Char Type is fix-lengthed). MEDIUMTEXT: 16M document &#x2F; LONGTEXT: 4GB document &#x2F; TINYTEXT: 255 Bytes &#x2F; TEXT: equal to VARCHAR, 64KB Note that Chinese charcter takes 3 Byte each, so we allocate $3n$ Bytes to string with length of $n$ (pick the upper bound). Integer Type: TINYINT: 1 Byte, UNSIGNED TINYINT and SMALLINT : 2 Byte, MEDIUMINT: 3 Bytes, INT: 4 Bytes, BIGINT: 8 Bytes. Leading zero filling is supported: INT(4) -&gt; ‘0003’ Float Type: DECIMAL(p,s) defines the int length and decimal length, it can be viewed as a fixed-point decimal (int). DECIMAL &#x3D; DEC &#x3D; NUMERIC &#x3D; FIXED FLOAT: 4 Bytes, DOUBLE: 8 Bytes (Expressed in exponential form) Boolean: BOOL &#x2F; BOOLEAN, 1 bit Enumerate Type: ENUM(‘a’, ‘b’, ‘c’): value must be selected from the given set. This is not a good design as it is complex to change the domain of legal values, you may even need to rebuild the entire table. It is not reusable itself. Creating a table to store the mapping relationship is recommended. Time: Timestamp can only store date up to 2038 AD as it takes 4 Bytes. To store later time, use Datastamp. BLOB for Binary Large Object: TINYBLOB &#x2F; BLOB &#x2F; MEDIUMBLOB &#x2F; LONGBLOB takes 255 Bytes &#x2F; 65KB &#x2F; 16 MB &#x2F; 4 GB, respectively. Store files in the FileSystem as possible rather than store them in the database. Otherwise, you may come into problems like high memory usage, slow copying, low performance, indirect and complicated IO, etc. JSON: In order to set JSON object, you can use string form like: ‘{ “k”:v}’. You can also create the object with function:.1JSON_OBJECT(&#x27;weight&#x27;, 10, &#x27;dimensions&#x27;, JSON_ARRAY(1, 2, 3)); In order to extract the attributes included in JSON, you can use 12345678JSON_EXTRACT(properties, ‘$.weight’);-- while the properties stand for the desired column name / key of the JSON object. JSON_EXTRACT(properties, ‘$.weight.data.sub.time’);-- You can use multiple dots to get the nested attributes.properties -&gt; ‘$.weight’; -- Semi-CPP syntax is also supportedproperties -&gt; ‘$.weight[idx]’;-- Can specify a certain element of the JSON list use &#x27;[]&#x27; At the same time, it should be noted that the returned results are still in JSON format. 1234properties -&gt; ‘$.weight’;-- would return something like &quot;sony&quot;, which leads to problem when comparing with other resultsproperties -&gt;&gt; ‘$.weight’;-- is able to return sony, without &quot;&quot; In terms of updating partial attributes, you can use JSON_SET. Here SET stands for the motion of setting. 123SET properties = JSON_SET / JSON_REMOVE(properties, ‘$.weight’, 30, ‘$.age’, 10) WHERE id=1-- JSON_REMOVE is used to remove attributes The above example can be used to set part of the attributes in JSON object properties.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"SQL-Study-Note-7 - Transactions","slug":"SQL-Study-Note-7","date":"2022-04-17T05:07:50.000Z","updated":"2022-04-17T05:47:05.601Z","comments":true,"path":"2022/04/16/SQL-Study-Note-7/","link":"","permalink":"https://umiao.github.io/2022/04/16/SQL-Study-Note-7/","excerpt":"Transactions Principles of Transaction (ACID) Atomicity Consistency Isolation Durability","text":"Transactions Principles of Transaction (ACID) Atomicity Consistency Isolation Durability 12345START TRANSACTION;// Instruction block to be executed-- If only part of the transaction is done and the connection to server is lost, the finished part would be rolled backCOMMIT; A transaction would lock the lines and tables to be updated so that they are untouchable to other transactions.If one transaction comes into locked resources, it would wait the owner of the lock to finish, or until it expires the time limit itself.At the same time, ROLLBACK is also a SQL instruction and keyword. Different level of transaction isolation: 12SET TRANSACTION ISOLATION LEVEL SERIALIZABLE;-- Note that this setting is session-leveled. It comes with the tradeoff that the higher the isolation level is, the lower performance it reaches. (In extreme situation, serializable level would not benefit from distributed architecture.) Read Uncommitted: May read uncommited data (dirty read). Rarely used in actual application as the performance improvement is very limited. Read Committed: Default level for most DBMS (Oracle, SQL server). However, it may counter Nonrepeatable Read: same select may have different result within one transaction. This is due to the UPDATE operation made by other transactions and can be solved by Line Level Lock. Repeatable Read: Applied with Line Level Lock. However, it can still encounter Phantom Reads (caused by the delete &#x2F; insert operations made by other transactions). Require Table Level Lock to solve. Serializable: Ban the parallel processing and sort all the transactions.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"SQL-Study-Note-6 - Trigger and Events","slug":"SQL-Study-Note-6","date":"2022-04-17T04:46:07.000Z","updated":"2022-04-17T05:05:21.886Z","comments":true,"path":"2022/04/16/SQL-Study-Note-6/","link":"","permalink":"https://umiao.github.io/2022/04/16/SQL-Study-Note-6/","excerpt":"TriggerTriggers are the code blocks executed automatically before insertion &#x2F; update &#x2F; delete take effect.","text":"TriggerTriggers are the code blocks executed automatically before insertion &#x2F; update &#x2F; delete take effect. 123456789DELIMITER $$CREATE TRIGGER payments_after_insert AFTER/BEFORE INSERT/UPDATE/DELETE ON payments FOR EACH ROWBEGIN -- You can either write SQL codes here or call the existing procedure...END $$DELIMITER ; Extensive syntax123NEW -- Return the just inserted line.OLD -- Return the just deleted line.NEW.amount -- Can use . to specify a column These two (NEW &#x2F; OLD) are keywords of MySQL.Triggers can be used to modify data of any table EXCEPT the table which is being listened by the trigger. This is because trigger can trigger itself and results in infinite loop. 12SHOW TRIGGERS -- Show all the created triggers.ShOW TRIGGERS LIKE &#x27;c%&#x27; -- Filter the triggers. Triggers can be also used for auditing purpose, i.e., record the executor, attribute and timestamp when an operation is done. EVENTEvents are periodically triggered codes for multiple tasks. 12SHOW VARIABLES -- Show all system variables.SET GLOBAL event_scheduler=ON / OFF -- Switch the event_scheduler Create Event: 12345678DELIMITER $$CREATE EVENT yearly_delete_state_audit_rows ON SCHEDULEEVERY 1 YEAR STARTS ‘2020-01-01’ ENDS ’2029-01-01’DO BEGIN -- Note that DO is required here....END $$DELIMITER ; Two ways to calculate the diff of time: 12NOW() – INTERVAL 1 YEAR;DATESUB(NOW(), INTERVAL 1 YEAR); Activate &#x2F; Deactivate Events: 1ALTER EVENT e_name DISABLE;","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"SQL-Study-Note-5 - Stored Procedure and User Defined Functions","slug":"SQL-Study-Note-5","date":"2022-04-17T02:01:32.000Z","updated":"2022-04-17T04:48:52.424Z","comments":true,"path":"2022/04/16/SQL-Study-Note-5/","link":"","permalink":"https://umiao.github.io/2022/04/16/SQL-Study-Note-5/","excerpt":"Stored ProcedureMotivationGenerally, developers prefer not to interpret string as SQL codes &#x2F; instructions due to security concerns.","text":"Stored ProcedureMotivationGenerally, developers prefer not to interpret string as SQL codes &#x2F; instructions due to security concerns. You can wrap the query and update functionality with Stored Procedure. DBMS is able to further optimize the stored procedure and enhance the security. The stored procedure itself is similar to a function implementation. Syntax Implementation123456DELIMITER $$CREATE PROCEDURE get_clients()BEGIN SELECT * FROM clients;END $$DELIMITER ; The reason of changing DELIMITER is that, we need to use ; to seperate SQL statements within the stored procedure. (We are forced to do so!)This will damage the integrity of our BEGIN-END statement block. Thus, we should temporarily change the DELIMITER (into $$) and change back to ; after the definition of our stored procedure. In order to avoid conflict of naming, MySQL would add a pair of &#96; to the names of databases, tables and columns. 1Student -&gt; `Student` -- Auto-Renamed to avoid conflicts. You can use 1CALL procedure_name() to call the defined stored procedure. If you are using the workbench of MySQL, you can simply right click Store Procedures to create one. In this case you do not need to worry about the delimiter and MySQL would help you with the transformation (Some sort of Syntactic sugar). Delete Stored Procedure1DROP PROCEDURE (IF EXISTS) get_clients; Parameter Setting of Stored Procedure12CREATE PROCEDURE get_clinets ( state CHAR(2) );-- You must specify the size and type of the passed parameter. After setting the parameter declaration, the parameters can be then used in the procedure for program logic building.All the parameters are REQUIRED. However, they can have default values.Even if you want to use the default value, you should pass a NULL to the procedure as a placeholder. 123456IF state IS NULL THEN SET state = ‘CA’; END IF;-- By using such statement, you can realize default value de facto.table.col_name = IFNULL(para, table.col_name);-- This one is more concise and thus recommended.-- If para is NULL, then use the default value. Parameter Verification and Constraints12345IF payment_amount &lt;= 0 THEN SIGNAL SQLSTATE &#x27;22003&#x27; SET MESSAGE_TEXT = ‘Invalid payment amount’;END IF-- payment_amount is required to &gt; 0. If not satisfied, a error code of &#x27;22003&#x27; is raised and the prompting MESSAGE_TEXT is written. SIGNAL is similar to throw exception in other languages. SQLSTATE is the predetermined error code (corresponding to ‘out of range’ here, refer the documentation). Provide Return Value for Stored Procedure1234CREATE PROCEDURE `name` (client_id INT, OUT invoices_count INT)BEGIN SELECT COUNT(*) INTO invoices_count FROM invoices;END This is a syntactic sugar anyway. What MySQL actually does is define two parameters and pass them to the PROCEDURE for updating. Then SELECT the updated parameter.At the same time, MySQL uses a ‘@’ prefix to identify variables. For an example, 1SET @a = 0; User Variable and Local VariableSET is used to assign the User Variable while DECLARE is used to assign the Local Variable used within a stored procedure. 1DECLARE risk_factor DECIMAL(9, 2) DEFAULT 0; You can assign the declared variable with the following code: 1SELECT COUNT(*) INTO risk_factor FROM table; User-Defined FunctionsThe only difference between the procedure and function is that function can only return one single value. 12345678CREATE FUNCTION get_risk_f ( client_id INT) -- Must specify the type for the input parameterRETURNS INTEGER -- The return parameter must have a type too. Function always returns a single value rather than a query result.DETERMINISTIC -- Optional attribute. Always return the same result for the same id.READS SQL DATA -- Optional. This function can read SQLMODIFIES SQL DATA -- Optional. This function can modify table.BEGIN...RETURN 1; Functions can be deleted with DROP keyword as well.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"SQL-Study-Note-4 - View","slug":"SQL-Study-Note-4","date":"2022-04-17T01:39:43.000Z","updated":"2022-04-17T02:00:31.084Z","comments":true,"path":"2022/04/16/SQL-Study-Note-4/","link":"","permalink":"https://umiao.github.io/2022/04/16/SQL-Study-Note-4/","excerpt":"ViewIntroduction to ViewWith the introduction of View, middle &#x2F; query results can be stored for further query and use, just like a real table.","text":"ViewIntroduction to ViewWith the introduction of View, middle &#x2F; query results can be stored for further query and use, just like a real table. Creation and Manipulation on View Creation of View: 1CREATE VIEW view_name AS (SELECT …) Created Views would NOT be stored with the tables. It would be stored in ‘Views’ instead. Alter &#x2F; Drop of View: 1234DROP VIEW sales_by_client;-- Drop / delete Operation.CREATE / REPLACE VIEW AS ...;-- REPLACE has the advantage against CREATE that it does not require the view to be dropped in advance. Created Views would NOT be stored with the tables. It would be stored in ‘Views’ instead. In fact, the Views should be viewed as stored query code. When you need to use them, you can simply rerun the script to retrieve the result. So you can use version control tools to store and share them. Updatable Views: Updatable Views stand for those Views who DO NOT contain the keywords of DISTINCT &#x2F; aggregate functions &#x2F; GROUP BY &#x2F; HAVING &#x2F; UNION. In this case, these Views can be updated via CREATE &#x2F; REPLACE. Update Opertion: 1UPDATE view_name SET due_date = DATE_ADD(due_date, INTERVAL 2 DAY) WHERE invoice_id = 1l Point of updatable views: If you DO NOT have the authorization to modify a table, you can still create a View based on that table and update the View, as long as it is Updatable. WITH CHECK OPTION: If the UPDATE operation may cause some rows to be deleted, you can add WITH CHECK OPTION to the end of the UPDATE code to prevent this from happening.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"SQL Study Note - 3 - Function and the Aggregate Function","slug":"SQL-Study-Note-3","date":"2022-04-17T01:34:31.000Z","updated":"2022-04-17T01:48:08.598Z","comments":true,"path":"2022/04/16/SQL-Study-Note-3/","link":"","permalink":"https://umiao.github.io/2022/04/16/SQL-Study-Note-3/","excerpt":"Function and the Aggregate Function","text":"Function and the Aggregate Function Aggregate Function: 1COUNT(), MAX(), MIN(), AVG(), SUM() It should be noted that COUNT(row_name) only returns the number of the non-empty records. If you need to find out the total number of rows, you shoud use COUNT(*). You can use COUNT (DISTINCT client_id) to find out the number of unique client_id, too. Non-Aggregate Function: 1234RAND() -- Generate a random number within (0, 1)RAND(seed) -- Specify the rand seedSQRT() -- Find the square root for each valueCONCAT(a, b) -- Concat two strings into one Non-aggregate function would return a same-lengthed sequence for the input values. These functions are element-wised. GROUP BY clause: Group the rows according to a given column name. Rows with the same value in the very column would be aggregated together. The order of query clause: IMPORTANT: SELECT -&gt; FROM -&gt; WHERE -&gt; ORDER BY Grouping based on multiple attributes: 1GROUP BY state, city In this case, the grouping would based on the tuple: (state, city) Having: 1SELECT SUM(res) AS aggregated_res FROM t GROUP BY state HAVING aggregated_res &gt; 100; You cannot use WHERE clause to filter the result of the aggreate function, because at that time, the query is not yet completed and the aggregate result is not yet calculated. It should be noted that HAVING clause supports the filtering on Composite Condition, e.g. HAVING aggre_res1 &gt; 10 AND aggre_res2 &lt; 10. The HAVING clause is execute after the query is finished. So that it can only filter the selected columns. The columns not selected cannot be used in the filtering constraint. HAVING does NOT support alias. WITH ROLLUP: 1SELECT SUM(res) AS aggregated_res FROM t GROUP BY state HAVING aggregated_res &gt; 100; Conduct an extra aggregate operation for all the aggregated results. E.g., the SUM &#x2F; MEAN of the aggregated results. This is only supported by MySQL. If composite grouping is applied, e.g., GROUP BY col_a, col_b, each group identified by a unique (col_a, col_b) would conduct an extra aggregation. Nesting Sub-query: 12SELECT * FROM (SELECT ...);SELECT * FROM table WHERE col_name IN / NOT IN (SELECT ...); I.E., select from the result of another select operation. ALL VS ANY &#x2F; SOME 1234SELECT * FROM invoices WHERE invoice_total &gt; ALL(SELECT invoice_total FROM invoices WHERE client_id = 3);-- Require the records to be greater than ANY of the sub-query results in order to be selectedSELECT * FROM invoices WHERE invoice_total &gt; ANY / SOME(SELECT invoice_total FROM invoices WHERE client_id = 3);-- Require the records to be greater than ONE of the sub-query results in order to be selected ANY &#x2F; SOME are completely equivalent. If one value of the sub-query satisfies the logical expression, the ‘&gt; ANY’ expression is satisfied. For ALL, only if all the values of the sub-query satisfies the logical expression, the ‘&gt; ALL’ expression can be satisfied. ‘&#x3D; ANY’ also equals to ‘IN’. Correlated sub-query Code-writing Order: SELECT, FROM, WHERE, GROUP BY, HAVING, ORDER BY Executing Order: FROM, WHERE, GROUP BY, HAVING, SELECT, ORDER BY The marked line is crucial, as the logic of correlated subquery would retrive one value from the main query at a time, working in an iterative manner, input the value into the sub-query to ge the result and pass the result back to the main query. The main query would check the constraint of WHERE clause and return the result; This means that different alias are required even they points to the same table. Sub-query can touch the alias of the main &#x2F; outer query. EXISTS 1SELECT * FROM c WHERE EXISTS (SELECT id FROM I WHERE c_id = c.c_id) EXISTS keyword has advantage against the ‘IN (sub-query)’ manner. This is because the ‘IN (sub-query)’ needs to finish the sub-query first before the sub-query can return any result to the outer query. However, EXISTS can work as a short-circuiting operator which stops immediately when the first results is found. Write Sub-query in SELECT &#x2F; FROM clause Use SELECT to duplicate the result of aggregate function1SELECT (SELECT AVG() FROM t) AS average FROM t; This query makes sense because, the aggregate function would only return one single value. If you want to conduct row-wise calculation on the aggregate value, you have to duplicate it. You can also SELECT FROM a sub-query like it is a real-table. However, an alias is required. This may make the query too complicated and should be used with care. Numerical Function123456789ROUND(num, mantissas_n); -- Round the given number, and mantissas_n decimal places are preserved.TRUNCATE(num, mantissas_n); -- Truncate the given number with the decimal setting.--Simiarly, we have:CEILING()FLOOR()ABS()RAND() Other available functions can be found in the MySQL documentation. String Function1234567891011121314151617181920LENGTH(&#x27;sky&#x27;); -- Return the length of the input string.UPPER(&#x27;sky&#x27;);--transform to upper caseLOWER(&#x27;sky&#x27;);--transform to lower caseLTRIM() / RTRIM() / TRIM();-- Remove the left / right / both side of spaces.LEFT(string, n);-- Return the left n chars;RIGHT(string, n);-- Return the right n chars;SUBSTRING(string, start, n);-- Return n chars starts from start.LOCATE(&#x27;n&#x27;, &#x27;kinter&#x27;);-- Find the smallest index where the pattern (&#x27;n&#x27;) occurs in the searched string (&#x27;kinter&#x27;). Index starts from 1.REPLACE(string, source, target);-- Replace all the pattern of source to target, in string.CONCAT(a, b);-- Concat strings into 1. Refer the documentation for more string functions. Time Function1234567891011121314NOW();-- Return the current time and date.CURDATE();-- Return the current date.CURTIME():-- Return the current time.YEAR(time) / MONTH() / DAY() / HOUR() / MINUTE() / SECOND() ...-- Extract the year... of a time / date. DAYNAME(time);-- Return the name of day, like FridayMONTHNAME(time);-- Return sth like DECEMBER...EXTRACT( YEAR FROM NOW() )-- Personalize a date / time.. Year can be substitute with other keywords E.g., extract order of this year: YEAR(date) &#x3D; YEAR(NOW()); Fomat Time &#x2F; Date123456789101112DATE_FORMAT(NOW(), &#x27;%y&#x27;);-- %y for 2-digit year, and %Y for 4-digit year;-- Similar for D/d/M/m, ...-- Can be use to format time as well.SELECT DATE_ADD( NOW(), INTERVAL 1 DAY );-- Return the date of 1 day later. Accept negative value like -1.-- Can Use DATE_SUB instead and the behavior is very similar.DATEDIFF(d_1, d_2);-- Find the diff of two dates. (by days, the input accepts DATE only)-- Result can be negative, calculated by d_1 - d_2.TIME_TO_SEC(time);-- Transform time to second, starts from 12 am. IFNULL &amp; COALESCE123SELECT order_id IFNULL(shipper_id, &#x27;Not Assigned&#x27;) AS s;-- If a shipper_id is NULL, return &#x27;Not Assigned&#x27; instead.COALESCE(shipper_id, comments, ..., &#x27;Not assigned&#x27;); The COALESCE is the generalization of IFNULL. The principle is, by offering a bunch of column_name &#x2F; values, return the first one which IS NOT NULL. IF &#x2F; CASE (Conditional Statement)1234567891011IF (expression, first, second);-- If the expression is TRUE, then return first; else, return second.SELECT order_id CASE WHEN YEAR(order_date) = YEAR(NOW()) THEN ‘Active’ WHEN YEAR(order_date) = YEAR(NOW()) - 1 THEN ‘Last Year’ ELSE ‘Other cases’END AS categoryFROM orders-- The case statement is quite similar with IF statement.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"SQL Study Note - 2 - The Update / Delete / Insert Syntax","slug":"SQL-Study-Note-2","date":"2022-04-17T01:29:50.000Z","updated":"2022-04-17T01:48:03.088Z","comments":true,"path":"2022/04/16/SQL-Study-Note-2/","link":"","permalink":"https://umiao.github.io/2022/04/16/SQL-Study-Note-2/","excerpt":"The update &#x2F; delete &#x2F; insert syntax","text":"The update &#x2F; delete &#x2F; insert syntax The column attributes of table: PK： primary key NN： Not Null UQ：Unique Index B： binary UN： unsigned data type ZF： zero filled AI： Auto incremental G： Generated column Data Type in MySQL: INT (11): integer with a length of 11 VARCHAR(50): array of char with a size $\\le 50$. CHAR(50): array of char with a size $&#x3D; 50$. DEFAULT: Use the given default value to fill this column. Insert data to table: 1INSERT INTO customers VALUES (DEFAULT, ‘John’, ‘Smith’, ‘1990-01-01’, NULL, ‘address’, ‘city’, ‘CA’, DEFAULT); At the same time, MySQL allows to specify the column names to be assigned values. 1INSERT INTO customers (first_name, …, state, points) VALUES (…); In this case, you do not need to assign the values to the order (of columns) defined by the table. The number of affected rows would be returned after successful insertion. Insert Multi-rows: 1INSERT INTO shippers (name) VALUES (&#x27;S1&#x27;), (&#x27;S2&#x27;), (&#x27;S3&#x27;); LAST_INSERT_ID: Returns the most recently generated Auto Incremental ID. Enable hierarchical data insertion. I.E., find the ID of the latest inserted data, and then use the id to associate &#x2F; update other tables. This syntax feature can eliminate ambiguity, and it is also convenient to correspond a main table record to multiple sub table records. Duplicate Table: 1CREATE TABLE orders_archived AS SELECT * FROM orders; Use the selected partial &#x2F; entire data of other table to create a duplicate. However, column attributes (constraints) like PK, AI would be ignored. Batch Insertion with Select: 1INSERT INTO orders_archived SELECT * FROM orders WHERE order_date &lt; ’2019-01-01’; Column attributes (constraints) like PK, AI would be ignored as well. Truncate Table in workbench: Right click a table and select ‘truncate table’ would remove all the data (records) but would not remove the table itself. Update a single row: 1UPDATE invoices SET payment_total = 10, payment_date=’2019-03-01’ WHERE invoice_id = 1; Filter the results (single line) that meet the conditions and update the filtered results. It is also allowed to use default, arithmetic expression, etc. as the new value of the selected row. Note that even if multiple statements are selected for your filter criteria, MySQL workbench runs in the security update mode by default, allowing you to update only one row at a time. However, in other environments, there is no such problem. You can drag it to the bottom of SQL editor and choose to uncheck the safe updates option. After changing the settings, you need to reconnect for the settings to take effect. WHERE attribute IN (1,2,3) can be used to filter multiple records. Use SELECT to UPDATE (apply sub-query in UPDATE): 1UPDATE invoices SET payment_total = 10, payment_date=’2019-03-01’ WHERE client_id = ( SELECT client_id FROM clients WHERE name=’Myworks’ ); In this manner, a single logical judgement is replaced with a sub-query to update multiple values at a time. If multiple values would be returned in your sub-query, the WHERE clause should be changed to WHERE client_id IN (sub-query). You should test the sub-query before update the table. You CANNOT update the same table where you conduct your sub-query —- If you have to do so, create a duplicate and give it an alias. DELETE:1DELETE FROM invoices WHERE (invoice_id=1); Obviously, the condition within the parenthesis can be a sub-query, too. Rebuild the Database:12DROP DB If EXISTS DB;-- Conduct the script of DB building then.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]},{"title":"SQL Study Note - 1 - Syntax Basics","slug":"SQL-Study-Note-1","date":"2022-04-16T21:05:53.000Z","updated":"2022-04-17T01:31:33.088Z","comments":true,"path":"2022/04/16/SQL-Study-Note-1/","link":"","permalink":"https://umiao.github.io/2022/04/16/SQL-Study-Note-1/","excerpt":"Overview The core of database system is to interact with DB (DataBase) with DBMS (Database Management System).","text":"Overview The core of database system is to interact with DB (DataBase) with DBMS (Database Management System). The DB can be generally divided into: Relational DB NoSQL DB (e.g., KV (key-value) based DB) Recommended code style of SQL (Structed Query Language): Capitalize all keywords and reserved words, and lowercase all other contents. Each statement should end with ;. Keywords and Syntax RulesThe query syntax SHOW DATABASES 1SHOW DATABASES; List all the names of exisitng databases (under the current schema). USE 1234USE database_name;SELECT * from table;SELECT * from database_name.table; -- Must specify the database_name for the DBs which are not in use Select one DB as the default one. SELECT 1SELECT (column_name) FROM (table_name) WHERE (condition) ORDER BY (col_name). Select the desired data from a table. It should be noted that you should use &#x3D; in SQL to determine equivalence. (It DOES NOT mean assignment.) You can conduct calculation on the selected result. AS 1SELECT (column_name) FROM (table_name) WHERE (condition) ORDER BY (col_name). For each col_name, table_name to be queried and the queried results, you can always to set alias (surname) for them with AS. – &#x2F; comment 12-- This a comment.// This is also acceptable. DISTINCT 1SELECT DISTINCT column_name FROM t Add DISTINCT ahead of the queried target (column) to receive all the unique values. AND &#x2F; OR &#x2F; NOT 1SELECT * FROM t WHERE col_a &gt; 10 and col_b = &#x27;CA&#x27; Logical operator to be used with the WHERE clause. IN 1SELECT * FROM t WHERE col_a IN (&#x27;A&#x27;, &#x27;B&#x27;) Determine if the queried value belongs to a set. BETWEEN 123SELECT * FROM t WHERE point BETWEEN 100 AND 300;SELECT * FROM t WHERE point &gt;= 100 AND point &lt;= 300;-- These two are equivalent Determine if the queried value within a given interval. Both ends of the interval are closed ([beg, end]). LIKE12SELECT * FROM t WHERE name like &#x27;b%&#x27;-- Able to match &#x27;Bob&#x27; and &#x27;bike&#x27; Provide functionality similar to Regular Expression. % can match arbitrary string, _ can match arbitrary char. Not sensitive to the case. REGEXP1SELECT * FROM t WHERE name REGEX &#x27;^f[a-z]+d&#x27; Match a given Regular Expression pattern. IS NULL12SELECT * FROM t WHERE name IS NULL;SELECT * FROM t WHERE name IS NOT NULL; Query all the records which are (not) null in a given column. ORDER BY1SELECT * FROM t ORDER BY col_name DESC / AESC Decide the sorting order of the returned records. LIMIT12SELECT * FROM t LIMIT offset, tot_numSELECT * FROM t LIMIT tot_num Restrict the number of the returned records. Skip the first $n&#x3D;$offset records and then return tot_num lines of records. Return all the records if number of matched records fewer than tot_num. INNER JOIN123SELECT * FROM t_a JOIN t_b on t_a.col_1 = t_b.col_2;SELECT * FROM t_a JOIN t_b on t_a.col_1 = DB_2.t_b.col_2;-- You can conduct crossed-DB join by specifying the name of the DB not in use. Can be simply writtene as JOIN. Concat two tables based on the given condition. Conduct Cartesian product implicitly. SELF JOIN1SELECT * FROM t_a AS a JOIN t_a AS b on a.col_1 = b.col_2; A table can join with itself, but different alias are required. Multi-table JOIN1SELECT * FROM t_a JOIN t_b on a.col_1 = b.col_2 JOIN t_c on b.col_2 = c.col_3; Multiple (n) tables can be joined but this is not recommended when $ n &gt; 3 $ due to performance concern. Composite JOIN &#x2F; Implicit JOIN1SELECT * FROM t_a JOIN t_b on a.col_1 = b.col_2 JOIN t_c on b.col_2 = c.col_3; Sometimes only a tuple of multiple attributes can uniquely identify a row of the table. In this case, these attributes become Composite Primary Key. 1SELECT * FROM t_a, t_b; Implicit JOIN is conducted in the above example but this is not recommended, too. OUTER JOIN1SELECT * FROM t_a AS a JOIN t_b AS b on a.col_1 = b.col_2 JOIN t_c AS c on b.col_2 = c.col_3; When using INNER JOIN, some records of the left table cannot match with the right table because the condition of the ON clause is not satisfied. However, if we want to return all the records of the left (right) table regardless of the boolean value of the ON clause, we can use LEFT (RIGHT) OUTER JOIN. You should use RIGHT JOIN instead of LEFT JOIN as possible. SELF OUTER JOIN is similar, and alias is still required. USING123SELECT * FROM t_a JOIN t_b on t_a.col_1 = t_b.col_1;SELECT * FROM t_a JOIN t_b USING (col_1);-- These two are equivalent. Can be used to simplify the code, if the column names of the to-be-joined tables are exactly the same. You can Join on a tuple like ‘USING (id1, id2, id3)’ and these column names should be exactly the same as well (in the two tables). NATURAL JOIN1SELECT * FROM t_a NATURAL JOIN t_b; Let the compiler (DBMS) to decide the way of join. Not recommended to use! CROSS JOIN1SELECT * FROM t_a CROSS JOIN t_b; Conduct Cartesian Product. UNION1SELECT * FROM a UNION SELECT * FROM b; Concatenate multiple queried results together (on the direction of row). The column names should be exactly the same. IT SHOULD BE NOTED that ORDER BY can be set only once, so union all the results before setting the ORDER clause.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"}]}],"categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://umiao.github.io/categories/Data-Science/"},{"name":"Data System","slug":"Data-Science/Data-System","permalink":"https://umiao.github.io/categories/Data-Science/Data-System/"},{"name":"Job Search","slug":"Job-Search","permalink":"https://umiao.github.io/categories/Job-Search/"},{"name":"Software Engineering","slug":"Job-Search/Software-Engineering","permalink":"https://umiao.github.io/categories/Job-Search/Software-Engineering/"},{"name":"Productivity","slug":"Productivity","permalink":"https://umiao.github.io/categories/Productivity/"},{"name":"Investment","slug":"Investment","permalink":"https://umiao.github.io/categories/Investment/"},{"name":"Financial Firm","slug":"Job-Search/Financial-Firm","permalink":"https://umiao.github.io/categories/Job-Search/Financial-Firm/"},{"name":"AI","slug":"AI","permalink":"https://umiao.github.io/categories/AI/"},{"name":"NLP","slug":"AI/NLP","permalink":"https://umiao.github.io/categories/AI/NLP/"},{"name":"General Knowledge","slug":"Data-Science/General-Knowledge","permalink":"https://umiao.github.io/categories/Data-Science/General-Knowledge/"},{"name":"SQL","slug":"Data-Science/SQL","permalink":"https://umiao.github.io/categories/Data-Science/SQL/"},{"name":"SQL","slug":"Job-Search/SQL","permalink":"https://umiao.github.io/categories/Job-Search/SQL/"},{"name":"UCLA","slug":"UCLA","permalink":"https://umiao.github.io/categories/UCLA/"},{"name":"Course Study","slug":"UCLA/Course-Study","permalink":"https://umiao.github.io/categories/UCLA/Course-Study/"},{"name":"ECE209 in 2022 spring","slug":"UCLA/Course-Study/ECE209-in-2022-spring","permalink":"https://umiao.github.io/categories/UCLA/Course-Study/ECE209-in-2022-spring/"}],"tags":[{"name":"DataScience","slug":"DataScience","permalink":"https://umiao.github.io/tags/DataScience/"},{"name":"Data System","slug":"Data-System","permalink":"https://umiao.github.io/tags/Data-System/"},{"name":"Keyboard","slug":"Keyboard","permalink":"https://umiao.github.io/tags/Keyboard/"},{"name":"Term","slug":"Term","permalink":"https://umiao.github.io/tags/Term/"},{"name":"Linux","slug":"Linux","permalink":"https://umiao.github.io/tags/Linux/"},{"name":"Vim","slug":"Vim","permalink":"https://umiao.github.io/tags/Vim/"},{"name":"CLI","slug":"CLI","permalink":"https://umiao.github.io/tags/CLI/"},{"name":"English","slug":"English","permalink":"https://umiao.github.io/tags/English/"},{"name":"Vocabulary","slug":"Vocabulary","permalink":"https://umiao.github.io/tags/Vocabulary/"},{"name":"Word Frequency","slug":"Word-Frequency","permalink":"https://umiao.github.io/tags/Word-Frequency/"},{"name":"options","slug":"options","permalink":"https://umiao.github.io/tags/options/"},{"name":"futures","slug":"futures","permalink":"https://umiao.github.io/tags/futures/"},{"name":"trading","slug":"trading","permalink":"https://umiao.github.io/tags/trading/"},{"name":"investment","slug":"investment","permalink":"https://umiao.github.io/tags/investment/"},{"name":"Productivity","slug":"Productivity","permalink":"https://umiao.github.io/tags/Productivity/"},{"name":"Apple Watch","slug":"Apple-Watch","permalink":"https://umiao.github.io/tags/Apple-Watch/"},{"name":"Tips","slug":"Tips","permalink":"https://umiao.github.io/tags/Tips/"},{"name":"Software Engineering","slug":"Software-Engineering","permalink":"https://umiao.github.io/tags/Software-Engineering/"},{"name":"Cyber Security","slug":"Cyber-Security","permalink":"https://umiao.github.io/tags/Cyber-Security/"},{"name":"Certificate","slug":"Certificate","permalink":"https://umiao.github.io/tags/Certificate/"},{"name":"OOD","slug":"OOD","permalink":"https://umiao.github.io/tags/OOD/"},{"name":"Object Oriented Design","slug":"Object-Oriented-Design","permalink":"https://umiao.github.io/tags/Object-Oriented-Design/"},{"name":"IQ","slug":"IQ","permalink":"https://umiao.github.io/tags/IQ/"},{"name":"Brainteasers","slug":"Brainteasers","permalink":"https://umiao.github.io/tags/Brainteasers/"},{"name":"Math","slug":"Math","permalink":"https://umiao.github.io/tags/Math/"},{"name":"NLP","slug":"NLP","permalink":"https://umiao.github.io/tags/NLP/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://umiao.github.io/tags/Machine-Learning/"},{"name":"GBM","slug":"GBM","permalink":"https://umiao.github.io/tags/GBM/"},{"name":"XGBoost","slug":"XGBoost","permalink":"https://umiao.github.io/tags/XGBoost/"},{"name":"SQL","slug":"SQL","permalink":"https://umiao.github.io/tags/SQL/"},{"name":"Random Forest","slug":"Random-Forest","permalink":"https://umiao.github.io/tags/Random-Forest/"},{"name":"Regularization","slug":"Regularization","permalink":"https://umiao.github.io/tags/Regularization/"},{"name":"Naive Bayes","slug":"Naive-Bayes","permalink":"https://umiao.github.io/tags/Naive-Bayes/"},{"name":"UCLA","slug":"UCLA","permalink":"https://umiao.github.io/tags/UCLA/"},{"name":"SVM","slug":"SVM","permalink":"https://umiao.github.io/tags/SVM/"}]}